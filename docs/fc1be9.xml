<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>歸藏 / @op7418</title>
        <link>https://nitter.cz/op7418</link>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745002229302751645#m</id>
            <title>RT by @op7418: 北京时间这周六上午 11 点直播和indigo聊聊 AI 内容生成和超级个体的相关内容。

感兴趣的各位可以到时候来看看，会在我和他的推特推流。</title>
            <link>https://nitter.cz/op7418/status/1745002229302751645#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745002229302751645#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 08:38:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>北京时间这周六上午 11 点直播和indigo聊聊 AI 内容生成和超级个体的相关内容。<br />
<br />
感兴趣的各位可以到时候来看看，会在我和他的推特推流。</p>
<p><a href="https://nitter.cz/indigo11/status/1745000156419019064#m">nitter.cz/indigo11/status/1745000156419019064#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745141172639838566#m</id>
            <title>RT by @op7418: 再说一下为什么我觉得这个路子对了，很多人认为这玩意“不过就是一个语音助手”只要手机厂商腾出手来立刻就完蛋。

这个设备最重要的就是那个行动模型而不是语音和LLM功能，是设备自动判断意图在无数app中精确执行指令的能力，更深入的还有联动多个app执行一个任务的能力。

我们使用手机场景一种是用来消费内容的，比如看视频，这部分交互没有任何优化空间。

但是，影响我们效率的更多是使用工具类和生活类软件执行任务的动作，想一下你从手机几十上百个app中找到你想要的在打开对应的功能学习使用要多少时间。
再想一下这个一句话就能完成要多少时间。

他不一定做好了这个事情，但是路子是对的，未来智能设备的内容消费软件的交互不会改变，但是使用频率较低的应用使用方式都会变成类似的方式。</title>
            <link>https://nitter.cz/op7418/status/1745141172639838566#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745141172639838566#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 17:50:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>再说一下为什么我觉得这个路子对了，很多人认为这玩意“不过就是一个语音助手”只要手机厂商腾出手来立刻就完蛋。<br />
<br />
这个设备最重要的就是那个行动模型而不是语音和LLM功能，是设备自动判断意图在无数app中精确执行指令的能力，更深入的还有联动多个app执行一个任务的能力。<br />
<br />
我们使用手机场景一种是用来消费内容的，比如看视频，这部分交互没有任何优化空间。<br />
<br />
但是，影响我们效率的更多是使用工具类和生活类软件执行任务的动作，想一下你从手机几十上百个app中找到你想要的在打开对应的功能学习使用要多少时间。<br />
再想一下这个一句话就能完成要多少时间。<br />
<br />
他不一定做好了这个事情，但是路子是对的，未来智能设备的内容消费软件的交互不会改变，但是使用频率较低的应用使用方式都会变成类似的方式。</p>
<p><a href="https://nitter.cz/op7418/status/1744904773873377355#m">nitter.cz/op7418/status/1744904773873377355#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745287152278347871#m</id>
            <title>RT by @op7418: 字节昨天发布的这个视频生成模型MagicVideo-V2的效果很好啊，动作幅度很大而且很自然。

与 Runway 和 SVD 相比，写实内容的细节不够丰富，纹理缺失，有一种塑料质感。

功能也很全面：
集成了文本到图像（T2I）、图像到视频（I2V）、视频到视频（V2V）和视频帧插值（ VFI）模块。

模型的大致训练方式为：
T2I 模块创建封装所描述场景的 1024×1024 图像。随后，I2V 模块对该静态图像进行动画处理，生成 600×600×32 帧的序列，其中潜在噪声优先确保与初始帧的连续性。

V2V 模块将这些帧增强至 1048×048 分辨率，同时细化视频内容。最后，插值模块将序列扩展至 94 帧，得到分辨率为 1048×1048 的视频，保证模型具有高美感又具有时间平滑性。

项目地址：https://magicvideov2.github.io/</title>
            <link>https://nitter.cz/op7418/status/1745287152278347871#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745287152278347871#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jan 2024 03:30:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>字节昨天发布的这个视频生成模型MagicVideo-V2的效果很好啊，动作幅度很大而且很自然。<br />
<br />
与 Runway 和 SVD 相比，写实内容的细节不够丰富，纹理缺失，有一种塑料质感。<br />
<br />
功能也很全面：<br />
集成了文本到图像（T2I）、图像到视频（I2V）、视频到视频（V2V）和视频帧插值（ VFI）模块。<br />
<br />
模型的大致训练方式为：<br />
T2I 模块创建封装所描述场景的 1024×1024 图像。随后，I2V 模块对该静态图像进行动画处理，生成 600×600×32 帧的序列，其中潜在噪声优先确保与初始帧的连续性。<br />
<br />
V2V 模块将这些帧增强至 1048×048 分辨率，同时细化视频内容。最后，插值模块将序列扩展至 94 帧，得到分辨率为 1048×1048 的视频，保证模型具有高美感又具有时间平滑性。<br />
<br />
项目地址：<a href="https://magicvideov2.github.io/">magicvideov2.github.io/</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDUyODA1NjI4MDAzMjA1MTIvcHUvaW1nLy1LVGo4c0tyWFBXY0F3ZnUuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745275643531825155#m</id>
            <title>RT by @op7418: SD Web UI 的 Contorlnet 插件现在已经支持 IPadapter FaceID 模型的使用。

这个模型是之前 IPadapter Face 的升级版本，可以更好的从照片提取人像特征还原到生成的图片中。

比较麻烦的是 FaceID 需要安装 insightface才能生效。安装之后下载对应的模型和 Lora  放到模型文件夹就行。

ControlNet 插件更新日志：https://github.com/Mikubill/sd-webui-controlnet/discussions/2442</title>
            <link>https://nitter.cz/op7418/status/1745275643531825155#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745275643531825155#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jan 2024 02:45:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>SD Web UI 的 Contorlnet 插件现在已经支持 IPadapter FaceID 模型的使用。<br />
<br />
这个模型是之前 IPadapter Face 的升级版本，可以更好的从照片提取人像特征还原到生成的图片中。<br />
<br />
比较麻烦的是 FaceID 需要安装 insightface才能生效。安装之后下载对应的模型和 Lora  放到模型文件夹就行。<br />
<br />
ControlNet 插件更新日志：<a href="https://github.com/Mikubill/sd-webui-controlnet/discussions/2442">github.com/Mikubill/sd-webui…</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RoMnpySGJBQUF2UHpGLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745252782754693375#m</id>
            <title>RT by @op7418: 我去这么快，Midjourney 会在这个月内上线一致性角色生成功能。
会在二月上线初始的视频生成功能，风格微调也会很快上线。</title>
            <link>https://nitter.cz/op7418/status/1745252782754693375#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745252782754693375#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jan 2024 01:14:23 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>我去这么快，Midjourney 会在这个月内上线一致性角色生成功能。<br />
会在二月上线初始的视频生成功能，风格微调也会很快上线。</p>
<p><a href="https://nitter.cz/nickfloats/status/1745180951926251578#m">nitter.cz/nickfloats/status/1745180951926251578#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745264532451397773#m</id>
            <title>想拼车 ChatGPT 团队版的朋友可以关注一下 C 大的测试结果。</title>
            <link>https://nitter.cz/op7418/status/1745264532451397773#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745264532451397773#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jan 2024 02:01:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>想拼车 ChatGPT 团队版的朋友可以关注一下 C 大的测试结果。</p>
<p><a href="https://nitter.cz/Cydiar404/status/1745258400857125349#m">nitter.cz/Cydiar404/status/1745258400857125349#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745133154548265130#m</id>
            <title>RT by @op7418: 之前一直传的团队版ChatGPT终于来了，这个对拼单的比较合算。每人每月25美元，只能按年支付。

团队版比普通版多的权益有：

◈更多的GPT-4消息上限，但是没说多多少。
◈可以创建与团队内部共享的GPTs。
◈用于工作空间管理的管理员控制台。
◈可以选择不使用你的数据进行模型训练。

网页版左下角点击按钮升级新的Plus计划。</title>
            <link>https://nitter.cz/op7418/status/1745133154548265130#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745133154548265130#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 17:19:01 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>之前一直传的团队版ChatGPT终于来了，这个对拼单的比较合算。每人每月25美元，只能按年支付。<br />
<br />
团队版比普通版多的权益有：<br />
<br />
◈更多的GPT-4消息上限，但是没说多多少。<br />
◈可以创建与团队内部共享的GPTs。<br />
◈用于工作空间管理的管理员控制台。<br />
◈可以选择不使用你的数据进行模型训练。<br />
<br />
网页版左下角点击按钮升级新的Plus计划。</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RmMGRUUWJNQUU5alNLLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1744973171852480705#m</id>
            <title>RT by @op7418: 试用了一下Magnific AI新上线的 8 倍放大，真的强。
既保留了原图的结构和内容，又合理的增加了对应的细节。

没有用 16 倍是因为 Midjourney 的图片放大 16 倍会超出 10K 分辨率的上限。现在应该是 11K 的分辨率。

  一个小 tips ：预览的时候按住 Z 滚动滚轮，可以放大图片。

这里使用：https://magnific.ai</title>
            <link>https://nitter.cz/op7418/status/1744973171852480705#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1744973171852480705#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 06:43:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>试用了一下Magnific AI新上线的 8 倍放大，真的强。<br />
既保留了原图的结构和内容，又合理的增加了对应的细节。<br />
<br />
没有用 16 倍是因为 Midjourney 的图片放大 16 倍会超出 10K 分辨率的上限。现在应该是 11K 的分辨率。<br />
<br />
  一个小 tips ：预览的时候按住 Z 滚动滚轮，可以放大图片。<br />
<br />
这里使用：<a href="https://magnific.ai">magnific.ai</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDQ5NzMwNDYzMjk1Nzc0NzIvcHUvaW1nL1k2bVlGRkx2SmQ1X3hHTjIuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745137719351915005#m</id>
            <title>RT by @op7418: 来了！！ GPTs商店正式上线。

跟泄露的内容一致，包括搜索，使用排行和GPTs分类，搜索支持中文，以对话数量作为排行依据。

快去看一下你的能不能被搜到。#gpts

访问 GPTs 商店：https://chat.openai.com/gpts</title>
            <link>https://nitter.cz/op7418/status/1745137719351915005#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745137719351915005#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 17:37:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>来了！！ GPTs商店正式上线。<br />
<br />
跟泄露的内容一致，包括搜索，使用排行和GPTs分类，搜索支持中文，以对话数量作为排行依据。<br />
<br />
快去看一下你的能不能被搜到。<a href="https://nitter.cz/search?q=%23gpts">#gpts</a><br />
<br />
访问 GPTs 商店：<a href="https://chat.openai.com/gpts">chat.openai.com/gpts</a></p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDUxMzcwOTIzNDExNzAxNzYvcHUvaW1nL0JHSkdQd0RUMU5wRFZ4d0suanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/oran_ge/status/1745237898633609412#m</id>
            <title>RT by @op7418: 昨天被 @op7418 强烈安利，刚刚把视频看完了。
这个 demo 想做的事情很多是我想做的事情。
r1 的硬件倒是不那么惊艳，LAM 的操作系统则超越了现在的 Agent 范式，直接从行动出发去做操作系统，非常惊艳。
可想而知的是困难重重和badcase丛生，作为第一波的用户体验未必好。
好在199美金也不贵。</title>
            <link>https://nitter.cz/oran_ge/status/1745237898633609412#m</link>
            <guid isPermaLink="false">https://nitter.cz/oran_ge/status/1745237898633609412#m</guid>
            <pubDate></pubDate>
            <updated>Thu, 11 Jan 2024 00:15:14 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>昨天被 <a href="https://nitter.cz/op7418" title="歸藏">@op7418</a> 强烈安利，刚刚把视频看完了。<br />
这个 demo 想做的事情很多是我想做的事情。<br />
r1 的硬件倒是不那么惊艳，LAM 的操作系统则超越了现在的 Agent 范式，直接从行动出发去做操作系统，非常惊艳。<br />
可想而知的是困难重重和badcase丛生，作为第一波的用户体验未必好。<br />
好在199美金也不贵。</p>
<p><a href="https://nitter.cz/op7418/status/1744904773873377355#m">nitter.cz/op7418/status/1744904773873377355#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/_akhaliq/status/1744914502703874215#m</id>
            <title>RT by @op7418: ByteDance just announced MagicVideo-V2

Multi-Stage High-Aesthetic Video Generation

paper page: https://huggingface.co/papers/2401.04468

The growing demand for high-fidelity video generation from textual descriptions has catalyzed significant research in this field. In this work, we introduce MagicVideo-V2 that integrates the text-to-image model, video motion generator, reference image embedding module and frame interpolation module into an end-to-end video generation pipeline. Benefiting from these architecture designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution video with remarkable fidelity and smoothness. It demonstrates superior performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph, Moon Valley and Stable Video Diffusion model via user evaluation at large scale</title>
            <link>https://nitter.cz/_akhaliq/status/1744914502703874215#m</link>
            <guid isPermaLink="false">https://nitter.cz/_akhaliq/status/1744914502703874215#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 02:50:11 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ByteDance just announced MagicVideo-V2<br />
<br />
Multi-Stage High-Aesthetic Video Generation<br />
<br />
paper page: <a href="https://huggingface.co/papers/2401.04468">huggingface.co/papers/2401.0…</a><br />
<br />
The growing demand for high-fidelity video generation from textual descriptions has catalyzed significant research in this field. In this work, we introduce MagicVideo-V2 that integrates the text-to-image model, video motion generator, reference image embedding module and frame interpolation module into an end-to-end video generation pipeline. Benefiting from these architecture designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution video with remarkable fidelity and smoothness. It demonstrates superior performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph, Moon Valley and Stable Video Diffusion model via user evaluation at large scale</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3NDQ5MTQxMDI2OTcyNTkwMDkvcHUvaW1nL1pucnJoaDc0MGJzd3pXTE8uanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745152663136973221#m</id>
            <title>R to @op7418: 关于这玩意能不能成功？这个交互方式有没有用？以及是不是非要这样的设备？是三个问题。
也可以看一下下面的讨论：</title>
            <link>https://nitter.cz/op7418/status/1745152663136973221#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745152663136973221#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 18:36:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>关于这玩意能不能成功？这个交互方式有没有用？以及是不是非要这样的设备？是三个问题。<br />
也可以看一下下面的讨论：</p>
<p><a href="https://nitter.cz/levelsio/status/1744790008219816338#m">nitter.cz/levelsio/status/1744790008219816338#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745147894687281666#m</id>
            <title>R to @op7418: 补充一下这个档位的PLUS 可以使用32K上下文的 GPT4，以及按月付费的金额应该是 30美元。</title>
            <link>https://nitter.cz/op7418/status/1745147894687281666#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745147894687281666#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 18:17:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>补充一下这个档位的PLUS 可以使用32K上下文的 GPT4，以及按月付费的金额应该是 30美元。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745146855074509084#m</id>
            <title>Perplexity Design 团队正在招聘，设计师们可以看一下。</title>
            <link>https://nitter.cz/op7418/status/1745146855074509084#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745146855074509084#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 18:13:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Perplexity Design 团队正在招聘，设计师们可以看一下。</p>
<p><a href="https://nitter.cz/henrymodis/status/1745144777589600675#m">nitter.cz/henrymodis/status/1745144777589600675#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745144610958291413#m</id>
            <title>R to @op7418: 小红书的搬运者也被KO</title>
            <link>https://nitter.cz/op7418/status/1745144610958291413#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745144610958291413#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 18:04:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>小红书的搬运者也被KO</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0RmX3hRamJNQUVGa0xXLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/op7418/status/1745142075065315371#m</id>
            <title>同时在第一季度，Open AI将推出一个GPTs构建者收入计划。
美国的构建者将根据用户与他们的GPT的参与度而获得报酬。</title>
            <link>https://nitter.cz/op7418/status/1745142075065315371#m</link>
            <guid isPermaLink="false">https://nitter.cz/op7418/status/1745142075065315371#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 10 Jan 2024 17:54:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>同时在第一季度，Open AI将推出一个GPTs构建者收入计划。<br />
美国的构建者将根据用户与他们的GPT的参与度而获得报酬。</p>
<p><a href="https://nitter.cz/op7418/status/1745137719351915005#m">nitter.cz/op7418/status/1745137719351915005#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>