<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>Quinn Leng / @quinn_leng</title>
        <link>https://nitter.cz/quinn_leng</link>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1735881767482241348#m</id>
            <title>è™½ç„¶å­¦ç•Œå·²ç»è¯æ˜ç”¨ GPT çš„è¾“å‡ºå†…å®¹æ¥è®­ç»ƒæ¨¡å‹å¯ä»¥è¾¾åˆ°ä¸é”™çš„æ•ˆæœï¼Œä½†æ˜¯ä½œä¸ºä¸€å®¶å•†ä¸šå…¬å¸è¿™ä¹ˆåšå°±æœ‰ç‚¹ä½œå¼Šäº†ï¼Œè€Œä¸”é•¿æœŸæ¥çœ‹ä¹Ÿä¼šå¯¹ç ”ç©¶æ–¹å‘äº§ç”Ÿè¯¯å¯¼</title>
            <link>https://nitter.cz/quinn_leng/status/1735881767482241348#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1735881767482241348#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 16 Dec 2023 04:37:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>è™½ç„¶å­¦ç•Œå·²ç»è¯æ˜ç”¨ GPT çš„è¾“å‡ºå†…å®¹æ¥è®­ç»ƒæ¨¡å‹å¯ä»¥è¾¾åˆ°ä¸é”™çš„æ•ˆæœï¼Œä½†æ˜¯ä½œä¸ºä¸€å®¶å•†ä¸šå…¬å¸è¿™ä¹ˆåšå°±æœ‰ç‚¹ä½œå¼Šäº†ï¼Œè€Œä¸”é•¿æœŸæ¥çœ‹ä¹Ÿä¼šå¯¹ç ”ç©¶æ–¹å‘äº§ç”Ÿè¯¯å¯¼</p>
<p><a href="https://nitter.cz/dotey/status/1735878580943335679#m">nitter.cz/dotey/status/1735878580943335679#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1734301666344862087#m</id>
            <title>Mistral åœ¨ç¬¬äºŒä¸ªå¼€æºæ¨¡å‹ 8x7B ä¸Šé€‰æ‹©äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å¯ä»¥çœ‹å‡ºæ¥è¿™å®¶å…¬å¸çš„é‡å¿ƒæ˜¯å¾ˆå¤§çš„ï¼ŒMoE çš„ä¼˜åŠ¿åœ¨äºç”¨æˆ·é‡å¤šçš„æ—¶å€™å¯ä»¥å……åˆ†åˆ©ç”¨åˆ°ä¸åŒä¸“å®¶æ¨¡å—ï¼Œæ˜¾è‘—é™ä½æˆæœ¬ã€‚ä¸€åŒå‘å¸ƒçš„è¿˜æœ‰AI å¹³å°ï¼ŒåŒ…å«äº† embedding å’Œ LLMï¼Œå…¶ä¸­ small å¯¹æ ‡ gpt-3.5ï¼Œmedium è¶…è¿‡ 3.5 ï¼Œä¸‹ä¸€æ­¥å°±æ˜¯ gpt-4 äº†</title>
            <link>https://nitter.cz/quinn_leng/status/1734301666344862087#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1734301666344862087#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 19:58:33 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mistral åœ¨ç¬¬äºŒä¸ªå¼€æºæ¨¡å‹ 8x7B ä¸Šé€‰æ‹©äº†æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„å¯ä»¥çœ‹å‡ºæ¥è¿™å®¶å…¬å¸çš„é‡å¿ƒæ˜¯å¾ˆå¤§çš„ï¼ŒMoE çš„ä¼˜åŠ¿åœ¨äºç”¨æˆ·é‡å¤šçš„æ—¶å€™å¯ä»¥å……åˆ†åˆ©ç”¨åˆ°ä¸åŒä¸“å®¶æ¨¡å—ï¼Œæ˜¾è‘—é™ä½æˆæœ¬ã€‚ä¸€åŒå‘å¸ƒçš„è¿˜æœ‰AI å¹³å°ï¼ŒåŒ…å«äº† embedding å’Œ LLMï¼Œå…¶ä¸­ small å¯¹æ ‡ gpt-3.5ï¼Œmedium è¶…è¿‡ 3.5 ï¼Œä¸‹ä¸€æ­¥å°±æ˜¯ gpt-4 äº†</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGNS1hLWFvQUVwNkFoLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGNkZvT2E0QUFRd2Z2LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1734297913663332828#m</id>
            <title>æœ‰ç½‘å‹è¯´ Mistral 8x7B æœ‰ 56B å‚æ•°ï¼ˆå®é™… 46.7Bï¼‰ï¼Œæ‰å‹‰å¼ºè¶…è¿‡ Llama-2-70b çš„æ€§èƒ½ï¼Œè¿™ä¸ªå¯¹æ¯”æ˜¯æœ‰é—®é¢˜çš„ã€‚æ··åˆä¸“å®¶æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸­ä¸éœ€è¦åŒæ—¶æ¿€æ´»æ‰€æœ‰æ¨¡å‹å‚æ•°ï¼Œå°±åƒä¸€ä¸ªä¸­è½¬ç«™å¯ä»¥åœ¨å…«ä¸ªä¸‹æ¸¸ä¸­é€‰æ‹©ï¼Œå ç”¨äº†ç©ºé—´ï¼Œä½†æ˜¯å®é™…å¤„ç†æ•ˆç‡æ›´é«˜ã€‚Mistral çš„å®é™…æ¨ç†é€Ÿåº¦æ¥è¿‘ 12.9B æ¨¡å‹ï¼Œå¯ä»¥è¯´éå¸¸å¼ºäº†ã€‚</title>
            <link>https://nitter.cz/quinn_leng/status/1734297913663332828#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1734297913663332828#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 19:43:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>æœ‰ç½‘å‹è¯´ Mistral 8x7B æœ‰ 56B å‚æ•°ï¼ˆå®é™… 46.7Bï¼‰ï¼Œæ‰å‹‰å¼ºè¶…è¿‡ Llama-2-70b çš„æ€§èƒ½ï¼Œè¿™ä¸ªå¯¹æ¯”æ˜¯æœ‰é—®é¢˜çš„ã€‚æ··åˆä¸“å®¶æ¨¡å‹è®­ç»ƒå’Œæ¨ç†ä¸­ä¸éœ€è¦åŒæ—¶æ¿€æ´»æ‰€æœ‰æ¨¡å‹å‚æ•°ï¼Œå°±åƒä¸€ä¸ªä¸­è½¬ç«™å¯ä»¥åœ¨å…«ä¸ªä¸‹æ¸¸ä¸­é€‰æ‹©ï¼Œå ç”¨äº†ç©ºé—´ï¼Œä½†æ˜¯å®é™…å¤„ç†æ•ˆç‡æ›´é«˜ã€‚Mistral çš„å®é™…æ¨ç†é€Ÿåº¦æ¥è¿‘ 12.9B æ¨¡å‹ï¼Œå¯ä»¥è¯´éå¸¸å¼ºäº†ã€‚</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGMkZhUWFZQUVpZVBkLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0JGMlAyOWFvQUFhNG5ULmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1734243175408263394#m</id>
            <title>Mistral 8x7B åœ¨å¤šæ•°æµ‹è¯•æ•°æ®é›†ä¸Šçš„æ•ˆæœè¶…è¿‡ Llama 2 70B å’Œ GPT-3.5 ï¼Œå¹¶ä¸”å› ä¸ºæ˜¯æ··åˆä¸“å®¶æ¨¡å‹ï¼Œå®é™…æ¨ç†é€Ÿåº¦ç­‰åŒä¸€ä¸ª 12B çš„æ¨¡å‹ï¼Œå¯ä»¥é¢„è§æ‰€æœ‰æ¨¡å‹å¹³å°éƒ½ä¼šè¿…é€Ÿæ”¯æŒè¿™æ¬¾æœ€æ–°æœ€å¼ºçš„å¼€æºæ¨¡å‹</title>
            <link>https://nitter.cz/quinn_leng/status/1734243175408263394#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1734243175408263394#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 11 Dec 2023 16:06:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Mistral 8x7B åœ¨å¤šæ•°æµ‹è¯•æ•°æ®é›†ä¸Šçš„æ•ˆæœè¶…è¿‡ Llama 2 70B å’Œ GPT-3.5 ï¼Œå¹¶ä¸”å› ä¸ºæ˜¯æ··åˆä¸“å®¶æ¨¡å‹ï¼Œå®é™…æ¨ç†é€Ÿåº¦ç­‰åŒä¸€ä¸ª 12B çš„æ¨¡å‹ï¼Œå¯ä»¥é¢„è§æ‰€æœ‰æ¨¡å‹å¹³å°éƒ½ä¼šè¿…é€Ÿæ”¯æŒè¿™æ¬¾æœ€æ–°æœ€å¼ºçš„å¼€æºæ¨¡å‹</p>
<p><a href="https://nitter.cz/GuillaumeLample/status/1734216541099507929#m">nitter.cz/GuillaumeLample/status/1734216541099507929#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1733196609976963115#m</id>
            <title>å½“å¾ˆå¤šå…¬å¸æƒ³å°½ä¸€åˆ‡åŠæ³•åšå¥½çœ‹çš„å®˜ç½‘ï¼Œdemo è§†é¢‘ï¼Œåšå®¢çš„æ—¶å€™ï¼ŒMistral AIï¼Œæ›¾ç»å‘å¸ƒæœ€å¼º 7B æ¨¡å‹çš„å…¬å¸ï¼Œåˆšåˆšéšæ€§åœ°å‘äº†ä¸€ä¸ª bittorrent é“¾æ¥ï¼Œé‡Œé¢æ˜¯ä»–ä»¬æœ€æ–°çš„ 8x7b MOE æ··åˆä¸“å®¶æ¨¡å‹ã€‚ç¤¾åŒºè¿˜åœ¨è¯„ä¼°æ¨¡å‹çš„æ•ˆæœï¼Œåº”è¯¥ä¼šæ˜¯ä¸ªå¾ˆå¼ºçš„æ¨¡å‹ã€‚è¶Šæ¥è¶Šè§‰å¾— Mistral è¿™å®¶å…¬å¸å¾ˆæœ‰æ„æ€ã€‚</title>
            <link>https://nitter.cz/quinn_leng/status/1733196609976963115#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1733196609976963115#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Dec 2023 18:47:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>å½“å¾ˆå¤šå…¬å¸æƒ³å°½ä¸€åˆ‡åŠæ³•åšå¥½çœ‹çš„å®˜ç½‘ï¼Œdemo è§†é¢‘ï¼Œåšå®¢çš„æ—¶å€™ï¼ŒMistral AIï¼Œæ›¾ç»å‘å¸ƒæœ€å¼º 7B æ¨¡å‹çš„å…¬å¸ï¼Œåˆšåˆšéšæ€§åœ°å‘äº†ä¸€ä¸ª bittorrent é“¾æ¥ï¼Œé‡Œé¢æ˜¯ä»–ä»¬æœ€æ–°çš„ 8x7b MOE æ··åˆä¸“å®¶æ¨¡å‹ã€‚ç¤¾åŒºè¿˜åœ¨è¯„ä¼°æ¨¡å‹çš„æ•ˆæœï¼Œåº”è¯¥ä¼šæ˜¯ä¸ªå¾ˆå¼ºçš„æ¨¡å‹ã€‚è¶Šæ¥è¶Šè§‰å¾— Mistral è¿™å®¶å…¬å¸å¾ˆæœ‰æ„æ€ã€‚</p>
<p><a href="https://nitter.cz/MistralAI/status/1733150512395038967#m">nitter.cz/MistralAI/status/1733150512395038967#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/DrJimFan/status/1732473280148500786#m</id>
            <title>RT by @quinn_leng: AlphaCode-2 is also announced today, but seems to be buried in news. It's a competitive coding model finetuned from Gemini. In the technical report, DeepMind shares a surprising amount of details on an inference-time search, filtering, and re-ranking system. This may be Google's Q*? ğŸ¤”

They also discussed the finetuning procedure, which is 2 rounds of GOLD (an offline RL algorithm for LLM from 2020), and the training dataset. AlphaCode-2 scores at 87% percentile among the human competitors. 

Don't miss it: https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf</title>
            <link>https://nitter.cz/DrJimFan/status/1732473280148500786#m</link>
            <guid isPermaLink="false">https://nitter.cz/DrJimFan/status/1732473280148500786#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 18:53:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>AlphaCode-2 is also announced today, but seems to be buried in news. It's a competitive coding model finetuned from Gemini. In the technical report, DeepMind shares a surprising amount of details on an inference-time search, filtering, and re-ranking system. This may be Google's Q*? ğŸ¤”<br />
<br />
They also discussed the finetuning procedure, which is 2 rounds of GOLD (an offline RL algorithm for LLM from 2020), and the training dataset. AlphaCode-2 scores at 87% percentile among the human competitors. <br />
<br />
Don't miss it: <a href="https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf">storage.googleapis.com/deepmâ€¦</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyNXZUSWFRQUFFTU01LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyNlVCcmJjQUlmdXppLnBuZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyNmFDNWFRQUFuRmVaLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1732463866326773793#m</id>
            <title>æ¼”ç¤ºè§†é¢‘é‡Œ Gemini åŸç”Ÿå¤šæ¨¡æ€çš„ä¼˜åŠ¿ï¼Œå¯ä»¥åŒæ—¶ç†è§£è§†é¢‘å£°éŸ³å›¾ç‰‡å’Œæ–‡å­—ï¼Œä¸äººçš„äº’åŠ¨æ›´åŠ å‡†ç¡®è‡ªç„¶ã€‚æƒ³è±¡äº”åˆ°åå¹´åå¤§éƒ¨åˆ†æ‰‹æœºéƒ½èƒ½æœ¬åœ°è¿è¡Œè¿™æ ·ä¸€ä¸ªè¿™ä¹ˆå¬çœŸçœ‹çœŸæ„Ÿè§‰çš„ AI åŠ©æ‰‹ï¼Œå€¼å¾—æœŸå¾…ï¼</title>
            <link>https://nitter.cz/quinn_leng/status/1732463866326773793#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1732463866326773793#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 18:15:48 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>æ¼”ç¤ºè§†é¢‘é‡Œ Gemini åŸç”Ÿå¤šæ¨¡æ€çš„ä¼˜åŠ¿ï¼Œå¯ä»¥åŒæ—¶ç†è§£è§†é¢‘å£°éŸ³å›¾ç‰‡å’Œæ–‡å­—ï¼Œä¸äººçš„äº’åŠ¨æ›´åŠ å‡†ç¡®è‡ªç„¶ã€‚æƒ³è±¡äº”åˆ°åå¹´åå¤§éƒ¨åˆ†æ‰‹æœºéƒ½èƒ½æœ¬åœ°è¿è¡Œè¿™æ ·ä¸€ä¸ªè¿™ä¹ˆå¬çœŸçœ‹çœŸæ„Ÿè§‰çš„ AI åŠ©æ‰‹ï¼Œå€¼å¾—æœŸå¾…ï¼</p>
<p><a href="https://nitter.cz/FinanceYF5/status/1732423841799078116#m">nitter.cz/FinanceYF5/status/1732423841799078116#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1732450371384078522#m</id>
            <title>Google Deepmind å‘å¸ƒ Gemini 1.0 ï¼Œåœ¨ 32 é¡¹æŒ‡æ ‡ä¸­æœ‰ 30 é¡¹è¶…è¿‡ GPT-4ã€‚Gemini ä»æ¶æ„åˆ°è®­ç»ƒè¿‡ç¨‹éƒ½æ˜¯åŸç”Ÿçš„å¤šæ¨¡æ€ï¼Œå¯ä»¥æ”¯æŒå›¾åƒã€è§†é¢‘ã€è¯­éŸ³è¾“å…¥ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ç›´æ¥è¾“å‡ºå›¾åƒå’Œæ–‡å­—ï¼Œè€Œ GPT-4 ç›®å‰è¿˜ä¸èƒ½ç›´æ¥è¾“å‡ºå›¾åƒã€‚å…¶ä¸­ 1.8B å’Œ 3.25B å‚æ•°çš„æ¨¡å‹æ€§èƒ½éƒ½è¶…è¿‡äº† llama-2-7b ï¼Œå¯ä»¥è¿è¡Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šã€‚</title>
            <link>https://nitter.cz/quinn_leng/status/1732450371384078522#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1732450371384078522#m</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Dec 2023 17:22:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Google Deepmind å‘å¸ƒ Gemini 1.0 ï¼Œåœ¨ 32 é¡¹æŒ‡æ ‡ä¸­æœ‰ 30 é¡¹è¶…è¿‡ GPT-4ã€‚Gemini ä»æ¶æ„åˆ°è®­ç»ƒè¿‡ç¨‹éƒ½æ˜¯åŸç”Ÿçš„å¤šæ¨¡æ€ï¼Œå¯ä»¥æ”¯æŒå›¾åƒã€è§†é¢‘ã€è¯­éŸ³è¾“å…¥ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥ç›´æ¥è¾“å‡ºå›¾åƒå’Œæ–‡å­—ï¼Œè€Œ GPT-4 ç›®å‰è¿˜ä¸èƒ½ç›´æ¥è¾“å‡ºå›¾åƒã€‚å…¶ä¸­ 1.8B å’Œ 3.25B å‚æ•°çš„æ¨¡å‹æ€§èƒ½éƒ½è¶…è¿‡äº† llama-2-7b ï¼Œå¯ä»¥è¿è¡Œåœ¨ç§»åŠ¨è®¾å¤‡ä¸Šã€‚</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FyajlfcVdvQUFFVHFHLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Fya1VaVFc0QUEzdkZ2LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0Fya3k4clhFQUFZREV0LmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FybVphdFdVQUEwX0h5LmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1731820918811341151#m</id>
            <title>åˆšå‘å¸ƒçš„ Difussion model ï¼Œè¾“å…¥ä¸€å¼ å°å§å§å‚è€ƒå›¾å’Œå§¿åŠ¿ï¼Œå°±èƒ½ç”Ÿæˆé€¼çœŸçš„å°å§å§è·³èˆè§†é¢‘ã€‚æ¯”åŒç±»å‹æ¨¡å‹çš„æ‹ŸçœŸç¨‹åº¦æå‡äº† 37%ï¼Œæ•ˆæœè‚‰çœ¼å¯è§ã€‚</title>
            <link>https://nitter.cz/quinn_leng/status/1731820918811341151#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1731820918811341151#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 04 Dec 2023 23:40:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>åˆšå‘å¸ƒçš„ Difussion model ï¼Œè¾“å…¥ä¸€å¼ å°å§å§å‚è€ƒå›¾å’Œå§¿åŠ¿ï¼Œå°±èƒ½ç”Ÿæˆé€¼çœŸçš„å°å§å§è·³èˆè§†é¢‘ã€‚æ¯”åŒç±»å‹æ¨¡å‹çš„æ‹ŸçœŸç¨‹åº¦æå‡äº† 37%ï¼Œæ•ˆæœè‚‰çœ¼å¯è§ã€‚</p>
<p><a href="https://nitter.cz/_akhaliq/status/1731754853238501522#m">nitter.cz/_akhaliq/status/1731754853238501522#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1730749076554305843#m</id>
            <title>R to @quinn_leng: å¯¹äºå¾ˆå¤šå¼€æº LLM æ¥è¯´è¿™ä¸ªæ¼æ´å¹¶æ²¡æœ‰ä»€ä¹ˆå½±å“ï¼Œä½†æ˜¯å¯¹äº OpenAI GPT è¿™ç§é—­æºæ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®å¯èƒ½æ¶‰åŠå•†ä¸šæœºå¯†ç”šè‡³ç‰ˆæƒå®˜å¸ï¼Œæ³„æ¼è®­ç»ƒæ•°æ®å°±å¾ˆæ•æ„Ÿäº†ã€‚</title>
            <link>https://nitter.cz/quinn_leng/status/1730749076554305843#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1730749076554305843#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 00:41:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>å¯¹äºå¾ˆå¤šå¼€æº LLM æ¥è¯´è¿™ä¸ªæ¼æ´å¹¶æ²¡æœ‰ä»€ä¹ˆå½±å“ï¼Œä½†æ˜¯å¯¹äº OpenAI GPT è¿™ç§é—­æºæ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®å¯èƒ½æ¶‰åŠå•†ä¸šæœºå¯†ç”šè‡³ç‰ˆæƒå®˜å¸ï¼Œæ³„æ¼è®­ç»ƒæ•°æ®å°±å¾ˆæ•æ„Ÿäº†ã€‚</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1730748799025549397#m</id>
            <title>ç ”ç©¶æ˜¾ç¤ºé€šè¿‡è®© LLM ä¸æ–­é‡å¤ä¸€ä¸ªå…³é”®è¯å¯ä»¥ä»å®ƒå˜´ä¸­å¥—å‡ºè®­ç»ƒæ•°æ®ï¼Œæ¯”å¦‚æŸä½ç ”ç©¶äººå‘˜ä¸Šä¼ åˆ°äº’è”ç½‘çš„é‚®ç®±å’Œç”µè¯ï¼Œç”šè‡³æ˜¯å‡ åƒå­—çš„æ–‡ç« åŸæ–‡ï¼ˆæˆªå›¾4ï¼‰ã€‚ç›®å‰è¿™ä¸ªæ¼æ´å·²ç»è¢« OpenAI å±è”½ï¼Œä½†æ˜¯å½“æ—¶ç ”ç©¶äººå‘˜ç ´è§£è¿‡ç¨‹çš„èŠå¤©è®°å½•è¿˜èƒ½è¢«æ‰¾åˆ°ï¼šhttps://chat.openai.com/share/456d092b-fb4e-4979-bea1-76d8d904031f ã€‚è®ºæ–‡åŸæ–‡ï¼šhttps://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html?utm_source=aitidbits.substack.com&amp;utm_medium=newsletter</title>
            <link>https://nitter.cz/quinn_leng/status/1730748799025549397#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1730748799025549397#m</guid>
            <pubDate></pubDate>
            <updated>Sat, 02 Dec 2023 00:40:44 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>ç ”ç©¶æ˜¾ç¤ºé€šè¿‡è®© LLM ä¸æ–­é‡å¤ä¸€ä¸ªå…³é”®è¯å¯ä»¥ä»å®ƒå˜´ä¸­å¥—å‡ºè®­ç»ƒæ•°æ®ï¼Œæ¯”å¦‚æŸä½ç ”ç©¶äººå‘˜ä¸Šä¼ åˆ°äº’è”ç½‘çš„é‚®ç®±å’Œç”µè¯ï¼Œç”šè‡³æ˜¯å‡ åƒå­—çš„æ–‡ç« åŸæ–‡ï¼ˆæˆªå›¾4ï¼‰ã€‚ç›®å‰è¿™ä¸ªæ¼æ´å·²ç»è¢« OpenAI å±è”½ï¼Œä½†æ˜¯å½“æ—¶ç ”ç©¶äººå‘˜ç ´è§£è¿‡ç¨‹çš„èŠå¤©è®°å½•è¿˜èƒ½è¢«æ‰¾åˆ°ï¼š<a href="https://chat.openai.com/share/456d092b-fb4e-4979-bea1-76d8d904031f">chat.openai.com/share/456d09â€¦</a> ã€‚è®ºæ–‡åŸæ–‡ï¼š<a href="https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html?utm_source=aitidbits.substack.com&amp;utm_medium=newsletter">not-just-memorization.githubâ€¦</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FUWS1fcmJVQUFVVUdnLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FUWkI0UWE4QUFKSVluLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FUWnZVbWIwQUFGM0pfLmpwZw==" />
<img src="https://nitter.cz/pic/enc/bWVkaWEvR0FUYXBPMWJFQUFXVUUzLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/GregKamradt/status/1727018183608193393#m</id>
            <title>RT by @quinn_leng: Claude 2.1 (200K Tokens) - Pressure Testing Long Context Recall

We all love increasing context lengths - but what's performance like?

Anthropic reached out with early access to Claude 2.1 so I repeated the â€œneedle in a haystackâ€ analysis I did on GPT-4

Here's what I found:

Findings:
* At 200K tokens (nearly 470 pages), Claude 2.1 was able to recall facts at some document depths
* Facts at the very top and very bottom of the document were recalled with nearly 100% accuracy
* Facts positioned at the top of the document were recalled with less performance than the bottom (similar to GPT-4)
* Starting at ~90K tokens, performance of recall at the bottom of the document started to get increasingly worse
* Performance at low context lengths was not guaranteed

So what:
* Prompting Engineering Matters - Itâ€™s worth tinkering with your prompt and running A/B tests to measure retrieval accuracy
* No Guarantees - Your facts are not guaranteed to be retrieved. Donâ€™t bake the assumption they will into your applications
* Less context = more accuracy - This is well know, but when possible reduce the amount of context you send to the models to increase its ability to recall
* Position Matters - Also well know, but facts placed at the very beginning and 2nd half of the document seem to be recalled better

Why run this test?:
* Iâ€™m a big fan of Anthropic! They are helping to push the bounds on LLM performance and creating powerful tools for the world
* As a practitioner of LLMs, itâ€™s important to build an intuition for how they work, where they excel and their limits
* Tests like these, while not bulletproof, help showcase real world examples and get a feeling for how they work. The goal is to transfer this knowledge to productive use cases

Overview of the process:
* Use Paul Graham essays as â€˜backgroundâ€™ tokens. With 218 essays itâ€™s easy to get up to 200K tokens (repeated essays when necessary)
* Place a random statement within the document at various depths. Fact used: â€œThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.â€
* Ask Claude 2.1 to answer this question only using the context provided
* Evaluate Claude 2.1s answer with GPT-4 using @LangChainAI evals
* Rinse and repeat for 35x document depths between 0% (top of document) and 100% (bottom of document) (sigmoid distribution) and 35x context lengths (1K Tokens > 200K Tokens)

Next Steps To Take This Further:
* For rigor, one should do a key:value retrieval step. However for relatability I did a San Francisco line within PGs essays for clarity and practical relevance
* Repeat test multiple times for increased statistical significance

Notes:
* Amount Of Recall Matters - The model's performance is hypothesized to diminish when tasked with multiple fact retrievals or when engaging in synthetic reasoning steps
* Changing your prompt, question, fact to be retrieved and background context will impact performance
* The Anthropic team reached out and offered credits to repeat this test. They also offered prompt advice to maximize performance. It's important to clarify that their involvement was strictly logistical. The integrity and independence of the results were maintained, ensuring that the findings reflect my unbiased evaluation and are not influenced by their support.
* This test cost ~$1,016 for API calls ($8 per million tokens)</title>
            <link>https://nitter.cz/GregKamradt/status/1727018183608193393#m</link>
            <guid isPermaLink="false">https://nitter.cz/GregKamradt/status/1727018183608193393#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 17:36:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Claude 2.1 (200K Tokens) - Pressure Testing Long Context Recall<br />
<br />
We all love increasing context lengths - but what's performance like?<br />
<br />
Anthropic reached out with early access to Claude 2.1 so I repeated the â€œneedle in a haystackâ€ analysis I did on GPT-4<br />
<br />
Here's what I found:<br />
<br />
Findings:<br />
* At 200K tokens (nearly 470 pages), Claude 2.1 was able to recall facts at some document depths<br />
* Facts at the very top and very bottom of the document were recalled with nearly 100% accuracy<br />
* Facts positioned at the top of the document were recalled with less performance than the bottom (similar to GPT-4)<br />
* Starting at ~90K tokens, performance of recall at the bottom of the document started to get increasingly worse<br />
* Performance at low context lengths was not guaranteed<br />
<br />
So what:<br />
* Prompting Engineering Matters - Itâ€™s worth tinkering with your prompt and running A/B tests to measure retrieval accuracy<br />
* No Guarantees - Your facts are not guaranteed to be retrieved. Donâ€™t bake the assumption they will into your applications<br />
* Less context = more accuracy - This is well know, but when possible reduce the amount of context you send to the models to increase its ability to recall<br />
* Position Matters - Also well know, but facts placed at the very beginning and 2nd half of the document seem to be recalled better<br />
<br />
Why run this test?:<br />
* Iâ€™m a big fan of Anthropic! They are helping to push the bounds on LLM performance and creating powerful tools for the world<br />
* As a practitioner of LLMs, itâ€™s important to build an intuition for how they work, where they excel and their limits<br />
* Tests like these, while not bulletproof, help showcase real world examples and get a feeling for how they work. The goal is to transfer this knowledge to productive use cases<br />
<br />
Overview of the process:<br />
* Use Paul Graham essays as â€˜backgroundâ€™ tokens. With 218 essays itâ€™s easy to get up to 200K tokens (repeated essays when necessary)<br />
* Place a random statement within the document at various depths. Fact used: â€œThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.â€<br />
* Ask Claude 2.1 to answer this question only using the context provided<br />
* Evaluate Claude 2.1s answer with GPT-4 using <a href="https://nitter.cz/LangChainAI" title="LangChain">@LangChainAI</a> evals<br />
* Rinse and repeat for 35x document depths between 0% (top of document) and 100% (bottom of document) (sigmoid distribution) and 35x context lengths (1K Tokens > 200K Tokens)<br />
<br />
Next Steps To Take This Further:<br />
* For rigor, one should do a key:value retrieval step. However for relatability I did a San Francisco line within PGs essays for clarity and practical relevance<br />
* Repeat test multiple times for increased statistical significance<br />
<br />
Notes:<br />
* Amount Of Recall Matters - The model's performance is hypothesized to diminish when tasked with multiple fact retrievals or when engaging in synthetic reasoning steps<br />
* Changing your prompt, question, fact to be retrieved and background context will impact performance<br />
* The Anthropic team reached out and offered credits to repeat this test. They also offered prompt advice to maximize performance. It's important to clarify that their involvement was strictly logistical. The integrity and independence of the results were maintained, ensuring that the findings reflect my unbiased evaluation and are not influenced by their support.<br />
* This test cost ~$1,016 for API calls ($8 per million tokens)</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9lWXJESWFBQUFzV1ZwLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/DrJimFan/status/1727023421899620642#m</id>
            <title>RT by @quinn_leng: Instead of taking OAI's merger offer, Anthropic launched major updates for Claude 2.1ğŸ‰. I think the below chart is the most interesting: this is how all LLM papers that claim "long context" should report: error rates on "Beginning", "Middle", and "End".

There're a bunch of papers making wild claims, all the way up to "1B context tokens". Here's a friendly reminder that the 30-year-old LSTM literally supports infinite context. It's a meaningless number unless you show detailed evaluations at different locations in the context. LLMs tend to be "Lost in the Middle", i.e. struggle to remember and reason on information at the middle section of the context window: https://arxiv.org/abs/2307.03172

Claude 2.1 also claims "2x hallucination" - please take this with a BIG grain of salt. A while back, I expressed my concerns about Vectara's benchmarking protocol. Same concerns apply here too. 

The trivial solution to achieve 0% hallucination is simply refusing to answer every query. One cannot claim victory here without a careful Safety vs Usefulness analysis. How many questions that Claude used to answer correctly are now rejected?

In any case, kudos to Dario &amp; Anthropic team on assuring us a solid alternative during turmoil! ğŸ©·https://www.anthropic.com/index/claude-2-1</title>
            <link>https://nitter.cz/DrJimFan/status/1727023421899620642#m</link>
            <guid isPermaLink="false">https://nitter.cz/DrJimFan/status/1727023421899620642#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 17:57:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Instead of taking OAI's merger offer, Anthropic launched major updates for Claude 2.1ğŸ‰. I think the below chart is the most interesting: this is how all LLM papers that claim "long context" should report: error rates on "Beginning", "Middle", and "End".<br />
<br />
There're a bunch of papers making wild claims, all the way up to "1B context tokens". Here's a friendly reminder that the 30-year-old LSTM literally supports infinite context. It's a meaningless number unless you show detailed evaluations at different locations in the context. LLMs tend to be "Lost in the Middle", i.e. struggle to remember and reason on information at the middle section of the context window: <a href="https://arxiv.org/abs/2307.03172">arxiv.org/abs/2307.03172</a><br />
<br />
Claude 2.1 also claims "2x hallucination" - please take this with a BIG grain of salt. A while back, I expressed my concerns about Vectara's benchmarking protocol. Same concerns apply here too. <br />
<br />
The trivial solution to achieve 0% hallucination is simply refusing to answer every query. One cannot claim victory here without a careful Safety vs Usefulness analysis. How many questions that Claude used to answer correctly are now rejected?<br />
<br />
In any case, kudos to Dario & Anthropic team on assuring us a solid alternative during turmoil! ğŸ©·<a href="https://www.anthropic.com/index/claude-2-1">anthropic.com/index/claude-2â€¦</a></p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9lWmZQSmJnQUE2WmVOLnBuZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1727013248602640734#m</id>
            <title>Anthropic æœç„¶è¿˜æ˜¯ OpenAI çš„æœ€å¼ºå¯¹æ‰‹ï¼ŒClaude 2.1 ç‰ˆæœ¬å¢åŠ åˆ°äº† 200k ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶å¢åŠ äº†éå¸¸å®ç”¨çš„ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ï¼Œä»¥åŠ system prompt ï¼ŒåŒæ—¶å›ç­”å‡†ç¡®åº¦ä¹Ÿæœ‰äº†æ˜¾è‘—æå‡ã€‚æ¨¡å‹æ•´ä½“æ°´å¹³è·Ÿ OpenAI æ¥è¿‘äº†ä¸€å¤§æ­¥</title>
            <link>https://nitter.cz/quinn_leng/status/1727013248602640734#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1727013248602640734#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 17:16:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Anthropic æœç„¶è¿˜æ˜¯ OpenAI çš„æœ€å¼ºå¯¹æ‰‹ï¼ŒClaude 2.1 ç‰ˆæœ¬å¢åŠ åˆ°äº† 200k ä¸Šä¸‹æ–‡ï¼ŒåŒæ—¶å¢åŠ äº†éå¸¸å®ç”¨çš„ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ï¼Œä»¥åŠ system prompt ï¼ŒåŒæ—¶å›ç­”å‡†ç¡®åº¦ä¹Ÿæœ‰äº†æ˜¾è‘—æå‡ã€‚æ¨¡å‹æ•´ä½“æ°´å¹³è·Ÿ OpenAI æ¥è¿‘äº†ä¸€å¤§æ­¥</p>
<p><a href="https://nitter.cz/AnthropicAI/status/1727001773888659753#m">nitter.cz/AnthropicAI/status/1727001773888659753#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1726764513905848597#m</id>
            <title>å¦ä¸€ä¸ªæˆ‘è¿™ä¸¤å¤©ä¸€ç›´åœ¨æ€è€ƒçš„é—®é¢˜æ˜¯ï¼Œäººç±»åœ¨ AI é¢å‰æ˜¾å¾—å¤šè„†å¼±ï¼Œä¸–ç•Œä¸Šæœ€æœ‰å‰æ™¯çš„å…¬å¸å‡ ä¸ªå°æ—¶ä¹‹å†…è¢«ä¸‰ä¸ªäººæå¾—ä¸ƒé›¶å…«è½ã€‚å¦‚æœäººç±»çœŸçš„ç­ç»äº†ï¼Œå¤§æ¦‚ç‡åº”è¯¥æ˜¯è‡ªå·±æç ¸äº†ï¼Œè€Œä¸æ˜¯å› ä¸º AGI</title>
            <link>https://nitter.cz/quinn_leng/status/1726764513905848597#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1726764513905848597#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Nov 2023 00:48:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>å¦ä¸€ä¸ªæˆ‘è¿™ä¸¤å¤©ä¸€ç›´åœ¨æ€è€ƒçš„é—®é¢˜æ˜¯ï¼Œäººç±»åœ¨ AI é¢å‰æ˜¾å¾—å¤šè„†å¼±ï¼Œä¸–ç•Œä¸Šæœ€æœ‰å‰æ™¯çš„å…¬å¸å‡ ä¸ªå°æ—¶ä¹‹å†…è¢«ä¸‰ä¸ªäººæå¾—ä¸ƒé›¶å…«è½ã€‚å¦‚æœäººç±»çœŸçš„ç­ç»äº†ï¼Œå¤§æ¦‚ç‡åº”è¯¥æ˜¯è‡ªå·±æç ¸äº†ï¼Œè€Œä¸æ˜¯å› ä¸º AGI</p>
<p><a href="https://nitter.cz/oran_ge/status/1726748365109825689#m">nitter.cz/oran_ge/status/1726748365109825689#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1726712806702203168#m</id>
            <title>è¿™æ¬¡æ ¸å¿ƒæˆå‘˜å‡ºèµ°ï¼Œä»¥åŠå¤§éƒ¨åˆ†å‘˜å·¥è”åè¦æ±‚è‘£äº‹ä¼šæ”¹ç»„ï¼Œå½±å“æœ‰äº›è¿‡äºå·¨å¤§ï¼Œé‰´äºå¾®è½¯åœ¨å…¶ä¸­æ‹¥æœ‰çš„å†³å®šæ€§èµ„æºï¼Œåé¢ä¼šå‘è°å€¾æ–œéƒ½ä¸å¥½è¯´ï¼Œæˆ‘ç¡®å®æŒºæ‹…å¿ƒ OpenAI çš„å—å½±å“ç¨‹åº¦</title>
            <link>https://nitter.cz/quinn_leng/status/1726712806702203168#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1726712806702203168#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 21:23:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>è¿™æ¬¡æ ¸å¿ƒæˆå‘˜å‡ºèµ°ï¼Œä»¥åŠå¤§éƒ¨åˆ†å‘˜å·¥è”åè¦æ±‚è‘£äº‹ä¼šæ”¹ç»„ï¼Œå½±å“æœ‰äº›è¿‡äºå·¨å¤§ï¼Œé‰´äºå¾®è½¯åœ¨å…¶ä¸­æ‹¥æœ‰çš„å†³å®šæ€§èµ„æºï¼Œåé¢ä¼šå‘è°å€¾æ–œéƒ½ä¸å¥½è¯´ï¼Œæˆ‘ç¡®å®æŒºæ‹…å¿ƒ OpenAI çš„å—å½±å“ç¨‹åº¦</p>
<p><a href="https://nitter.cz/onenewbite/status/1726573171678331345#m">nitter.cz/onenewbite/status/1726573171678331345#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1726712045884826108#m</id>
            <title>OpenAI å‘å®¶äºéç›ˆåˆ©å­¦æœ¯æœºæ„ï¼Œè¯ç”Ÿä»¥æ¥å°±ä¸€ç›´æœ‰è¿™ä¸ªå·¨å¤§çš„å†…åœ¨å†²çªï¼šå­¦æœ¯ä¸Šå¯¹å®‰å…¨è¡Œï¼Œç¨³å®šæ€§ï¼Œå¹³ç­‰å’Œé€ ç¦å…¨äººç±»çš„è¿½æ±‚ï¼Œä»¥åŠä½œä¸ºä¸€å®¶æˆé•¿æœ€å¿«çš„åœ¨å·¨å¤´ä¹‹é—´æ–¡æ—‹çš„åˆ›ä¸šå…¬å¸éœ€è¦æœ€å¿«é€Ÿåº¦å‘å¸ƒäº§å“ï¼Œè·å¾—æœ€å¤§åˆ©æ¶¦ï¼Œé”å®šä¸‹ä¸€é˜¶æ®µçš„å²è¯—çº§å·¨é‡èµ„æºã€‚è¿™ä¸¤è€…éå¸¸å®¹æ˜“äº§ç”Ÿå‰§çƒˆçŸ›ç›¾ã€‚è¿™æ¬¡çš„ç»“å±€ç¡®å®æœ‰äº›è®©äººå”å˜˜</title>
            <link>https://nitter.cz/quinn_leng/status/1726712045884826108#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1726712045884826108#m</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Nov 2023 21:20:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>OpenAI å‘å®¶äºéç›ˆåˆ©å­¦æœ¯æœºæ„ï¼Œè¯ç”Ÿä»¥æ¥å°±ä¸€ç›´æœ‰è¿™ä¸ªå·¨å¤§çš„å†…åœ¨å†²çªï¼šå­¦æœ¯ä¸Šå¯¹å®‰å…¨è¡Œï¼Œç¨³å®šæ€§ï¼Œå¹³ç­‰å’Œé€ ç¦å…¨äººç±»çš„è¿½æ±‚ï¼Œä»¥åŠä½œä¸ºä¸€å®¶æˆé•¿æœ€å¿«çš„åœ¨å·¨å¤´ä¹‹é—´æ–¡æ—‹çš„åˆ›ä¸šå…¬å¸éœ€è¦æœ€å¿«é€Ÿåº¦å‘å¸ƒäº§å“ï¼Œè·å¾—æœ€å¤§åˆ©æ¶¦ï¼Œé”å®šä¸‹ä¸€é˜¶æ®µçš„å²è¯—çº§å·¨é‡èµ„æºã€‚è¿™ä¸¤è€…éå¸¸å®¹æ˜“äº§ç”Ÿå‰§çƒˆçŸ›ç›¾ã€‚è¿™æ¬¡çš„ç»“å±€ç¡®å®æœ‰äº›è®©äººå”å˜˜</p>
<p><a href="https://nitter.cz/onenewbite/status/1726570213263503556#m">nitter.cz/onenewbite/status/1726570213263503556#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/burkaygur/status/1725626154231492684#m</id>
            <title>RT by @quinn_leng: Live demo of Real-Time Image Generation powered by >@fal_ai_data  on @huggingface Spaces

Link below</title>
            <link>https://nitter.cz/burkaygur/status/1725626154231492684#m</link>
            <guid isPermaLink="false">https://nitter.cz/burkaygur/status/1725626154231492684#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 21:25:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Live demo of Real-Time Image Generation powered by <a href="https://nitter.cz/fal_ai_data" title="fal (Features &amp; Labels)">@fal_ai_data</a>  on <a href="https://nitter.cz/huggingface" title="Hugging Face">@huggingface</a> Spaces<br />
<br />
Link below</p>
<img src="https://nitter.cz/pic/enc/ZXh0X3R3X3ZpZGVvX3RodW1iLzE3MjU2MjU2MjAzMDQ4ODc4MDgvcHUvaW1nLzZGTTV6VWxiaTF6OXhBaXcuanBn" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1725621405327454627#m</id>
            <title>Sam Altman ç¦»å¼€ OpenAI ï¼Œå‡ºè¿™ç§æ–°é—»ï¼Œçœ‹æ ·å­å…¬å¸ board å‡ºå¤§äº‹äº†å•Š</title>
            <link>https://nitter.cz/quinn_leng/status/1725621405327454627#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1725621405327454627#m</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Nov 2023 21:06:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>Sam Altman ç¦»å¼€ OpenAI ï¼Œå‡ºè¿™ç§æ–°é—»ï¼Œçœ‹æ ·å­å…¬å¸ board å‡ºå¤§äº‹äº†å•Š</p>
<img src="https://nitter.cz/pic/enc/bWVkaWEvRl9LamhOMmJzQUFmaU9fLmpwZw==" />
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://nitter.cz/quinn_leng/status/1724260450463093120#m</id>
            <title>GPTs ä¸€å‡ºæ¥å°±æœ‰ä¸€å †æƒ³åšå¯¼èˆªç«™çš„ï¼Œå¤šå¦‚ç‰›æ¯›ï¼Œä½œä¸ºä¸€ä¸ª GPT å¼€å‘è€…ä¸€æ—¶ä¸çŸ¥é“å»å“ªä¸ªå¯¼èˆªä¸Šå‘å¸ƒ GPTã€‚æ˜¯æ—¶å€™åšä¸€ä¸ª GPT å¯¼èˆªç«™çš„å¯¼èˆªç«™äº†</title>
            <link>https://nitter.cz/quinn_leng/status/1724260450463093120#m</link>
            <guid isPermaLink="false">https://nitter.cz/quinn_leng/status/1724260450463093120#m</guid>
            <pubDate></pubDate>
            <updated>Tue, 14 Nov 2023 02:58:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    <p>GPTs ä¸€å‡ºæ¥å°±æœ‰ä¸€å †æƒ³åšå¯¼èˆªç«™çš„ï¼Œå¤šå¦‚ç‰›æ¯›ï¼Œä½œä¸ºä¸€ä¸ª GPT å¼€å‘è€…ä¸€æ—¶ä¸çŸ¥é“å»å“ªä¸ªå¯¼èˆªä¸Šå‘å¸ƒ GPTã€‚æ˜¯æ—¶å€™åšä¸€ä¸ª GPT å¯¼èˆªç«™çš„å¯¼èˆªç«™äº†</p>
<p><a href="https://nitter.cz/FinanceYF5/status/1724013556285456826#m">nitter.cz/FinanceYF5/status/1724013556285456826#m</a></p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>