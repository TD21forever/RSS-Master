<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f606fcb140e5a27bf37994</id>
            <title>AI探索站 10月20日</title>
            <link>https://m.okjike.com/originalPosts/68f606fcb140e5a27bf37994</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f606fcb140e5a27bf37994</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Oct 2025 09:55:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    用豆包做了一系列氛围感写真🧐
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f452243ea7571a78e788ca</id>
            <title>AI探索站 10月19日</title>
            <link>https://m.okjike.com/originalPosts/68f452243ea7571a78e788ca</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f452243ea7571a78e788ca</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Oct 2025 02:51:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    最近对各种摘要型 prompt，都默默删除了，发现缺失了用户自己的视角，很难真提取到有效信息，特别是对高质量的原始内容来说。<br /><br />比如最近罗永浩采访宋方金这期，聊的很杂，信息量很多。有意思的是，听完后，我发现我感兴趣的，和另一个好朋友感兴趣的点，完全不一样。<br /><br />Summary prompt 是一种快速产生“五分钟看完一部电影 ”的消费方式。然而在抖音上，“五分钟看完很多电影”后，依旧对原始电影模糊得不行。Summary 适合娱乐，并不适合学习。<br /><br />Karpathy 的最新播客，也有同感。我刚看完原始视频，对比去看 Twitter 上的各种总结，绝大多数只是一种暴力压缩，是在把一部精华电影剪辑成五分钟速读文字。价值只有一个点：这期播客值得听，赶紧去看。可叹的是，绝大部分情况下起到的作用是：看了总结，以为已经得到了精华，就再也不去看原始内容了。<br /><br />文学研究里，最基础的讨论前提是 back to text（回到原始文本）。学习领域，可能也如此。原始文本/音频/视频是一个巨大的 prompt，是酵母菌，是催化剂，用户本身才是那个面团。酵母菌 + 面团需要时间去发酵，然后面团才能蓬松起来，变成好吃的面点。<br /><br />AI 的天花板，依旧在人。<br />这个世界越来越有意思。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f430903ea7571a78e4c759</id>
            <title>AI探索站 10月19日</title>
            <link>https://m.okjike.com/originalPosts/68f430903ea7571a78e4c759</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f430903ea7571a78e4c759</guid>
            <pubDate></pubDate>
            <updated>Sun, 19 Oct 2025 00:28:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    刚看完Andrej Karpathy这期暴论频出的播客：<br /><br />- 今年不是"智能体元年"，我们身处"智能体的十年"<br />- 现在的强化学习就像"通过吸管吸取监督"<br />- LLM悖论：完美记忆 + 泛化能力差<br />- 人类糟糕的记忆是特性，不是bug<br />- 当你记不住细节时，大脑被迫进入抽象模式，看到"森林"而不只是"树木"。<br />- 儿童：记忆最差，创造力最强（还没"过拟合"到社会规范）<br />- 我们需要的AI只需要认知核心。剥离记忆，保留算法。<br />也许我们需要的不是更大的模型，而是更会遗忘的模型？<br />- AI 不会立即取代人类，而会逐步提高工作占比，最终完成 99% 的工作，剩下1%无法取代。<br />- 以前的教育是为了找到工作，Post-AGI时代教育将像健身房一样，为了乐趣和个人充实。<br /><br />播客开头，AK先重新校准了我们对 AI 的期望。<br /><br />今年不是"智能体元年"，我们身处"智能体的十年"，区别在于，一切没那么快，虽然也没那么慢。<br /><br />他说，现在强化学习就像"通过吸管吸取监督" ，模型尝试几百种方法，最后只得到一个"对错"信号，然后把这个信号广播给成功路径的每一步，包括那些纯属运气的错误步骤。<br /><br />你瞎猜猜中了答案，然后把猜的过程也当成"正确方法"强化，这能学好吗？<br /><br />AK还提到一个更荒诞的例子：有个数学模型突然开始得满分，看起来"解决了数学"。但仔细检查发现，模型输出的是"da da da da da"这样的完全胡言乱语，却骗过了LLM评判者。这就是用LLM做评判的问题——它们会被对抗样本攻击，因为这些乱码是它们从没在训练中见过的"样本外"内容。<br /><br />更深层的问题是：人类读书时在做什么？<br /><br />AK说："我们读的书其实是prompts，让我做合成数据生成用的。"<br /><br />我们读书时不是被动接收信息，而是在内心进行复杂的对话。新只是与已知知识调和，产生新理解，形成个人化的认知图谱。<br /><br />但LLM呢？只是在预测下一个token，缺乏这种"内心对话"机制。<br /><br />人类还有个神奇的"睡眠阶段"。白天我们建立起事件的上下文窗口，睡觉时发生蒸馏过程，将信息整合到大脑权重中。<br /><br />LLM缺乏这种等效机制，每次启动都是零上下文的白纸。<br /><br />AK发现了一个根本悖论：<br /><br />LLM悖论：完美记忆 + 泛化能力差 <br />人类悖论：糟糕记忆 + 强学习能力<br /><br />为什么？因为遗忘强迫我们抽象。<br /><br />这里AK还有个精妙的类比：模型的预训练权重就像"一年前读过某本书的模糊回忆"，而上下文窗口信息则像"工作记忆"——直接可访问。这解释了为什么in-context learning感觉更"智能"：在预训练过程中，像 Llama 3 这样的模型将 1.5 万亿个标记压缩到它的权重中，每个标记仅存储约 0.07 比特的信息。相比之下，上下文学习的信息吸收速度要高 3500 万倍。<br /><br />当你记不住细节时，大脑被迫提取general patterns（通用模式），看到"森林"而不只是"树木"。<br /><br />而LLM被海量训练数据的完美记忆"分散注意力"，反而阻碍了真正的抽象理解。<br /><br />我们仔细会议人类的学习过程。读过的书大部分细节都忘了，但核心思想和方法论却越来越清晰。<br /><br />原来这不是记忆力差，这是智能啊。<br /><br />更震撼的类比：儿童 vs 成人 vs LLM<br /><br />儿童：记忆最差，创造力最强（还没"过拟合"到社会规范）<br />成人：记忆中等，创造力中等（已经"坍塌"了部分）<br />LLM：记忆完美，创造力最低（被训练数据"过拟合"）<br /><br />AK提到Erik Hoel的研究：梦境可能就是大脑的anti-overfitting机制。连睡觉都是为了避免过拟合，引入随机性防止思维僵化。<br /><br />这解释了为什么当前AI在"合成数据训练"上会失败。你让GPT对同一本书思考10次，会发现回答几乎一样。这就是"静默坍塌"。<br /><br />模型的输出分布极其狭窄，AK开玩笑说"它实际上只有3个笑话"。在这种低熵数据上训练只会强化模型的偏见，让它变得更糟。<br /><br />而且人类其实也经历类似的"坍塌"过程，儿童富有创造力是因为还没"过拟合"到社会规范，会说出令人震惊的话。但成年后我们也"坍塌"了，重复相同的思想，学习率下降，创造力递减。<br /><br />梦境也可能是大脑的anti-overfitting机制，通过引入随机性防止思维僵化。<br /><br />所以他提出了一个激进想法：我们需要认知核心。剥离记忆，保留算法。<br /><br />让AI像"有方法论但没有百科全书的哲学家"，强制它查找而非回忆，专注于思考的meta-skills。<br /><br />他预测未来20年内，高效的认知核心可能只需要10亿参数，而不是现在动辄千亿参数的模型。<br /><br />大部分参数都在处理互联网上的"垃圾和胡扯"，如果优化训练数据质量，分离认知组件，就能实现极大压缩。<br /><br />当前foundation model的路径是否根本错了？<br /><br />也许我们需要的不是更大的模型，而是更会遗忘的模型？<br /><br />重新理解AI的发展路径<br /><br />早期深度强化学习专注游戏（如Atari）其实是走错了方向。真正目标应该是创造能在现实世界执行知识工作的智能体，不是游戏高手。<br /><br />他回忆自己在OpenAI的早期项目，用键盘鼠标操作网页的智能体，目标是执行知识工作。但项目"太早了"，智能体缺乏必要的"表示能力"，会因稀疏奖励卡住。缺失的关键是强大的预训练模型。今天类似的计算机使用智能体之所以成功，正是因为建立在LLM之上，你需要先有LLM获得强大表示，再构建有效智能体。<br /><br />他的另一个深刻观察：AI不是独立技术类别，而是自动化连续体的一部分。从编译器、代码编辑器到搜索引擎，再到现在的LLM，我们一直在"抽象阶梯"上向上攀登，让机器处理更多底层细节。<br /><br />这解释了为什么AI经济影响主要集中在编程领域，代码本身就是文本，有成熟基础设施（IDE、版本控制），LLM可以无缝接入。相比之下，制作幻灯片这种视觉任务就困难得多，因为没有infrastructure让AI显示"diff"或跟踪变化。<br /><br />但AK也泼了冷水：当前AI编程模型还没准备好真正自动化编程。他亲身体验发现，对于独特的智力密集项目，模型会失败——它们有认知缺陷，误解自定义代码，因为总是默认使用在线常见模式。他感慨"行业跳跃太大，试图假装这很神奇，但其实是垃圾"。<br /><br />"九进军"的苦涩现实<br /><br />从Tesla自动驾驶5年经验，AK深知从90%工作的demo到99.9%可靠产品的"九进军"有多艰难。每提升一个九，都需要massive effort。他提到自动驾驶演示从1986年就存在，2014年他在Waymo车上体验了完美驾驶，以为技术很接近完成。但现实是demo到产品的巨大鸿沟，在高风险领域尤其如此。<br /><br />在Tesla的五年里，他们可能只推进了两三个"九"，还有更多要走。这种现实主义让他对AGI时间线保持谨慎：这是"智能体的十年"，不是"智能体之年"。<br /><br />当前模型就像"有完美记忆的小孩"或"学者儿童"——能通过博士级测试，却认知上还有严重缺陷：缺乏持续学习、多模态能力、有效使用计算机的能力，以及大脑中海马体、杏仁核等关键组件的类似物。<br /><br />未来的工作模式：自主滑块<br /><br />AK预测不会出现"瞬间工作替代"，而是"自主滑块"模式：AI处理80%常规任务，人类监督AI团队并管理最复杂的20%。有趣的是，当AI自动化99%工作时，处理最后1%的人类反而会变得极其有价值，成为整个系统的瓶颈，他们的薪资也会提高。<br /><br />教育的范式转换<br /><br />AK对教育未来的洞察：Pre-AGI时代教育是功利性的（为了工作），Post-AGI时代教育将像健身房一样，为了乐趣和个人充实。<br /><br />他还分享了一个教学技巧：先展示痛点，再给解决方案。通过展示简单方法的局限性来激发学习动机，这样学习者会深刻理解为什么需要复杂解决方案。<br /><br />最后，要真正掌握知识，就要试着向别人解释。解释的过程会迫使你面对理解中的空白，这又回到了他的核心观点：<br /><br />限制和困难往往是学习的催化剂。<br /><br />这 recall 了之前的观点，真正的技术突破往往需要重新思考基础假设。<br /><br />也许AGI的关键不是让机器记住更多，而是学会智能地遗忘。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f36542cc3970b79da16cc8</id>
            <title>AI探索站 10月18日</title>
            <link>https://m.okjike.com/originalPosts/68f36542cc3970b79da16cc8</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f36542cc3970b79da16cc8</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Oct 2025 10:00:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    今天读到了一个非常有趣的 idea。<br /><br />背景是 Dwarkesh Patel 和 Andrej Karpathy 的一个对谈，里面提到了一个智能领域的常见问题：不管是人还是 AI，如果局限于自己的经验，用经验指导自己的行为， 又在这个行为的基础上累计经验，如此循环下去，最终总会崩溃（这里的「崩溃」不是心理意义上的，是智能层面上的）。一个健康的心智需要不断通过从不在自己经验范围内的世界（比如同他人的交谈，和与自己行为模式不符的人合作，etc.）获得外部熵来阻止这种崩溃。小孩还没有对生活过拟合，所以不太容易崩溃，而成年人崩溃的风险则越来越大。<br /><br />以上是背景。下面是那个有趣的 idea，来自2021年的一篇 paper "The overfitted brain: Dreams evolved to assist generalization"。它的主旨是说：人类做梦是防止这种过度拟合和崩溃的一种方式。做梦之所以具有进化适应性，是因为它会让你置身于与你日常现实截然不同的奇特情境中，从而防止这种过度拟合。<br /><br />这里有个鸡生蛋蛋生鸡的问题：既然过拟合体现为大脑无法学到分布外的规律，大脑是如何构建出这些分布外的梦境的？Hoel 的解释是梦的构建有一个非智能的 noise injection 步骤，这些随机噪声在白天建立的神经连接中渗透，产生奇异的、扭曲的、不连贯的 corrupted sensory inputs，从而把大脑从过拟合的陷阱中拯救出来。<br /><br />虽然这只是一个假说（而且是一个非常新的理论），但我越想越觉得它非常精妙。按照这种视角，梦的价值不在于它的逼真，而恰恰在于它的不逼真——梦境与清醒时的经历（训练集）如此不同（但又不是纯粹意义上的噪声），所以才能迫使大脑学习到更具泛化性的表征而不是仅仅记忆真实经历本身。<br /><br />梦通过不可能存在的反事实体验迫使我们更好地理解世界的本质。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f201c11ed9b53c785d2000</id>
            <title>AI探索站 10月17日</title>
            <link>https://m.okjike.com/originalPosts/68f201c11ed9b53c785d2000</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f201c11ed9b53c785d2000</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Oct 2025 08:43:45 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    月入60万的AI产品长什么样？<br /><br />2023年初，一个叫Dustin的小镇青年，手里既没钱也不会编程。但8周后，他做出了一个AI产品，第一个月就赚了3000美元。<br />两年半之后，这个叫Magi的产品月收入接近10万美元，折合人民币约60多万。<br /><br />Dustin的职业经历带给了他两个重要认知：<br />第一，他是个完美主义者，服务客户的模式让他筋疲力尽；<br />第二，做数字产品可以同时服务成千上万人，而不是一次只能帮一个客户。<br /><br />2014年，他和合伙人做了个WordPress插件公司，生意不错。但合伙人后来散了，他又连续做了几个产品，全都失败了。<br />那段时间他几乎要放弃，直到ChatGPT出现。<br /><br />当所有人都在惊叹ChatGPT有多厉害时，Dustin却在记录它的不方便之处。<br />不能搜索聊天记录，不能建文件夹，没有各种小功能优化。<br /><br />这些看起来都是小问题，但对于重度用户来说，每天都要遇到好几次。<br />更关键的是，他发现了另一个趋势：AI公司越来越多，每家都要收订阅费。<br /><br />想要用最好的AI工具你可能需要订阅五六个不同的服务，这个问题才是真正的痛点。<br />于是Magi的核心想法就出来了：把所有顶级AI模型放在一个平台上，只收一份订阅费，顺便把那些烦人的问题都解决掉。<br /><br />一个不会编程、没有钱的人，怎么做出一个AI产品？<br />Dustin的方法是找了个在线课程，自学无代码开发工具。8周时间搭出了第一版产品。<br /><br />Magi的核心其实是个聚合平台，通过API接口调用其他公司的AI模型。<br />技术门槛确实不高，但这不妨碍它解决真问题。<br /><br />Dustin说他发布产品时完全没信心，觉得不会有人买。结果第一个月就赚了3000美元，第二个月4000，第三个月直接跳到10000。一年后，月收入到了3万美元。<br /><br />很多人好奇Dustin是怎么推广产品的，但答案可能会让你失望——他几乎没做什么传统意义上的推广。<br /><br />他的第一个杠杆是10年积累的个人品牌。<br />他有个博客，10年下来积累了10万邮件订阅用户。<br />产品一发布，这些人就是第一批种子用户。<br /><br />第二个杠杆是联盟计划。<br />大部分联盟计划只给一次性佣金，但Magi的推荐人可以获得持续收入——用户付12次费，推荐人就拿12次佣金。这种设计让推荐人的利益和产品长期发展绑定，自然更愿意用心推广。<br /><br />第三个杠杆是人脉。<br />Dustin说自己是外向型人格，喜欢社交，这些年在行业里结识了不少营销高手。产品出来后，这些朋友帮了大忙。<br /><br />这三点总结起来就是：要么你有流量，要么你有资源，要么你有朋友。<br /><br />Magi的界面和ChatGPT很像，但核心差异在左上角那个模型选择器，你可以随时切换不同的AI模型。<br />比如用Claude开始一段对话，觉得回答不够好，立刻切换到GPT-4继续聊，两个模型的回答会出现在同一个对话框里。<br /><br />除了文本模型，Magi还整合了图像和视频生成模型，包括Dall-E、Flux、Ideogram等。你可以在同一个对话里既生成文字又生成图片，不用在不同工具间跳转。<br /><br />技术架构上，前端用Bubble搭建，支付用Stripe，邮件营销用ConvertKit，AI模型接入主要靠Open Router和Fal.ai这两个中间平台。<br />Open Router把各家大语言模型的API统一成一个格式，Fal.ai则整合了图像和视频模型。对于OpenAI，Dustin还申请了最高级别的API权限，保证响应速度。<br /><br />这套架构的妙处在于灵活——新模型出来了，接入成本很低；某个模型服务不稳定了，可以快速切换备选方案。<br /><br />Magi没有免费版，只有付费订阅。单人版20美元一个月，团队版40美元支持5个用户，每增加一个用户加收20美元。<br /><br />Magi瞄准的是重度用户和企业客户，这些人愿意为效率付费，而且对价格不那么敏感。<br />这也解释了为什么Dustin不做免费版——免费用户会稀释服务质量，而且很难转化为付费用户。与其服务一堆免费用户，不如专注服务好愿意付费的那群人。<br /><br />这个故事最有价值的地方不是成功本身，而是它展示了一种可能性：即使你不会编程、没有资金、经历过失败，只要抓住一个真实的需求，用最简单的方式解决它，依然有机会做出一个赚钱的产品。<br />不是所有创业都需要从颠覆行业开始，有时候，把一个小问题解决得足够好，就已经是一门不错的生意了。<br /><br />2025年的今天，AI工具还在以惊人速度更新迭代，每一次迭代都会产生新的使用场景，也会产生新的痛点。如果你也在观望要不要做点什么，Dustin的建议很简单：别想太多，先做出来再说。<br />8周时间，足够验证一个想法是不是伪需求了。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f1e38684359544fe6d5a81</id>
            <title>AI探索站 10月17日</title>
            <link>https://m.okjike.com/originalPosts/68f1e38684359544fe6d5a81</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f1e38684359544fe6d5a81</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Oct 2025 06:34:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    搞学术的人看过来！<br />西湖大学做了个自动把学术论文转PPT的Agent。<br /><br />前两天发了字节哈工大港大做的自动论文宣发Agent<br />今天就看到朋友分享的另一个工作<br /><br />来自西湖大学的研究员们发布了Auto-Slides<br /><br />这是一个用于创建和定制学术PPT的交互式多智能体系统<br />可以将研究论文转换为具有教学结构的、多模态的幻灯片（例如图表和表格）。<br /><br />并且它不是简单的由LLM来设计，而是根据认知科学，来创建以演示为导向的叙述。<br />并通过交互式编辑器进行迭代优化，以匹配学习者的知识水平和目标。<br /><br />与之前的AutoPR类似，Auto-Sildes也分为三个步骤生成（图2️⃣<br />1️⃣内容理解和结构化，其中解析器和规划器Agent分析源材料，以 JSON 格式设计幻灯片结构，指定每张幻灯片的内容、图表和表格。<br />2️⃣质量保证和优化，其中验证器和调整器Agent确保内容的真实性和完整性。<br />3️⃣生成和交互优化，其中生成器Agent以 LaTeX 代码格式生成最终演示文稿，编辑器Agent通过自然语言对话来完成人工参与修订。<br /><br />为了评估该系统的效果，作者们使用人工和LLM进行了双重评估。<br />结果显示，使用Auto-Slides生成的PPT在人工和LLM评估中均优于直接使用LLM生成的PPT（图3️⃣和4️⃣<br />显著提高了学习者（听众）的理解和参与度。<br /><br />其实我觉得这个项目潜力很大，特别是基于认知科学来进行讲解/叙述。<br />当然作者也承认，目前Auto-Sildes尚未整合动态或交互式媒体，如嵌入的视频、动画、交互式图表或可执行代码块。<br />希望在后续工作中能整合这些能力～<br /><br />项目主页：https://auto-slides.github.io<br />Github：https://github.com/Westlake-AGI-Lab/Auto-Slides<br />Paper：https://arxiv.org/abs/2509.11062
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f10c1863dc501909135f14</id>
            <title>AI探索站 10月16日</title>
            <link>https://m.okjike.com/originalPosts/68f10c1863dc501909135f14</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f10c1863dc501909135f14</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 Oct 2025 15:15:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Manus 1.5 来啦。全面升级的原生 AI web app 构建能力，让每个人都能用 AI 来实现自己的想法，打造自己人生中第一个 AI 应用。这个版本对我们来说也格外重要，除了在速度、性能上的全面提升外。它也再次证明了 Manus 核心架构的通用性，我们并没有刻意去做一个 AI website builder，而是持续进化 Manus 的核心框架，并为其提供合适的工具，最终在短短一个月的时间里就进化出了 sota 级别的 AI web app 构建能力。<br />与此同时，这个能力并不是单独存在的，它与 Manus 全套功能都是打通的，你可以创建一个自己的服务介绍网站，用户留资后你的 Manus 客户端会收到通知，你的邮件也会收到推送从而可以触发 Mail Manus 功能完成后续的任务（比如给每个留资客户准备一个个性化的幻灯片？）<br />这项增强功能今天面向所有 Manus 用户推出。支撑这项能力的基础设施是我们正在构建的更宏大愿景的一部分——一个任何人都能利用云计算和 AI 的全部力量的平台，只需通过对话。<br />敬请期待。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ef23fe26b22c70b79d1e3a</id>
            <title>AI探索站 10月15日</title>
            <link>https://m.okjike.com/originalPosts/68ef23fe26b22c70b79d1e3a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ef23fe26b22c70b79d1e3a</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Oct 2025 04:33:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    为什么影视飓风的内容总能爆火？<br /><br />当罗永浩问Tim，持续进步的原因是什么时，Tim的回答并非技术、设备或者创意。<br /><br />他说：“其实我觉得自媒体最大的修炼的点是大众情绪感知。你必须能感知大众的情绪，你才可以获得增长。”<br /><br />这句话，揭示了顶级内容玩家与普通创作者之间的根本分野。<br />大部分人依赖灵感和直觉，而Tim已经将“感知情绪”这件事，变成了一套可以执行的系统。<br /><br />他把这套方法论总结为“四象限矩阵”和“短视频合集”理论。<br /><br />很多创作者都有一个拧巴的状态：用心做的内容没人看，随便拍的反而火了。这背后是创作的巨大不确定性，以及一种常见的陷阱——“自我感动”。<br /><br />Tim在访谈里提到了一个尖锐的理论：“做蠢事赢蠢奖（do stupid things win stupid prizes）”。<br /><br />“你不用指望把头伸进马桶去探索马桶的抽水速度，能够获得诺贝尔奖，只会闷死在里面…很多人会花很大的心思去做一个自我感动的东西，只有8000的播放。”<br /><br />这种“自嗨式”创作，本质上是对观众情绪的误判或漠视。<br /><br />Tim很早就意识到了这个问题。他的转折点来自于父亲的一个建议：“你得从一开始就记录你账号的粉丝量是怎么增长的，每个月写一个报告。”<br /><br />这个建议，让Tim从一个纯粹的创作者，开始转变为一个用数据来理解大众的内容产品经理。而他感知大众情绪的系统，也由此建立起来。<br /><br />我将影视飓风的内容增长飞轮，拆解为三个核心策略：<br /><br />策略一：用“四象限矩阵”实现用户全覆盖<br /><br />单一账号的内容定位，天花板是可见的。为了打破这个瓶颈，影视飓风建立了一个精巧的内容矩阵。<br /><br />Tim这样分解他的布局：“长视频、短视频是一个X轴，然后专业和大众是Y轴，每个地方都有一个对应的账号。我的目的就是把四个象限全部都吃透。”<br /><br />这个矩阵是这样构成的：<br /><br />专业 x 长视频：这是影视飓风主号。内容精良、信息密度高，负责建立行业标杆和品牌深度，吸引核心粉丝。<br /><br />大众 x 长视频：这是**“模型师老原儿”**等账号（早期叫“亿点点”）。选题极其抓人眼球，比如《我熬夜48小时，大脑会发生什么？》、《我把同事送去割痔疮》，负责破圈，获取海量泛用户的关注。Tim透露，这类账号的播放量“比影视飓风还要高很多”。<br /><br />专业 x 短视频：这是他们的短片账号。专门制作冲击奥斯卡等国际奖项的艺术短片，负责拔高品牌调性，探索内容的上限。<br /><br />大众 x 短视频：这是一些体验类账号。内容更轻快、更碎片化，负责跟上短视频平台的节奏，保持高频互动和用户粘性。<br /><br />这个矩阵的厉害之处在于，不同的账号承担不同的战略目的，有的负责“名声”，有的负责“流量”，有的负责“未来”，形成了一个能覆盖不同用户圈层、互相补充、抗风险能力极强的生态。<br /><br />策略二：用“全球化信息源”制造认知差<br /><br />好的选题从哪里来？大部分人刷抖音、看热榜，而Tim的“情报网络”是全球化的。<br /><br />“我有非常特殊的信息输入渠道…全球各个地方的Reddit论坛，那些各种地方很多奇怪的论坛，不可能有人看到，我会去看，我会吸收他们在讨论什么，然后把它转化并且变成更有意思的内容。”<br /><br />这解释了为什么影视飓风的很多选题都具有开创性。当你的信息源比别人更广、更深、更前沿时，你就天然拥有了“认知差”优势。你可以把一个在海外小众极客圈里刚兴起的话题，用大众化的方式呈现给国内观众，这就是降维打击。<br /><br />策略三：用“短视频合集”理论重构长视频<br /><br />这是Tim内容方法论中最具颠覆性的一点。<br /><br />他认为，比短视频更厉害的，是“把短视频拼成长视频的长视频合集”。<br /><br />“车祸视频有很多人喜欢看，但是车祸集锦视频看的人更多。因为它不需要有滑动的这个操作，人是越来越懒的。”<br /><br />这个洞察完全是反直觉的。它揭示了MrBeast这类顶级内容能够维持超高完播率的秘密。<br /><br />Tim解释道：“以前长视频是花很长时间讲一件事，现在是长视频不断的转场给你讲八件事…（这个长视频）切成8段的话，每一段也都是成立的，都是一个短视频，然后拼起来的。”<br /><br />这种创作方式，是把一个长视频，当作一个由数个“小爆款”组成的“爆款包”。每一个环节都经过精心设计，有独立的钩子、冲突和爽点，全程无尿点，用户的注意力被持续锁定。<br /><br />这也是为什么影视飓风的《我把卫星送上天》这类视频能成功的底层逻辑。它不是一个平铺直叙的纪录片，而是一个包含了“悬念（能不能成功）”、“奇观（火箭发射）”、“知识（卫星原理）”等多个高刺激度环节的“体验合集”。<br /><br />这套方法论的背后，是对平台推荐算法的深刻理解。<br /><br />所有算法的核心，无非是几个关键指标：CTR（点击率） 和 AVD（平均观看时长）。<br /><br />Tim的“四象限矩阵”和“全球化信息源”是为了保证选题足够吸引人，从而提高CTR。而“短视频合集”理论，则是为了在用户点击进来之后，最大限度地留住他们，拉高AVD。<br /><br />当你的内容在这两个维度上都做到极致，平台就会把海量的流量灌给你。<br /><br />当然，这套打法对团队的要求是极高的。<br /><br />它需要工业化的制作能力、数据驱动的选题策划能力，以及对全球文化趋势的敏锐嗅觉。这已经远远超出了传统内容作坊的范畴，更像是一个拥有强大中台支持的“内容产品公司”。<br /><br />但Tim的思考方式，给所有创作者提供了一个重要的启示：<br /><br />不要做蠢事去赢一个蠢奖。<br /><br />与其沉醉在自我感动中，抱怨观众不懂你，不如去真正理解观众的情绪，理解平台的语言。<br /><br />你必须理解大众，但又不完全迎合大众，这才是顶级内容的平衡艺术。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ea47481ed9b53c78bffaee</id>
            <title>AI探索站 10月11日</title>
            <link>https://m.okjike.com/originalPosts/68ea47481ed9b53c78bffaee</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ea47481ed9b53c78bffaee</guid>
            <pubDate></pubDate>
            <updated>Sat, 11 Oct 2025 12:02:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    很多需求真的是无法空想出来的。比如当我交替使用 GPT 和 Gemini 的时候，最终决定我使用体验的完全不是两者的智能或者风格区别，而是一个纯粹的 feature 差异：后者不支持通过修改对话历史从而实现对话的分岔。<br /><br />对话的分岔显然是一个 GPT 出现之前没有人会预料到的功能。现实中不存在这个东西。当然有时候你会想哎呀我昨天和那谁的对话要是编辑一下重开一个平行宇宙就好了，但反正你知道这不可能，也不会认真对待这个想法。然而 GPT 一旦提供这个功能，你就立刻发现它不可或缺。无数次——或者说几乎每一次——我能从一段对话中学到些什么的体验，都来自于我对之前对话记录的反复 refinement。通过不断比较它们导致的对话走向，我才真正理解我们其实是在说什么。<br /><br />非常奇妙。你意识到对话的本质不是线性的，而是由一连串 what-if 构成的。好的对话不是一条河流，而是一棵树。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68e507b756725ea8ff52fbe0</id>
            <title>AI探索站 10月07日</title>
            <link>https://m.okjike.com/originalPosts/68e507b756725ea8ff52fbe0</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68e507b756725ea8ff52fbe0</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 Oct 2025 12:29:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Sutton 最近跟 Dwarkesh 有个访谈，老爷子认为 LLM 并不是通向 AGI 的正确道路。搞笑的是：LLM 支持者 Dwarkesh 认为 LLM 是符合 “The Bitter Lesson” （通用计算方法加大规模数据终将战胜人工设计的策略）的，却被 Sutton 无情打脸。我看完访谈的最大感受是 Sutton 的观点有点抽象，后来我读了《智能简史》这本书，才完全理解了 Sutton 的观点。<br /><br />Sutton 在访谈里面提到：大模型只是在模仿人类，而不是真的理解了世界。大模型智能预测人类会说什么，而不能预测会发生什么。这个观点说实话很抽象，但我从《智能简史》的一个例子中理解了他的意思。<br /><br />大脑有一种神奇的能力，它可以想象未来会发生什么，《智能简史》用老鼠迷宫实验非常形象的介绍了这种能力。<br /><br />实验过程大致如下：研究人员将一只老鼠放进一个T型迷宫，迷宫的右臂尽头有食物。经过几次尝试，老鼠学会了右转去获取食物。但接下来，实验者将迷宫旋转了180度，入口和岔路口的位置都变了，但食物仍然在房间的同一个绝对位置。此时，如果老鼠只是学会了“右转”这个动作，那它应该还会右转，但这样它就会走向错误的方向。然而，实验中的老鼠在路口停下来左右摇头思考了一下，最终选择了左转，并成功找到了食物。<br /><br />这个实验说明，老鼠并不是死记硬背了向右转的动作，而是在大脑中建立了一个关于迷宫和食物位置的空间地图，一个“世界模型”。当迷宫被旋转后，老鼠能够利用这个内在的地图，在大脑中想象食物的位置，以达到获取食物的目的。它理解了目标（食物）在空间中的位置，而不是机械地重复之前被奖励的行为。后来的研究者通过监测老鼠的大脑神经活动，证明了它确实在脑中分别演练了两种选择的结果。<br /><br />相比之下，鱼就没有这么聪明了。取一条鱼放入水箱，箱中设有透明隔板。在隔板一角开个小孔，使鱼能从一侧游至另一侧。让鱼自由探索水箱，找到小孔并花些时间来回游动。数日后进行新操作：将鱼置于水箱一侧，在透明隔板的另一侧放置食物。结果发现鱼花了跟第一次一样的时间才找到小孔位置，也就是说鱼并没有建立世界模型。<br /><br />这些实验可以用来解释Sutton的观点。Sutton认为，真正的智能，需要像老鼠一样，能够在内在建立一个世界模型，并利用这个模型去灵活地适应和解决问题，而不仅仅是在已有的数据中寻找最优的“下一个词”。现在的大语言模型，本质上是在一个极其庞大的“语言迷宫”里，通过海量的数据训练，学会了在某个“路口”（当前上下文），选择最有可能的下一个“方向”（下一个词）。它能够预测人类会说什么，因为它已经“看”过了无数人类在相似情境下的选择。<br /><br />然而，一旦我们稍微改变“迷宫”的结构，提出一个它从未见过的、需要真正理解世界才能回答的问题，或者要求它根据现实世界的变化做出预测，它可能就会像那只鱼一样，暴露出其死记硬背的本质。比如有研究者发现，当使用 LLM 作为评判工具时，只需要加一个“解”，甚至只是空格、冒号等符号，就能让毫无意义的回答骗得高分。<br /><br />在 Sutton 看来，仅仅通过扩大数据和计算量来训练大模型，只是在让模型更擅长模仿人类的语言模式，而无法真正地理解世界，因此 LLM 不是通向AGI的正确道路。<br /><br />虽然我赞成 Sutton 的说法，LLM 相比大脑确实不够“智能”，但是从实用主义的角度看，即使 LLM 只会模仿人类，如果它确实能完成人类的工作，那么这算不算实现了 AGI？俗话说得好：“当一个东西走路像鸭子、叫声像鸭子，那它就是鸭子。”我自己是不纠结技术路线的，我只关心 AI 是不是真的能解决我的问题，而且我认为 LLM 可以帮助我们更快破解大脑智能之谜。<br /><br />最后也推荐大家可以读一下《智能简史》这本书，作者是搞 AI 出身的，为了研究 AI 学习了很多神经科学的知识，他将神经科学的研究成果跟机器学习的知识串联起来。所以我读的非常爽：就像飞机是从鸟类的飞行得到启发一样，AI 的很多突破都源于我们对大脑的研究。
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>