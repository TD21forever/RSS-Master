<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68a84eac2393a294a6985c54</id>
            <title>AI探索站 08月22日</title>
            <link>https://m.okjike.com/originalPosts/68a84eac2393a294a6985c54</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68a84eac2393a294a6985c54</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Aug 2025 11:04:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    来个乐子：百度官方的AI助手已经移除了自家文心大模型的支持，只剩下DeepSeek的2个开源版本可供选择。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68a82f8f6d3795b7b53d1857</id>
            <title>AI探索站 08月22日</title>
            <link>https://m.okjike.com/originalPosts/68a82f8f6d3795b7b53d1857</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68a82f8f6d3795b7b53d1857</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Aug 2025 08:51:27 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    被荣耀应用的各种证书、密钥等概念折磨一周后，我决定写个教程来造福后人。<br /><br />应该是全网写得最清晰的非技术人士能看懂，甚至能顺着我的思路用AI编程开发出来的鸿蒙教程了。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68a7dc589788e80d37d0294d</id>
            <title>AI探索站 08月22日</title>
            <link>https://m.okjike.com/originalPosts/68a7dc589788e80d37d0294d</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68a7dc589788e80d37d0294d</guid>
            <pubDate></pubDate>
            <updated>Fri, 22 Aug 2025 02:56:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Vercel 也开始做 AI 模型的 API 业务了<br /><br />AI Gateway 通过统一 API 接入，支持数百种模型和多家服务商，自动处理鉴权、限流、故障切换、用量追踪和账单，无需开发者手动管理各家 API Key 或切换服务商。<br /><br />没有额外加价，用户可自带 Key 和合同，享受零加价的模型调用。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68a737ed2fad277b48983a3b</id>
            <title>AI探索站 08月21日</title>
            <link>https://m.okjike.com/originalPosts/68a737ed2fad277b48983a3b</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68a737ed2fad277b48983a3b</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Aug 2025 15:14:53 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    今天higgsfield的 draw-to-video非常惊艳，只需上传一张静态图，在上面绘制涂鸦、文字或箭头等元素，即可生成具有电影质感的视频画面，指哪儿打哪儿，一发布就直接爆了。<br /><br />这个效果应该不是训练一个视频编辑模型，因为它可以基于minimax、veo3、seedance来生成的。<br /><br />推测是把用户的涂鸦信息转为prompt，直接给视频模型生成，或者用该prompt生成尾帧，然后基于首尾帧来生成视频<br /><br />其实Higgsfield不是首创了这种涂鸦交互，8月初Jaaz.app就支持在画布上，通过涂鸦、打字、箭头来指挥agent生图，原理就是把用户的涂鸦信息转为prompt，然后结合参考图生成对应的效果图。<br /><br />更早的时候，3月在公司内 AI群里分享过GPT4O的涂鸦P图案例，是用户自发探索出来的能力。估计当时已经有创业公司在思考怎么把这种交互进行产品化，当时写了这篇文档《当大模型越来越像人，人和模型应该如何交互》https://ix5mo5od606.feishu.cn/wiki/S7rMwMCyxiwsO1kQszDcTZcFnFf?from=from_copylink<br /><br />刚刚在Twitter看到AI创作者Brett的一段话，很有感触，在此分享：<br /><br />The best AI artists aren’t just good at prompting—they’re good at directing. <br /><br />They know how to generate, filter, combine, and remix until something original emerges.<br /><br />最出色的 AI 艺术家，不只是会写提示词，更擅长于‘导演’。<br /><br />他们懂得如何生成、筛选、融合与重组，直到独一无二的作品浮现。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68a72ea52fad277b48978ee0</id>
            <title>AI探索站 08月21日</title>
            <link>https://m.okjike.com/originalPosts/68a72ea52fad277b48978ee0</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68a72ea52fad277b48978ee0</guid>
            <pubDate></pubDate>
            <updated>Thu, 21 Aug 2025 14:35:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    最近 Agent 行业发展好像遇到了瓶颈。<br />从 Manus 开始，到 Macaron 结束。<br />如果真是这样的，耗时不到半年，也太快了吧。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68a1507d15068f2cb1ae1cb9</id>
            <title>AI探索站 08月17日</title>
            <link>https://m.okjike.com/originalPosts/68a1507d15068f2cb1ae1cb9</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68a1507d15068f2cb1ae1cb9</guid>
            <pubDate></pubDate>
            <updated>Sun, 17 Aug 2025 03:46:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    这个无限画布 AI 真的太🐮🍺太帅了<br />直接放图，随手画几笔，就能出结果了<br />连输入 Prompt 都省了<br />这是 Way2AGI 社区鹿演的第一个项目Jaaz<br />Jaaz 是一个开源多模态创作AI Agent，首创魔法画布轻松创作内容<br />Jaaz刚刚冲到日榜第二名！特别需要大家的建议和鼓励 ✨<br />🔗:https://www.producthunt.com/leaderboard/daily/2025/8/16/all
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/689b0806ba4e69f5b2cf3a2f</id>
            <title>AI探索站 08月12日</title>
            <link>https://m.okjike.com/originalPosts/689b0806ba4e69f5b2cf3a2f</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/689b0806ba4e69f5b2cf3a2f</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Aug 2025 09:23:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    遇到了一个有趣的问题，正好落在 AI 模型的能力边界处：试证明不可能把平面分成无穷个圆的无交并。<br /><br />在我尝试的所有模型里，只有 GPT 5 thinking model 成功做了出来（虽然花了点时间）。<br /><br />有趣的不是这个结论，而是观察它们的思路。所有失败的模型都有个共同点：它们的思考基本上是从文字到文字的。它们会调用自己脑海中各种已有的定理和知识，然后漫无目的地试图拼凑出一个证明，但所有这些定理，不管是拓扑的还是几何的还是测度的，对它们来说都是纯粹字面意义上的陈述。Qwen 的思考过程最典型：它滔滔不绝想了很久，但很显然从头到尾它都并不真的理解它在说什么。圆也罢，开集闭集也罢，Baire 纲定理也罢，对它来说都是纯粹的概念，给人的感觉是它甚至并不真的知道「圆是圆的」。<br /><br />微妙之处在于，这种「没有几何直觉的几何思考」在某些时候其实未必是一种劣势。现代数学早已挣脱了对三维现实想象的依赖，大部份数学思考本来也确实是在纯粹的概念思辨空间中进行（特别是当问题进入代数乃至范畴论的领域的时候，这时从概念到概念的思考就变成了一种必然）。有的时候，几何直觉甚至反而会成为一种束缚，特别是当思考高维空间的时候，基于低维现实的直观常常是有误导性的。在这些问题上，AI 的「盲目」反而带来了自由，使得它不必受困于视觉直觉。——当然，人类的视觉直觉可能会渗透进人类的文本语料里，在某种程度上「污染」AI，但这是另一个问题。<br /><br />然而对原问题来说，因为这是一个低维问题，直觉在这里不但有用，而且能大大缩短思考搜索的难度。在这一点上，一个把圆只作为抽象概念来理解的 AI 就会有巨大的劣势，因为它无法享受到几何直觉带来的跳步。这种直觉使得人可以一眼「看出」关键的构造，而这种构造在文本层面被搜索出来是困难的。<br /><br />考虑到 AI 的应用毕竟大多数情况下还是为了解决世界现实问题而不是思考高维几何，有几何直觉的 AI 会在大多数问题上显得聪明得多。于是一个现实问题是，这种直觉是只有依赖多模态的训练才能获取，还是可以通过精巧的文本训练就能实现？这有点像是 AI 领域的玛丽房间问题。这是一个经典的知识论思想实验：一个从出生就生活在黑白房间里、精通颜色物理与神经机制的科学家玛丽，当她第一次走出房间看到红色时，她是否获得了新的知识？<br /><br />今天大多数 AI 领域的困难都可以归结于此。人类是自己感官的奴隶，我们听到、看到、闻到，我们体会身体激素的涨落，我们想象、困惑、愤怒，然后试图把这一切投射在文字空间里。AI 则正好相反，它们在文字里理解这一切，但最终需要努力地——有时候是徒劳地——明白，一个圆在什么意义上是圆的。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68956c79369da0a84be8c3a2</id>
            <title>AI探索站 08月08日</title>
            <link>https://m.okjike.com/originalPosts/68956c79369da0a84be8c3a2</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68956c79369da0a84be8c3a2</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Aug 2025 03:18:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    ✨ GPT-5 初体验<br /><br />作为ChatGPT 的深度同行者，我在GPT-5 发布这个重要节点，选择了最朴素的交流方式—— 模拟大学生询问它有什么特别之处？ GPT-5  以极快的速度输出了一张场景匹配表，隐约揭示出我们正站在新的分水岭。<br /><br />随后，想到了ChatGPT最新Study模式正风靡全球，想看看它会怎么介绍给新时代的中学生们；让GPT-5 写了欢迎信进行自我介绍；GPT-5 非常快、而5 Pro的缜密和深度令人印象相当深刻。<br /><br />回想起任天堂刚刚发布了一个独立游戏的直面会还没看，要不先让Agent 帮我梳理一下？  GPT-5 在 Agent 模式花了18分钟，列出了一份详实的清单：好家伙，21个新游、熟悉面孔不多；不过铲子骑士家出的新作《挖掘者米娜》很期待呢<br /><br />打开Open AI 的GPT-5直播发布会，气氛依旧温暖、平等并充满人文气， 其中关于 GPT-5 语音模式的对话意外相当亲切；于是拿起手机端也聊了几句，更智能、也更个人化（调皮），我知道以后的日子里更离不开它了。<br /><br /> GPT-5的发布会如老朋友般松弛，这与当年的苹果式发布会炫酷创新形成鲜明反差。任何一个保持开放、拥抱变化的AI 创造者来说，无论过去三年多么的波澜壮阔，也会在这个时刻有那么点怀旧；Open AI 也找来了早期员工分享2022年底正式发布前，内部还称呼这个产品还叫Chat with GPT的往事，不疾不徐、简约克制。<br /><br />然而，新时代的改变又是极为剧烈的：过去我们与AI对话，现在我们委托AI执行；每天数十个任务ChatGPT Agents ——做研究、写创意文案、完成海量设计和编码。这个全能助手已经彻底融入工作和生活，比移动设备更加亲密和自然。 <br /><br />我知道无法在短短几十分钟揭示GPT-5的全貌， 便让它以自己最擅长的方式—— Deep Research ——进行一次深度回顾。<br /><br />三年巨浪，我们从旁观者成为创造者。这不只是技术进步的见证，更是每个深度用户内心变迁的缩影。 相信你也能在这个回顾中找到自己的心路历程。Enjoy～<br /><br />《从ChatGPT到GPT-5：三年AI革命的全景回顾》<br />https://chatgpt.com/s/dr_6895633264248191913fdb1138eaf853
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68943e5d723686893f13022d</id>
            <title>AI探索站 08月07日</title>
            <link>https://m.okjike.com/originalPosts/68943e5d723686893f13022d</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68943e5d723686893f13022d</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Aug 2025 05:49:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    感觉 OpenAI 最近开源的 gpt-oss 真的被严重低估了。它虽然不是最聪明的，但在使用场景和定位上是非常成功的（服了 Sam 老六）。<br /><br />首先最小尺寸的 gpt-oss:20b 绝对是你在 Mac 或者家用电脑上就能跑的最好模型之一（对我来说是“唯一”）。它是那种少有的“真正能用”的本地模型，对话体验非常好，持续对话下来非常稳定、不会出现输出混乱崩溃的问题。大部分早期能跑的本地模型是不具备的。<br /><br />它的尺寸刚刚好，不管是模型文件还是运行显/内存都非常小（大约是 12～16GB），大部分电脑都能使用。在我的 M4 Mac 上能做到 70token/秒 的输出，在我的 19 年老 Intel 的 Mac 上能做到大约 1 token/秒的输出。整体性能上非常出色。<br /><br />最为重要的是，虽然硬件要求非常低，但它的智能表现出乎意外的好。我把个人最近一些非编程类的问题同时发给 gpt-oss:20b 和元宝（DeepSeek R1），不管是回答速度还是回答质量，我更喜欢 gpt-oss:20b 多一点。这不是严谨的对比测试，我也只尝试了五、六个问题，但考虑到硬件要求，这样的回答效果已经让我感到满意了。<br /><br />如果你的对话需求非常简单，或者想要一个完全离线、隐私自由的本地模型，gpt-oss:20b 绝对是一个简单可靠的选择。<br /><br />其次是最大尺寸的 gpt-oss:120b，OpenAI 号称接近 o4-mini 的水平。理论上它也能在 Mac 或者家用电脑上运行起来。我记得大约需要 60～80 GB 的显/内存，对我的 Mac 来说非常吃力。如果电脑硬件足够强、或者并行几个 Mac Mini 跑起来问题应该不大，这就能拥有一个接近 o4-mini 水平的离线本地模型了。<br /><br />gpt-oss:120b 另外一个被低估的意义是在价格上。在 together.ai 上这个模型的价格低至 $0.15 / $0.60，相比之下 DeepSeek V3（注意是 V3 不是 R1）的价格是 $0.27 / $1.10。也就是说，这么一个号称性能接近 o4-mini 的模型，价格只需要 DeepSeek V3 的一半！！据我所知这样的价格已经是 LLM 中的最低档，类似价格的是 gpt-4o-nano 和 gemini 2.5 flash-lite。<br /><br />我现在非常怀疑 OpenAI 这波就是冲着 DeepSeek 来的……<br /><br />另外我还发现了 gpt-oss 非常容易破解，比其他开源模型简单很多。容易到什么程度呢，我可以在大约三次对话内让它回答各种非法问题。不过这个话题不适合在这里讨论了……
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/689386c4369da0a84bc0d4c6</id>
            <title>AI探索站 08月06日</title>
            <link>https://m.okjike.com/originalPosts/689386c4369da0a84bc0d4c6</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/689386c4369da0a84bc0d4c6</guid>
            <pubDate></pubDate>
            <updated>Wed, 06 Aug 2025 16:45:56 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    👀Gemini Storybook玩了，整体内容输出质量很高，故事情节连贯性和配图质量都符合预期。提了一个指令是“做一个梵高和毕加索绘画风格结合的插图（图7）”，生成物的确融入了毕加索立体主义时期的创作风格。<br /><br />主要是生成的真快，可以想象在哄娃睡觉这个场景下，面对不停提需求的娃，崩溃的家长可以直接打开Gemini寻求帮助了。<br /><br />📝prompt:<br />Using Vincent van Gogh artistic style to create a storybook about Pablo Picasso enthusiastically tries to convince van Gogh to collaboratively draw and commercialize their collaborative work through art dealers. The story should be open ending about the fate of van Gogh and should be based on historical facts. At least one picture in the storybook should demonstrate a collaborative work of van Gogh and Picasso. Targeted readers are females between 20-35 years old.
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>