<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68fa01bd3ea7571a785c4313</id>
            <title>AI探索站 10月23日</title>
            <link>https://m.okjike.com/originalPosts/68fa01bd3ea7571a785c4313</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68fa01bd3ea7571a785c4313</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Oct 2025 10:21:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    量子计算机的设计<br />还真是超出日常的想象<br />有意思<br />（来自 pichai 的推特
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f97a46d9abb9785d59163f</id>
            <title>AI探索站 10月23日</title>
            <link>https://m.okjike.com/originalPosts/68f97a46d9abb9785d59163f</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f97a46d9abb9785d59163f</guid>
            <pubDate></pubDate>
            <updated>Thu, 23 Oct 2025 00:43:50 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    所有liblib的融资新闻稿都刻意回避/弱化了lovart……你说为什么？
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f8fb0400c0686ab5163af4</id>
            <title>AI探索站 10月22日</title>
            <link>https://m.okjike.com/originalPosts/68f8fb0400c0686ab5163af4</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f8fb0400c0686ab5163af4</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Oct 2025 15:40:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    总觉得未来我们每个人会写大量的「意图文档」，大概包含三个模块：  <br /><br />① 最终目标； <br />② 关键约束条件（预算、时间、原则）； <br />③ 成功的衡量标准。  <br /><br />后续的任务拆解和执行，由AI 完成。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f8461a67913e3e94375a80</id>
            <title>AI探索站 10月22日</title>
            <link>https://m.okjike.com/originalPosts/68f8461a67913e3e94375a80</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f8461a67913e3e94375a80</guid>
            <pubDate></pubDate>
            <updated>Wed, 22 Oct 2025 02:48:58 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    这绝对是我见过最清晰、最实用的 AI 工具列表了，用好他们能省下大把时间！<br /><br />日常助手<br />1. Perplexity：AI 搜索引擎和研究助手<br />2. Claude：通用聊天机器人，超适合做项目和分享成果<br />3. ChatGPT：记得开启高级语音模式，直接和 AI 对话<br /><br />效率工具<br />1. Granola：AI 会议笔记工具，自动生成摘要<br />2. Wispr Flow：语音转文字工具，在任何应用里把说的话变成文本<br />3. Gamma：用 AI 快速制作演示稿、文档和网站<br />4. Adobe：和 PDF 对话，还能自动总结内容<br />5. Cubby：协作研究专用的 AI 工作空间<br />6. Cora：AI 邮件助手，帮你智能整理收件箱<br />7. Lindy：搭建 AI 智能体，自动处理工作流程<br /><br />粉丝增长<br />1. Delphi：创建 AI 克隆分身（文字、语音、视频），和粉丝互动<br />2. HeyGen：用 AI 虚拟形象批量制作内容或给视频配音翻译<br />3. Argil：专为社交媒体视频打造的 AI 虚拟形象<br />4. Overlap、Opus：把长视频自动剪成爆款短视频<br />5. Persona：为创作者打造的 AI 智能体搭建工具<br />6. Captions：AI 视频剪辑，自动加字幕还能修正眼神接触<br /><br />产品开发<br />1. Cursor：懂你代码的 AI 编程编辑器<br />2. Replit：用自然语言描述，AI 智能体直接帮你建应用和网站<br />3. Anychat：一站式调用各种 AI 模型<br />4. Codeium：AI 驱动的代码自动补全工具<br /><br />创意设计<br />1. ElevenLabs：逼真的 AI 语音生成<br />2. Suno、Udio：用文字描述直接生成音乐<br />3. Midjourney、Ideogram、Playground：AI 图片生成<br />4. Runway、Kling、Viggle：AI 视频生成<br />5. Krea：创意画布，实时生成和增强图片/视频<br />6. Photoroom：AI 图片编辑器，产品图修图神器<br /><br />学习成长<br />1. Rosebud：交互式日记，用 AI 帮你分析内心想法<br />2. Good Inside：育儿助手，提供个性化支持<br />3. Ada Health：AI 驱动的症状健康评估<br />4. Ash：个性化 AI 心理咨询和教练<br />5. NotebookLM：把任意文档变成 AI 播客<br />6. Particle：AI 新闻应用，多篇文章整合成一篇摘要<br /><br />娱乐玩耍<br />1. Remix：AI 图片/视频创作与分享的社交应用<br />2. Meta Imagine：在 Meta 系应用里生成自己和朋友的 AI 形象<br />3. Grok：xAI 出品的聊天机器人<br />4. Curio：给孩子的 AI 互动对话玩具
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f70a10a79910941039c917</id>
            <title>AI探索站 10月21日</title>
            <link>https://m.okjike.com/originalPosts/68f70a10a79910941039c917</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f70a10a79910941039c917</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Oct 2025 04:20:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    本日最好笑研究：<br /><br />在训练 AI 的时候，如果强迫 AI 大量阅读社交媒体，会对 AI 造成不可逆的脑损伤。<br /><br />与在高质量数据上训练的对照组相比，持续投喂垃圾数据（确切来说是让它拼命刷推）的 LLM 在推理、长上下文理解和安全性能上均表现出明显的衰退。模型在性格测试中表现出精神病态和自恋等特质的得分显著提高。错误分析显示，模型的主要病变是思维跳跃，即越来越倾向于截断或跳过解决问题所需的关键推理链条。<br /><br />研究者比较了什么样的帖子最有「毒性」，发现最好的相关性指标是参与度，也就是一条贴文有多火。最容易病毒式传播的内容也最容易导致脑损伤。<br /><br />这种认知衰退具有持久性。在模型出现脑损伤后，再用高质量的干净数据对其进行指令调整和继续预训练，也只能观察到部分但不完全的治愈。模型的表征漂移仍然存在。<br /><br />你很难不怀疑这个研究是在指桑骂槐。<br /><br />论文地址：https://llm-brain-rot.github.io/
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f6d424d9abb9785d23234f</id>
            <title>AI探索站 10月21日</title>
            <link>https://m.okjike.com/originalPosts/68f6d424d9abb9785d23234f</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f6d424d9abb9785d23234f</guid>
            <pubDate></pubDate>
            <updated>Tue, 21 Oct 2025 00:30:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    DeepSeek的论文每篇都是精品，R1养活了一批研究强化学习的人，OCR这篇意味CV研究员的春天到来了。用图片替代文本输入，确实是很有开创性的想法。DeepSeek真是开源菩萨，换做CloseAI估计要藏一辈子。<br /><br />大模型在处理长文章时，消耗的计算量会爆炸性增长。<br /><br />但如果把文字“画成图片”，模型只需要很少的“视觉 token”就能理解同样内容。<br /><br />就像人看书一样，我们也是靠视觉来阅读文字，如果这个方向靠谱，那么我们就相当于用OCR技术给大模型装上了眼睛。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f666506c58154f4d5d1965</id>
            <title>AI探索站 10月20日</title>
            <link>https://m.okjike.com/originalPosts/68f666506c58154f4d5d1965</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f666506c58154f4d5d1965</guid>
            <pubDate></pubDate>
            <updated>Mon, 20 Oct 2025 16:41:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    最近抖音很火的即梦或者豆包直出三宫格氛围人像照片<br /><br />只需要拿你的照片加上提示词就能搞定，建议用 2:3 比例<br /><br />提示词在下面，整了三套不错的👇：<br /><br />提示词 1：<br /><br />一张以图片人像为主角的三宫格胶片质感艺术感写真图,场景为清晨安静的图书馆,阳光从高窗斜射进来。<br />图中人物和参考图一致,人物和脸不变,衣服为简单的白色毛衣。第一张为近景,上半身背影,人物站在高大的书架前,仰头寻找一本书,添加中英字幕“故事都写在书里吗？-Are all the stories written in books?-”第二张为中景,人物侧身坐在窗边的桌前,阳光照在翻开的书页上,低头看书,添加中英字幕“我好像…读到了别人的脚本。-I seem to be... reading someone else's script.-”第三张为大特写,人物脸部位于画面偏左侧,合上书本,眼神平静地望向窗外的光,添加中英字幕“我的故事,从这一笔开始。-My story begins with this stroke.-”整体色调清冷,带有富士胶片效果,过度曝光,画面粗粝且色调偏冷,暗部细节保留完整,高光区域呈现自然晕化、均采用柔和漫射光,无明显硬边阴影,营造出文艺且充满自我探索情绪的氛围,三张图合成一个三宫格,字幕位于底部居。<br /><br />提示词 2：<br /><br />一张以图片人像为主角的三宫格胶片质感艺术感写真图,场景为古典美术馆的空旷走廊,早晨或傍晚,光线透过拱形窗户洒在地板上。<br />图中人物和参考图一致,人物和脸不变,衣服为简约款的白色针织衫或衬衫。第一张为近景,上半身背影,人物站在一幅巨型画作前,双手插兜,背影显得修长而有艺术气息，添加中英字幕“美,是否有终点？-Does beauty have an end?-”第二张为中景,人物侧身走在长廊上,目光落在墙壁上的雕塑或另一幅画上，光影勾勒出侧脸的轮廓，优雅而富有吸引力，添加中英字幕“我只是,路过每个瞬间。-I merely, pass through every moment.-”第三张为大特写,人物脸部位于画面偏左侧,微抬下巴,眼神略带疑惑却又充满好奇,仿佛在与艺术品对话，展现出一种知性的帅气，添加中英字幕“也许我本身,就是意义。-Perhaps I myself, am the meaning.-”整体色调清冷,带有富士胶片效果,过度曝光,画面粗粝且色调偏冷,暗部细节保留完整,高光区域呈现自然晕化、均采用柔和漫射光,无明显硬边阴影,营造出文艺且充满探索与沉静的氛围,三张图合成一个三宫格,字幕位于底部居中。<br /><br />提示词 3：<br />一张以图片人像为主角的三宫格胶片质感艺术感写真图,场景为霓虹闪烁的城市街道,刚下过雨,地面湿润反光。<br />图中人物和参考图一致,人物和脸不变,衣服为风衣,撑着一把透明的伞。第一张为近景,上半身背影,人物撑伞站在路口,看着对面的红绿灯和穿梭的车流,添加中英字幕“这座城市会为谁停留？-For whom does this city pause?-”第二张为中景,人物在公交站台的玻璃后,侧身看着玻璃上的雨滴,添加中英字幕“每个人都在等一趟车吗？-Is everyone just waiting for a bus?-”第三张为大特写,人物脸部位于画面偏左侧,,脸颊上有一滴分不清是雨水还是泪水的水珠,眼神平静地望向镜头外的霓虹,添加中英字幕“没关系，我的终点是我自己。-It's alright, my destination is myself.-”整体色调清冷,带有富士胶片效果,过度曝光,画面粗粝且色调偏冷,暗部细节保留完整,高光区域呈现自然晕化、均采用柔和漫射光,无明显硬边阴影,营造出文艺且充满自我探索情绪的氛围,三张图合成一个三宫格,字幕位于底部居中
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f36542cc3970b79da16cc8</id>
            <title>AI探索站 10月18日</title>
            <link>https://m.okjike.com/originalPosts/68f36542cc3970b79da16cc8</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f36542cc3970b79da16cc8</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Oct 2025 10:00:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    今天读到了一个非常有趣的 idea。<br /><br />背景是 Dwarkesh Patel 和 Andrej Karpathy 的一个对谈，里面提到了一个智能领域的常见问题：不管是人还是 AI，如果局限于自己的经验，用经验指导自己的行为， 又在这个行为的基础上累计经验，如此循环下去，最终总会崩溃（这里的「崩溃」不是心理意义上的，是智能层面上的）。一个健康的心智需要不断通过从不在自己经验范围内的世界（比如同他人的交谈，和与自己行为模式不符的人合作，etc.）获得外部熵来阻止这种崩溃。小孩还没有对生活过拟合，所以不太容易崩溃，而成年人崩溃的风险则越来越大。<br /><br />以上是背景。下面是那个有趣的 idea，来自2021年的一篇 paper "The overfitted brain: Dreams evolved to assist generalization"。它的主旨是说：人类做梦是防止这种过度拟合和崩溃的一种方式。做梦之所以具有进化适应性，是因为它会让你置身于与你日常现实截然不同的奇特情境中，从而防止这种过度拟合。<br /><br />这里有个鸡生蛋蛋生鸡的问题：既然过拟合体现为大脑无法学到分布外的规律，大脑是如何构建出这些分布外的梦境的？Hoel 的解释是梦的构建有一个非智能的 noise injection 步骤，这些随机噪声在白天建立的神经连接中渗透，产生奇异的、扭曲的、不连贯的 corrupted sensory inputs，从而把大脑从过拟合的陷阱中拯救出来。<br /><br />虽然这只是一个假说（而且是一个非常新的理论），但我越想越觉得它非常精妙。按照这种视角，梦的价值不在于它的逼真，而恰恰在于它的不逼真——梦境与清醒时的经历（训练集）如此不同（但又不是纯粹意义上的噪声），所以才能迫使大脑学习到更具泛化性的表征而不是仅仅记忆真实经历本身。<br /><br />梦通过不可能存在的反事实体验迫使我们更好地理解世界的本质。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f10c1863dc501909135f14</id>
            <title>AI探索站 10月16日</title>
            <link>https://m.okjike.com/originalPosts/68f10c1863dc501909135f14</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f10c1863dc501909135f14</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 Oct 2025 15:15:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Manus 1.5 来啦。全面升级的原生 AI web app 构建能力，让每个人都能用 AI 来实现自己的想法，打造自己人生中第一个 AI 应用。这个版本对我们来说也格外重要，除了在速度、性能上的全面提升外。它也再次证明了 Manus 核心架构的通用性，我们并没有刻意去做一个 AI website builder，而是持续进化 Manus 的核心框架，并为其提供合适的工具，最终在短短一个月的时间里就进化出了 sota 级别的 AI web app 构建能力。<br />与此同时，这个能力并不是单独存在的，它与 Manus 全套功能都是打通的，你可以创建一个自己的服务介绍网站，用户留资后你的 Manus 客户端会收到通知，你的邮件也会收到推送从而可以触发 Mail Manus 功能完成后续的任务（比如给每个留资客户准备一个个性化的幻灯片？）<br />这项增强功能今天面向所有 Manus 用户推出。支撑这项能力的基础设施是我们正在构建的更宏大愿景的一部分——一个任何人都能利用云计算和 AI 的全部力量的平台，只需通过对话。<br />敬请期待。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ea47481ed9b53c78bffaee</id>
            <title>AI探索站 10月11日</title>
            <link>https://m.okjike.com/originalPosts/68ea47481ed9b53c78bffaee</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ea47481ed9b53c78bffaee</guid>
            <pubDate></pubDate>
            <updated>Sat, 11 Oct 2025 12:02:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    很多需求真的是无法空想出来的。比如当我交替使用 GPT 和 Gemini 的时候，最终决定我使用体验的完全不是两者的智能或者风格区别，而是一个纯粹的 feature 差异：后者不支持通过修改对话历史从而实现对话的分岔。<br /><br />对话的分岔显然是一个 GPT 出现之前没有人会预料到的功能。现实中不存在这个东西。当然有时候你会想哎呀我昨天和那谁的对话要是编辑一下重开一个平行宇宙就好了，但反正你知道这不可能，也不会认真对待这个想法。然而 GPT 一旦提供这个功能，你就立刻发现它不可或缺。无数次——或者说几乎每一次——我能从一段对话中学到些什么的体验，都来自于我对之前对话记录的反复 refinement。通过不断比较它们导致的对话走向，我才真正理解我们其实是在说什么。<br /><br />非常奇妙。你意识到对话的本质不是线性的，而是由一连串 what-if 构成的。好的对话不是一条河流，而是一棵树。
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>