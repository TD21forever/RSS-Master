<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f399f83ea7571a78d862b4</id>
            <title>AI探索站 10月18日</title>
            <link>https://m.okjike.com/originalPosts/68f399f83ea7571a78d862b4</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f399f83ea7571a78d862b4</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Oct 2025 13:45:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    第一次看kk的书《失控》快20年了，我的互联网科技启蒙内容之一，这种作者见面直接讨论AI agent的感觉太奇妙了。探讨了很多很有意思的话题，也认识了很有意思的新朋友们，非常开心🥰
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f36542cc3970b79da16cc8</id>
            <title>AI探索站 10月18日</title>
            <link>https://m.okjike.com/originalPosts/68f36542cc3970b79da16cc8</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f36542cc3970b79da16cc8</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Oct 2025 10:00:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    今天读到了一个非常有趣的 idea。<br /><br />背景是 Dwarkesh Patel 和 Andrej Karpathy 的一个对谈，里面提到了一个智能领域的常见问题：不管是人还是 AI，如果局限于自己的经验，用经验指导自己的行为， 又在这个行为的基础上累计经验，如此循环下去，最终总会崩溃（这里的「崩溃」不是心理意义上的，是智能层面上的）。一个健康的心智需要不断通过从不在自己经验范围内的世界（比如同他人的交谈，和与自己行为模式不符的人合作，etc.）获得外部熵来阻止这种崩溃。小孩还没有对生活过拟合，所以不太容易崩溃，而成年人崩溃的风险则越来越大。<br /><br />以上是背景。下面是那个有趣的 idea，来自2021年 Cell 的一篇 paper "The overfitted brain: Dreams evolved to assist generalization"。它的主旨是说：人类做梦是防止这种过度拟合和崩溃的一种方式。做梦之所以具有进化适应性，是因为它会让你置身于与你日常现实截然不同的奇特情境中，从而防止这种过度拟合。<br /><br />这里有个鸡生蛋蛋生鸡的问题：既然过拟合体现为大脑无法学到分布外的规律，大脑是如何构建出这些分布外的梦境的？Hoel 的解释是梦的构建有一个非智能的 noise injection 步骤，这些随机噪声在白天建立的神经连接中渗透，产生奇异的、扭曲的、不连贯的 corrupted sensory inputs，从而把大脑从过拟合的陷阱中拯救出来。<br /><br />虽然这只是一个假说（而且是一个非常新的理论），但我越想越觉得它非常精妙。按照这种视角，梦的价值不在于它的逼真，而恰恰在于它的不逼真——梦境与清醒时的经历（训练集）如此不同（但又不是纯粹意义上的噪声），所以才能迫使大脑学习到更具泛化性的表征而不是仅仅记忆真实经历本身。<br /><br />梦通过不可能存在的反事实体验迫使我们更好地理解世界的本质。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f3163dcc3970b79d9ac1bd</id>
            <title>AI探索站 10月18日</title>
            <link>https://m.okjike.com/originalPosts/68f3163dcc3970b79d9ac1bd</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f3163dcc3970b79d9ac1bd</guid>
            <pubDate></pubDate>
            <updated>Sat, 18 Oct 2025 04:23:25 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    “别再被Demo骗了！”：Karpathy揭秘AI Agent从90%到99.999%的残酷真相<br /><br />当整个科技圈都在为AI Agent（人工智能体）的酷炫Demo欢呼时，有一个人却泼来一盆冷水。<br /><br />他就是Andrej Karpathy——一个履历堪称“AI界天花板”的人物。作为OpenAI的创始成员，他帮助构建了驱动ChatGPT的早期模型；作为前特斯拉AI总监，他领导团队埋头五年，只为攻克自动驾驶这个难题。<br /><br />正是这段横跨“数字智能”与“物理智能”的独特经历，让他对当前AI Agent的狂热，有了一份极其冷静甚至悲观的判断：<br /><br />“这不是智能体元年（the year of agents），而是智能体十年（the decade of agents）。”<br /><br />为什么？因为他用在特斯拉5年、每天都在追求99.999…%可靠性的亲身经历，看到了所有AI产品从惊艳Demo到可靠Product之间，那条由无数个“9”铺成的、漫长而艰难的行军路。<br /><br />一、2014年的“完美”幻觉：一个关于自动驾驶的故事<br /><br />故事要从十年前说起。<br /><br />2014年，Karpathy还在斯坦福读博，通过朋友关系，他体验了一次Waymo的自动驾驶汽车。那是一次“完美”的旅程，车辆在帕洛阿尔托的街道上平稳行驶，毫无差错。<br /><br />“我当时觉得，这东西（自动驾驶）已经非常接近成功了。”<br /><br />然而，十年过去了。自动驾驶依然没有大规模普及，甚至连Waymo自己，也只在少数几个城市的限定区域内运营。<br /><br />问题出在哪？<br /><br />Karpathy在特斯拉的五年找到了答案。他将其总结为一个概念：“九个九的行军”（The march of nines）。<br /><br />一个产品在Demo阶段达到90%的成功率可能很容易，但从90%到99%，再到99.9%，每增加一个“9”的可靠性，所需要付出的努力和资源，都是指数级增长的。<br /><br />“在特斯拉，我们每天都在为小数点后新增的那个‘9’而战。” Karpathy说，“而今天的AI Agent，就像2014年的那辆Waymo，它给了我们一个完美的幻觉，但实际上，它的‘九个九行军’才刚刚开始。”<br /><br />二、AI Agent如何重走自动驾驶之路？四大挑战<br /><br />Karpathy认为，今天的AI Agent，正面临着与自动驾驶当年一模一样的四大挑战。这四大挑战，构成了从Demo到产品之间的巨大鸿沟。<br /><br />挑战一：高昂的失败成本<br /><br />自动驾驶的失败成本是物理世界的人身安全，一次失误就可能导致严重事故。<br /><br />AI Agent的失败成本是什么？是数字世界的业务安全。一个企业级的Agent如果出错，可能会错误地修改数据库、泄露客户隐私、或者给出一个灾难性的商业决策。<br /><br />“在软件工程中，一个微小的错误就可能导致数百万用户的安全漏洞。这种成本，一点也不比自动驾驶低。” Karpathy强调。<br /><br />当失败的代价极其高昂时，99%的可靠性是完全不够的。你需要的，是99.999%甚至更高。<br /><br />挑战二：从90%到99.999%的艰难爬坡<br /><br />为什么提升一个“9”如此困难？<br /><br />因为现实世界充满了“长尾问题”（long tail problems）——那些极其罕见但又确实会发生的极端情况。<br /><br />对自动驾驶来说，可能是突然冲上马路的行人，或是从未见过的交通标识。对于AI Agent，则可能是用户一句带有歧义的指令，或是一个系统从未处理过的异常API返回。<br /><br />解决前90%的问题，靠的是模型的核心能力。而解决后面9.999%的长尾问题，则需要海量的数据、持续的迭代和近乎偏执的细节打磨。<br /><br />“每当你觉得解决了一个问题，就会有一千个新的、更奇怪的问题冒出来。”这正是Karpathy在特斯拉的日常。<br /><br />挑战三：看不见的“人”<br /><br />很多人以为，Waymo的无人车里真的“无人”。但Karpathy指出，这是一种误解。<br /><br />在你看不到的地方，有一个庞大的远程操作中心（tele-operation center），随时准备在车辆遇到困难时接管。<br /><br />“我们并没有完全移除人类，只是把驾驶员从车里，移到了你看不到的办公室里。”<br /><br />AI Agent同样如此。当一个Agent宣称能“全自动”完成任务时，其背后很可能有一个人类团队在进行监督、审核和处理异常。这种“人机协作”的模式，在未来很长一段时间内都将是常态。<br /><br />挑战四：经济可行性<br /><br />最后，也是最现实的问题：成本。<br /><br />自动驾驶的研发成本是天文数字。Waymo烧掉了数百亿美元，至今未能盈利。AI Agent同样面临高昂的训练和推理成本。<br /><br />一个AI产品最终能否成功，不仅仅取决于技术，更取决于它的经济模型。它能否在覆盖高昂成本的同时，创造出足够大的、让客户愿意付费的价值？<br /><br />“你必须让产品在经济上是可行的，否则它就只是一个昂贵的科学实验。”<br /><br />三、结论：乐观的技术，悲观的时间线<br /><br />正是基于这四大挑战的深刻洞见，Karpathy才做出了“智能体十年”的判断。<br /><br />他预测，未来十年，AI Agent不会像很多人想象的那样，以一种颠覆性的姿态瞬间取代人类工作。它会以一种更渐进、更务实的方式渗透到我们的工作流中。<br /><br />他提出了一个“自主性滑块”（autonomy slider）的概念：AI会先从处理80%的常规任务开始，将剩下20%最棘手的任务交给人类。随着技术的成熟，这个“滑块”会慢慢地、一点一点地向100%自主移动。<br /><br />“我对技术本身是乐观的，我相信这些问题终将被解决。” Karpathy在访谈的最后说。<br /><br />“但我对时间线是现实的，甚至有点悲观。我之所以听起来悲观，只是因为当我打开Twitter时，看到太多毫无意义的、为了融资而存在的炒作。”<br /><br />这或许就是Andrej Karpathy带给我们最重要的启发：在一个被AI浪潮席卷的时代，保持兴奋很容易，但保持清醒和耐心，才是一种更稀缺、也更可贵的能力。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f204c9f82cb59d6650ac6a</id>
            <title>AI探索站 10月17日</title>
            <link>https://m.okjike.com/originalPosts/68f204c9f82cb59d6650ac6a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f204c9f82cb59d6650ac6a</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Oct 2025 08:56:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    buer，你们投资圈是怎么混进去这位张口就来的姐啊，豆包日均消耗1.6万亿Tokens，而OpenAI只有1000亿，比这个更离谱的是，当场表演小学算数之后，结论是1.6万亿除以1000亿大于100⋯⋯
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f1e0c6494ea31afe4d25db</id>
            <title>AI探索站 10月17日</title>
            <link>https://m.okjike.com/originalPosts/68f1e0c6494ea31afe4d25db</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f1e0c6494ea31afe4d25db</guid>
            <pubDate></pubDate>
            <updated>Fri, 17 Oct 2025 06:23:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    麦肯锡晒出了 OpenAI 颁发给他们的一个奖章，纪念他们在 GPT 上消耗了超过一千亿 token。<br /><br />陷入沉思：客户们要每月花几十万刀请咨询公司来画 slides，然后咨询公司转手每月花两百刀让 GPT 来画。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f11b6100c0686ab57507eb</id>
            <title>AI探索站 10月16日</title>
            <link>https://m.okjike.com/originalPosts/68f11b6100c0686ab57507eb</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f11b6100c0686ab57507eb</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 Oct 2025 16:20:49 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    很多朋友问我这几天用 Suno 做的音乐是咋做的<br /><br />索性搞了一套万能 Suno 提示词，一键直出！<br /><br />还有新版 Suno V5 的操作教程，两种生成音乐的方式<br /><br />这里阅读不方便的可以去看长文：https://mp.weixin.qq.com/s/_bTK1mARfkQCTb3ekKGN_A<br /><br />这套流程主要有两部分，也是最近爆火的 Suno 生成音乐的两种流派。<br /><br />一种是需要垫原来的音乐的，他还是原来的曲调，支持会重新混音和编曲演绎。<br /><br />另一种是不需要垫原始的音频的纯提示词生成<br />这种就只用原始歌曲的歌词的，曲调会完全根据提示词生成。<br /><br />好了我们开始我们的教程，先教第二种也就是不需要垫音频的版本。<br />这两部分提示词和设置完全相同，只是垫不垫音频的区别。<br /><br />我们进到 Suno 之后，需要先切换到“Custum”这个 Tab 的部分，这边和对生成歌曲做详细的设置。<br /><br />先介绍基础需要关注的部分界面内容：<br /><br />Lyrics：很明显这部分就是歌词，但是不止可以写歌词，你可以对每部分歌词进行标注，放在放括号里面的提示词 AI 是不会唱的，可以理解为控制每部分歌词的提示词。<br /><br />Styles：这部分就是我们认为的的传统提示词了，主要描述整个歌曲的风格、唱法和乐器编排等。<br /><br />Vocal Gender：这个正常就是你生成音乐的演唱是男声还是女声<br /><br />Weirdnes：这个叫怪异值，如果你玩过 MJ 应该很熟悉，数值越高生成的音乐就会越怪，跟主流音乐不同，更加复杂。<br /><br />Style Influence：风格遵循，就是生成音乐需要多遵循你的风格提示词，数值越低，模型的自由度越高。<br /><br />所以你发现了，我们需要写两部分的提示词，首先是风格提示词，另一部分是每段 Verse 的提示词来控制每段提示词的演唱风格。<br /><br />你知道藏师傅的风格的，我向来喜欢一步到位。<br /><br />所以我整了一套提示词，你只要发给 LLM，把你想要模仿的歌手名字和歌曲的歌词给他，他就可以给出这两部分的完整提示词了。<br /><br />比如这里，我把上面的提示词发给了 Gemini 2.5 Pro，然后他就会问我要歌手名称和歌词。<br /><br />然后我就给他发了我想要模仿的歌手名称为李荣浩和黄轩，歌词是《兰亭序》的完整歌词，这里你想让他唱多少就写多少就行。<br /><br />这个时候我们就可以把 LLM 给我们的风格风格提示词和分段指令+歌词传给 Suno 了，分段指令+歌词放在歌词（Lyrics）部分。<br /><br />然后怪异度和风格遵循你可以自己选择，我建议都试试体验一下他们的数值高低的区别。<br /><br />这里歌词可以用已经发行歌曲的歌词，也可以用你自己编的，当然你用“哈基米南北绿豆”也是可以的。<br /><br />然后我们再来看一下第二种，也就是需要上传音频保持原始曲调的音乐怎么做。<br /><br />第一种可以说是除了歌词都是原创，第二种就把 AI 当做一个修音师和混音师来用了，歌曲的编曲和歌词都是不变的，只是音色和编排变了。<br /><br />提示词的部分跟第一部分是一致的，唯一的区别就是我们在“Custum”这边点那个“+Audio”按钮去上传原始的音乐音频，然后选择 Cover 也就是覆盖完全重新演绎，右边的 Extend 是延长的意思，有需要你也可以用。<br /><br />如果你直接拿原始的音乐去上传可能发现了 Suno 是有版权验证的，他不让你混音原始的版权音乐，我们需要绕过去，这里有两个办法：<br /><br />1. 你自己唱一遍原始音乐，你的声音唱他是不会限制你的，或者找别人翻唱的音频，这个版权库没有。<br /><br />2. 然后就是对原始音频进行处理，比如用剪映之类的分离配乐保留人声后剪辑一下，这个不建议使用有风险🐶。<br /><br />然后就是继续我们第一种方案的流程了，用 LLM 提示词生成 Suno 提示词填写到对应的位置，这里就不重复了。<br /><br />需要注意的是你上传音乐之后，下面会多一个选项“Audio Influence”就是跟原始声音的相似度，这里我建议调的低一点，我们只参考音频曲调，不去复刻音色，不然会有风险。<br /><br />当然如果这个音频是你唱的，你可以把这个调高点，把 Suno 当你的调音师用，估计有这个功能之后，会唱歌的美女 UP 主会越来越多了。<br /><br />好了以上就是这次 Suno 的完整教程了。<br /><br />AI 音乐也跟图片模型一样从纯生产逻辑变为了编辑和创作逻辑，整个自由度和准确性搞了很多。<br /><br />Suno V5 可以说是音乐模型中的 banana 了。<br /><br />AI 音乐从 Suno V5 开始已经走到了临界点，甚至可以说音乐行业在这一刻走到了临界点。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68f10c1863dc501909135f14</id>
            <title>AI探索站 10月16日</title>
            <link>https://m.okjike.com/originalPosts/68f10c1863dc501909135f14</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68f10c1863dc501909135f14</guid>
            <pubDate></pubDate>
            <updated>Thu, 16 Oct 2025 15:15:36 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Manus 1.5 来啦。全面升级的原生 AI web app 构建能力，让每个人都能用 AI 来实现自己的想法，打造自己人生中第一个 AI 应用。这个版本对我们来说也格外重要，除了在速度、性能上的全面提升外。它也再次证明了 Manus 核心架构的通用性，我们并没有刻意去做一个 AI website builder，而是持续进化 Manus 的核心框架，并为其提供合适的工具，最终在短短一个月的时间里就进化出了 sota 级别的 AI web app 构建能力。<br />与此同时，这个能力并不是单独存在的，它与 Manus 全套功能都是打通的，你可以创建一个自己的服务介绍网站，用户留资后你的 Manus 客户端会收到通知，你的邮件也会收到推送从而可以触发 Mail Manus 功能完成后续的任务（比如给每个留资客户准备一个个性化的幻灯片？）<br />这项增强功能今天面向所有 Manus 用户推出。支撑这项能力的基础设施是我们正在构建的更宏大愿景的一部分——一个任何人都能利用云计算和 AI 的全部力量的平台，只需通过对话。<br />敬请期待。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ef23fe26b22c70b79d1e3a</id>
            <title>AI探索站 10月15日</title>
            <link>https://m.okjike.com/originalPosts/68ef23fe26b22c70b79d1e3a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ef23fe26b22c70b79d1e3a</guid>
            <pubDate></pubDate>
            <updated>Wed, 15 Oct 2025 04:33:02 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    为什么影视飓风的内容总能爆火？<br /><br />当罗永浩问Tim，持续进步的原因是什么时，Tim的回答并非技术、设备或者创意。<br /><br />他说：“其实我觉得自媒体最大的修炼的点是大众情绪感知。你必须能感知大众的情绪，你才可以获得增长。”<br /><br />这句话，揭示了顶级内容玩家与普通创作者之间的根本分野。<br />大部分人依赖灵感和直觉，而Tim已经将“感知情绪”这件事，变成了一套可以执行的系统。<br /><br />他把这套方法论总结为“四象限矩阵”和“短视频合集”理论。<br /><br />很多创作者都有一个拧巴的状态：用心做的内容没人看，随便拍的反而火了。这背后是创作的巨大不确定性，以及一种常见的陷阱——“自我感动”。<br /><br />Tim在访谈里提到了一个尖锐的理论：“做蠢事赢蠢奖（do stupid things win stupid prizes）”。<br /><br />“你不用指望把头伸进马桶去探索马桶的抽水速度，能够获得诺贝尔奖，只会闷死在里面…很多人会花很大的心思去做一个自我感动的东西，只有8000的播放。”<br /><br />这种“自嗨式”创作，本质上是对观众情绪的误判或漠视。<br /><br />Tim很早就意识到了这个问题。他的转折点来自于父亲的一个建议：“你得从一开始就记录你账号的粉丝量是怎么增长的，每个月写一个报告。”<br /><br />这个建议，让Tim从一个纯粹的创作者，开始转变为一个用数据来理解大众的内容产品经理。而他感知大众情绪的系统，也由此建立起来。<br /><br />我将影视飓风的内容增长飞轮，拆解为三个核心策略：<br /><br />策略一：用“四象限矩阵”实现用户全覆盖<br /><br />单一账号的内容定位，天花板是可见的。为了打破这个瓶颈，影视飓风建立了一个精巧的内容矩阵。<br /><br />Tim这样分解他的布局：“长视频、短视频是一个X轴，然后专业和大众是Y轴，每个地方都有一个对应的账号。我的目的就是把四个象限全部都吃透。”<br /><br />这个矩阵是这样构成的：<br /><br />专业 x 长视频：这是影视飓风主号。内容精良、信息密度高，负责建立行业标杆和品牌深度，吸引核心粉丝。<br /><br />大众 x 长视频：这是**“模型师老原儿”**等账号（早期叫“亿点点”）。选题极其抓人眼球，比如《我熬夜48小时，大脑会发生什么？》、《我把同事送去割痔疮》，负责破圈，获取海量泛用户的关注。Tim透露，这类账号的播放量“比影视飓风还要高很多”。<br /><br />专业 x 短视频：这是他们的短片账号。专门制作冲击奥斯卡等国际奖项的艺术短片，负责拔高品牌调性，探索内容的上限。<br /><br />大众 x 短视频：这是一些体验类账号。内容更轻快、更碎片化，负责跟上短视频平台的节奏，保持高频互动和用户粘性。<br /><br />这个矩阵的厉害之处在于，不同的账号承担不同的战略目的，有的负责“名声”，有的负责“流量”，有的负责“未来”，形成了一个能覆盖不同用户圈层、互相补充、抗风险能力极强的生态。<br /><br />策略二：用“全球化信息源”制造认知差<br /><br />好的选题从哪里来？大部分人刷抖音、看热榜，而Tim的“情报网络”是全球化的。<br /><br />“我有非常特殊的信息输入渠道…全球各个地方的Reddit论坛，那些各种地方很多奇怪的论坛，不可能有人看到，我会去看，我会吸收他们在讨论什么，然后把它转化并且变成更有意思的内容。”<br /><br />这解释了为什么影视飓风的很多选题都具有开创性。当你的信息源比别人更广、更深、更前沿时，你就天然拥有了“认知差”优势。你可以把一个在海外小众极客圈里刚兴起的话题，用大众化的方式呈现给国内观众，这就是降维打击。<br /><br />策略三：用“短视频合集”理论重构长视频<br /><br />这是Tim内容方法论中最具颠覆性的一点。<br /><br />他认为，比短视频更厉害的，是“把短视频拼成长视频的长视频合集”。<br /><br />“车祸视频有很多人喜欢看，但是车祸集锦视频看的人更多。因为它不需要有滑动的这个操作，人是越来越懒的。”<br /><br />这个洞察完全是反直觉的。它揭示了MrBeast这类顶级内容能够维持超高完播率的秘密。<br /><br />Tim解释道：“以前长视频是花很长时间讲一件事，现在是长视频不断的转场给你讲八件事…（这个长视频）切成8段的话，每一段也都是成立的，都是一个短视频，然后拼起来的。”<br /><br />这种创作方式，是把一个长视频，当作一个由数个“小爆款”组成的“爆款包”。每一个环节都经过精心设计，有独立的钩子、冲突和爽点，全程无尿点，用户的注意力被持续锁定。<br /><br />这也是为什么影视飓风的《我把卫星送上天》这类视频能成功的底层逻辑。它不是一个平铺直叙的纪录片，而是一个包含了“悬念（能不能成功）”、“奇观（火箭发射）”、“知识（卫星原理）”等多个高刺激度环节的“体验合集”。<br /><br />这套方法论的背后，是对平台推荐算法的深刻理解。<br /><br />所有算法的核心，无非是几个关键指标：CTR（点击率） 和 AVD（平均观看时长）。<br /><br />Tim的“四象限矩阵”和“全球化信息源”是为了保证选题足够吸引人，从而提高CTR。而“短视频合集”理论，则是为了在用户点击进来之后，最大限度地留住他们，拉高AVD。<br /><br />当你的内容在这两个维度上都做到极致，平台就会把海量的流量灌给你。<br /><br />当然，这套打法对团队的要求是极高的。<br /><br />它需要工业化的制作能力、数据驱动的选题策划能力，以及对全球文化趋势的敏锐嗅觉。这已经远远超出了传统内容作坊的范畴，更像是一个拥有强大中台支持的“内容产品公司”。<br /><br />但Tim的思考方式，给所有创作者提供了一个重要的启示：<br /><br />不要做蠢事去赢一个蠢奖。<br /><br />与其沉醉在自我感动中，抱怨观众不懂你，不如去真正理解观众的情绪，理解平台的语言。<br /><br />你必须理解大众，但又不完全迎合大众，这才是顶级内容的平衡艺术。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ea47481ed9b53c78bffaee</id>
            <title>AI探索站 10月11日</title>
            <link>https://m.okjike.com/originalPosts/68ea47481ed9b53c78bffaee</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ea47481ed9b53c78bffaee</guid>
            <pubDate></pubDate>
            <updated>Sat, 11 Oct 2025 12:02:16 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    很多需求真的是无法空想出来的。比如当我交替使用 GPT 和 Gemini 的时候，最终决定我使用体验的完全不是两者的智能或者风格区别，而是一个纯粹的 feature 差异：后者不支持通过修改对话历史从而实现对话的分岔。<br /><br />对话的分岔显然是一个 GPT 出现之前没有人会预料到的功能。现实中不存在这个东西。当然有时候你会想哎呀我昨天和那谁的对话要是编辑一下重开一个平行宇宙就好了，但反正你知道这不可能，也不会认真对待这个想法。然而 GPT 一旦提供这个功能，你就立刻发现它不可或缺。无数次——或者说几乎每一次——我能从一段对话中学到些什么的体验，都来自于我对之前对话记录的反复 refinement。通过不断比较它们导致的对话走向，我才真正理解我们其实是在说什么。<br /><br />非常奇妙。你意识到对话的本质不是线性的，而是由一连串 what-if 构成的。好的对话不是一条河流，而是一棵树。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68e507b756725ea8ff52fbe0</id>
            <title>AI探索站 10月07日</title>
            <link>https://m.okjike.com/originalPosts/68e507b756725ea8ff52fbe0</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68e507b756725ea8ff52fbe0</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 Oct 2025 12:29:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Sutton 最近跟 Dwarkesh 有个访谈，老爷子认为 LLM 并不是通向 AGI 的正确道路。搞笑的是：LLM 支持者 Dwarkesh 认为 LLM 是符合 “The Bitter Lesson” （通用计算方法加大规模数据终将战胜人工设计的策略）的，却被 Sutton 无情打脸。我看完访谈的最大感受是 Sutton 的观点有点抽象，后来我读了《智能简史》这本书，才完全理解了 Sutton 的观点。<br /><br />Sutton 在访谈里面提到：大模型只是在模仿人类，而不是真的理解了世界。大模型智能预测人类会说什么，而不能预测会发生什么。这个观点说实话很抽象，但我从《智能简史》的一个例子中理解了他的意思。<br /><br />大脑有一种神奇的能力，它可以想象未来会发生什么，《智能简史》用老鼠迷宫实验非常形象的介绍了这种能力。<br /><br />实验过程大致如下：研究人员将一只老鼠放进一个T型迷宫，迷宫的右臂尽头有食物。经过几次尝试，老鼠学会了右转去获取食物。但接下来，实验者将迷宫旋转了180度，入口和岔路口的位置都变了，但食物仍然在房间的同一个绝对位置。此时，如果老鼠只是学会了“右转”这个动作，那它应该还会右转，但这样它就会走向错误的方向。然而，实验中的老鼠在路口停下来左右摇头思考了一下，最终选择了左转，并成功找到了食物。<br /><br />这个实验说明，老鼠并不是死记硬背了向右转的动作，而是在大脑中建立了一个关于迷宫和食物位置的空间地图，一个“世界模型”。当迷宫被旋转后，老鼠能够利用这个内在的地图，在大脑中想象食物的位置，以达到获取食物的目的。它理解了目标（食物）在空间中的位置，而不是机械地重复之前被奖励的行为。后来的研究者通过监测老鼠的大脑神经活动，证明了它确实在脑中分别演练了两种选择的结果。<br /><br />相比之下，鱼就没有这么聪明了。取一条鱼放入水箱，箱中设有透明隔板。在隔板一角开个小孔，使鱼能从一侧游至另一侧。让鱼自由探索水箱，找到小孔并花些时间来回游动。数日后进行新操作：将鱼置于水箱一侧，在透明隔板的另一侧放置食物。结果发现鱼花了跟第一次一样的时间才找到小孔位置，也就是说鱼并没有建立世界模型。<br /><br />这些实验可以用来解释Sutton的观点。Sutton认为，真正的智能，需要像老鼠一样，能够在内在建立一个世界模型，并利用这个模型去灵活地适应和解决问题，而不仅仅是在已有的数据中寻找最优的“下一个词”。现在的大语言模型，本质上是在一个极其庞大的“语言迷宫”里，通过海量的数据训练，学会了在某个“路口”（当前上下文），选择最有可能的下一个“方向”（下一个词）。它能够预测人类会说什么，因为它已经“看”过了无数人类在相似情境下的选择。<br /><br />然而，一旦我们稍微改变“迷宫”的结构，提出一个它从未见过的、需要真正理解世界才能回答的问题，或者要求它根据现实世界的变化做出预测，它可能就会像那只鱼一样，暴露出其死记硬背的本质。比如有研究者发现，当使用 LLM 作为评判工具时，只需要加一个“解”，甚至只是空格、冒号等符号，就能让毫无意义的回答骗得高分。<br /><br />在 Sutton 看来，仅仅通过扩大数据和计算量来训练大模型，只是在让模型更擅长模仿人类的语言模式，而无法真正地理解世界，因此 LLM 不是通向AGI的正确道路。<br /><br />虽然我赞成 Sutton 的说法，LLM 相比大脑确实不够“智能”，但是从实用主义的角度看，即使 LLM 只会模仿人类，如果它确实能完成人类的工作，那么这算不算实现了 AGI？俗话说得好：“当一个东西走路像鸭子、叫声像鸭子，那它就是鸭子。”我自己是不纠结技术路线的，我只关心 AI 是不是真的能解决我的问题，而且我认为 LLM 可以帮助我们更快破解大脑智能之谜。<br /><br />最后也推荐大家可以读一下《智能简史》这本书，作者是搞 AI 出身的，为了研究 AI 学习了很多神经科学的知识，他将神经科学的研究成果跟机器学习的知识串联起来。所以我读的非常爽：就像飞机是从鸟类的飞行得到启发一样，AI 的很多突破都源于我们对大脑的研究。
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>