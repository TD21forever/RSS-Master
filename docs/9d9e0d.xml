<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6854e3cff4324211644808b0</id>
            <title>AI探索站 06月20日</title>
            <link>https://m.okjike.com/originalPosts/6854e3cff4324211644808b0</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6854e3cff4324211644808b0</guid>
            <pubDate></pubDate>
            <updated>Fri, 20 Jun 2025 04:30:07 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Gemini 现在支持上传视频分析了<br /><br />终于不用一直用 AI studio 了，这估计也是第一家直接能分析长视频的 AI<br /><br />可以参考我前几天交的这个提示词和方法分析爆款视频提取创作逻辑
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6854c2a6f0d718ce7aecf7ab</id>
            <title>AI探索站 06月20日</title>
            <link>https://m.okjike.com/originalPosts/6854c2a6f0d718ce7aecf7ab</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6854c2a6f0d718ce7aecf7ab</guid>
            <pubDate></pubDate>
            <updated>Fri, 20 Jun 2025 02:08:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    Dia 要有类似 Arc 的侧边栏了，你要真这样改，那我就真换默认浏览器了啊
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68540a04058533d925c154f7</id>
            <title>AI探索站 06月19日</title>
            <link>https://m.okjike.com/originalPosts/68540a04058533d925c154f7</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68540a04058533d925c154f7</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Jun 2025 13:00:52 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    豆包新上线了AI播客，瑞士军刀功能再+1，目前支持PDF和网页链接的上传，总体来说，通过大模型的智能识别，豆包现在可以把任何内容转化成一条高度口语化的双人对话播客，属于趣味性和实用价值都很高的一次尝试。<br /><br />熟悉大模型播客产品的人都知道，豆包这次对标的是谷歌NotebookLM——或者说是它最出圈的Audio Overviews功能——后者通过识别用户上传的文本、网页、视频，就可以转化成一条口语化且带有情绪表达的AI播客，深得用户喜爱。<br /><br />Audio Overviews大约是在上个月开始支持中文的，但在中文播客市场砸出来的水花并没有想象中的大，一方面是众所周知的产品迁移成本，另一方面，中文播客市场体系化进度实在迟缓，用户习惯是高度分散的，这就导致了播客本身的适配场景很多，深究起来的播客用户以及潜在播客用户也不少，但商业化空间始终有限。<br /><br />说回正题，我第一时间试了下豆包的AI播客，并分别投喂了两个不同的网站，一个是我写的刘强东前两天内部讲话的文章「刘强东的机巧」，另一个是B站UP主对Prompt, Agent, MCP等AI技术的科普文。<br /><br />先说结论，在真正听完豆包生成的这两条AI播客之前，我对这项功能的完整程度预期并不高，原因在于，在这种复杂的任务上，目前很多主流大模型的做法还是「边吞边吐」，由此就会破坏内容输出的结构性。<br /><br />但豆包已经可以做到在10分钟左右的播客篇幅里基于框架生成内容了，在「刘强东的机巧」生成AI播客的任务里，所有对话的前后呼应都很强，能听得出它是按照同一条逻辑线不断往下捋的，有点意外。<br /><br />另外就是，豆包AI播客的拟人程度已经可以做到以假乱真了，这真的不是夸张，对话的流畅度、松弛感以及合时宜的抑扬顿挫，像我这种文字工作者，文章简单拿来改一改就能直接原地起个播客账号的程度。<br /><br />那条硬核技术帖转播客的任务表现也相当亮眼，首次提及专业名词的时候，会贴心附上一段对这个概念的解释。整体的输出脉络，也都是建立在「我要深入浅出讲明白这条科普」这个最终目的上。<br /><br />说人话，就是AI播客让内容的「可听性」变强了，哪怕注意力没有完全集中在耳朵上，这种通俗易懂的内容也变得更容易被消化。<br /><br />播客——以及整个音频产业——一直以来的优势，是它不会完全参与到竞争用户注意力的零和博弈里去，大部分情况下，刷视频、聊微信、逛淘宝都是非此即彼的单一选项，但播客只占用一个耳朵，由此它能与很多不同的场景做适配。<br /><br />豆包不是第一个推出这种功能的大模型，但它在应用场景上的成熟度是完全可以进到第一梯队的，不仅能把拗口的文字进行口语化改造再丝滑地表达出来，同时所有内容输出也都是基于原稿，不存在自己加戏的幻觉问题。<br /><br />当然，作为新上线的功能，豆包AI播客还会经历一个漫长的迭代过程，比如目前它做不到像NotebookLM一样吃下视频内容，对话的声音、关键信息的提炼浓度，以及生成后的整体风格也都不是客制化的可选项，离用户可以随心所欲地深度使用它，尚且还有一段路要走。<br /><br />但这并不妨碍我们从这个简单的小功能身上窥见AI在未来的使用场景，一切都是假以时日的问题。<br /><br />虽然知道AI的技术一日千里，但每次实际体验的时候，那种奇妙感还是会忍不住涌上来。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6853e1a9d82bae994ad6a8e9</id>
            <title>AI探索站 06月19日</title>
            <link>https://m.okjike.com/originalPosts/6853e1a9d82bae994ad6a8e9</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6853e1a9d82bae994ad6a8e9</guid>
            <pubDate></pubDate>
            <updated>Thu, 19 Jun 2025 10:08:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    搞了一整天终于搞定了<br /><br />给大家带来 Midjourney V1 视频模型的完整测评混剪<br /><br />以及为什么我觉得这个“480P”的垃圾模型很重要<br /><br />详细的教程和视频可以在这里看：https://mp.weixin.qq.com/s/phWDQJzoUdJRT8mTLiAZwQ<br /><br />来看一下模型的测试结果：<br /><br />Midjourney Video 第一个长处是美学表现相当顶级。<br /><br />这是 Midjourney 的看家本领，在色彩表现、氛围营造上无可挑剔。<br /><br />然后是高风格化视频的表现。<br /><br />图像和视频的风格是非常多的，不只是所谓的写实和动漫，尤其是MJ 生成的图片。<br />得益于MJ 本身的图片数据，导致 V1 模型在处理罕见的高风格化视频的时候非常稳，色彩、笔触、氛围都能保持得住。<br />而且即使是运动中新出现的部分也可以维持住原有的风格。<br /><br />另外他们的生成速度也非常快。<br /><br />我自己开秒表试了一下，一次生成 4 个视频只需要 65 秒的时间。<br />这在现在 1080P 动辄十几二十分钟的生成时间上可以说是清流了，普通用户完全没办法等这么长时间。<br /><br />视频延长的稳定性也很好，基本上延长最后一次到第 17 秒的时候，视频依然没有崩溃，即使是复杂场景，这个在其他模型上是很难见到的。<br /><br />说完了好的再说不行的。在现在视频模型经常考核的，提示词理解、复杂运动稳定性和物理特性上，基本处于二流模型水平。<br /><br />与现在第一梯队的hailuo2、Kling 2.1、Seeddance 1.0 Pro、Veo 3 没法比，而且这些大部分默认分辨率都已经是 1080P 了，他还 480P。<br /><br />看到这里你肯定也像大部分人认为的一样，觉得 Midjourney 视频模型这也太拉了，怎么跟其他模型竞争。<br /><br />我不这么认为。<br /><br />我觉得 Midjourney 很清楚他们模型的问题也清楚现在的视频模型竞争格局，但他们选择不去管这些，选择不去跟其他视频模型设定好的框架竞争。<br /><br />在发布 V1 视频模型的时候他们重申了一下自己的愿景，他们长期目标是构建一个实时图像生成的 AI 系统。<br /><br />你可以进入到图像所在的时间进行移动和游览，图像中的其他角色和环境也会随着你的移动变化，你可以跟所有的元素交互。实现这个方案需要四个部分：<br /><br />- 视觉呈现，也就是现在 MJ 的图片模型<br />- 然后要让图像动起来，就是现在发布的 V1 视频模型<br />- 之后如果需要跟环境交互的话需要赋予内容实体，一个 3D 模型<br />- 最后生成速度要跟的上你的移动速度，也就是实时生成模型<br /><br />看到这里你也许就能理解为什么他们不在乎现在其他视频模型在乎的那些东西，他们唯二在技术上在意和下功夫的地方就是两个，生成速度和长时间一致性。<br /><br />这两个是实现他们愿景中视频模型负责的最关键的部分，物理的部分会交由 3D 模型完成。<br /><br />可能有人说之前 Runway 也说过类似的话，但他们现在没声音了。<br /><br />这个就要说到 Midjourney 的另一个优势了，他们没有融资压力，完全是自给自足，所以与需要频繁融资续命的公司不同，他们可以慢下来，可以与主流不同。<br /><br />其实这个愿景实现之后的场景现在已经可以一窥究竟了，你可以去 Midjourney 的视频探索页面看看。<br />昨晚模型发布之后我刷了这个页面一个小时，困的不行了才恋恋不舍的去睡觉。<br /><br />他们好像展示在我面前的真是一个个的异次元世界入口，每个世界的画面风格和物产都不同。<br /><br />看着这些视频的时候我会不自觉的开始想想每个画面后的世界跟我产生的故事。<br /><br />现在是不是知道为啥我要给模型测试视频起这个《精骛八极，心游万仞》的名字了。<br /><br />这句话是陆机描述在进行艺术构思、艺术创作时需要做到的事情，思想纵横驰骋而不受时空的限制，就像骏马驰骋于天地四方，又像心灵畅游于万仞天空，是不是很像 Midjourney 的愿景。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68526147b4800e190d59ec51</id>
            <title>AI探索站 06月18日</title>
            <link>https://m.okjike.com/originalPosts/68526147b4800e190d59ec51</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68526147b4800e190d59ec51</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Jun 2025 06:48:39 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    学习@哥飞 的小树林理论：种下一颗树苗，成长速度可能会很快！<br /><br />去年年底上的一个站 ，历经半年，排名突然就起来了，流量也起来了！<br /><br />给出一些基本数据，供大家参考：<br />KD:44<br />每月搜索量：约20万<br />这个词不是新词，是刚开始ai出来的时候产生的，大概时间有2年左右。<br /><br />做完站后做了如下工作：<br />第一个月上DR低的外链，约20条<br />第2-3月上DR中等的外链，约100条<br />半年左右，上高DR的外链，约200条。<br /><br />后面就没怎么管了，静等花开！<br /><br />ps.这段时间尝试了各种外链的DR对排名的影响，感觉加外链也需要循序渐进，外链与你网站要对等的DR来加，小树苗期间去做做论坛blog小型导航站没问题，中期可尝试付费提交高DR的，后期可买guest posts提升DR。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/6852513a2b50c6891806fad4</id>
            <title>AI探索站 06月18日</title>
            <link>https://m.okjike.com/originalPosts/6852513a2b50c6891806fad4</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/6852513a2b50c6891806fad4</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Jun 2025 05:40:10 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    藏师傅又把压箱底的家伙事掏出来了，今天下午发
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/685228962d05f8d12ae502df</id>
            <title>AI探索站 06月18日</title>
            <link>https://m.okjike.com/originalPosts/685228962d05f8d12ae502df</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/685228962d05f8d12ae502df</guid>
            <pubDate></pubDate>
            <updated>Wed, 18 Jun 2025 02:46:46 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    一个新的即将爆火的 Veo3 ASMR 视频品类<br /><br />直接模仿 ASMR 主播带上人物口播和物品操作一起，这下 ASMR 女主播也有点难了<br /><br />提示词模板在下面👇<br /><br />【画面描述】<br /><br />ASMR视频，[景别，如：特写镜头/中景/微距镜头/俯拍视角]。一位 [主播的外貌和神态描述，如：长发飘飘的漂亮年轻女性/表情优雅的女士] 身穿 [服装描述，如：丝绸质感的吊带裙/黑色蕾丝上衣]，身处于一个 [场景环境和氛围，如：灯光柔和的卧室/舒适的书房/黑暗极简风格的房间] 中。<br />一支 [麦克风的描述，如：专业的录音室麦克风/复古的电容麦克风] 被放置在 [麦克风的位置，如：她的面前/离动作非常近的地方]。<br />她正在与 [核心ASMR道具的描述，如：一大张气泡膜/一块白色的固体香皂/一本皮质封面的日记本] 互动。她 [主播与道具互动的具体动作，如：用手指缓慢地按破每一个气泡/小心翼翼地削下薄薄的皂片/用钢笔在纸上缓缓书写]。镜头的视觉焦点集中在 [画面的主要焦点，如：她的手和道具/香皂的纹理/钢笔尖与纸张的接触点]。<br /><br />【音频描述】<br /><br />一个 [主播的音色和语气，如：温柔甜美的女声耳语/低沉平静的耳语/充满亲密感的呼吸声] 说道：“[具体的口播内容，如：‘我们一起来捏破它们吧’/‘听这轻柔的刮擦声’]。”<br />随后是 [核心ASMR音效的详细描述，使用拟声词和形容词，如：气泡被捏破时清脆响亮的“啪！”声/刀片刮过香皂时发出的酥脆、带有颗粒感的声音/钢笔在纸上书写时独特的、略带刮擦感的“沙沙”声]。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/684f02a9e441f085b7ce67c6</id>
            <title>AI探索站 06月15日</title>
            <link>https://m.okjike.com/originalPosts/684f02a9e441f085b7ce67c6</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/684f02a9e441f085b7ce67c6</guid>
            <pubDate></pubDate>
            <updated>Sun, 15 Jun 2025 17:28:09 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    我为什么要创建 Fellou AI 浏览器<br />这其实是一个关于生产力的故事
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/684277e74e98b7acd544ba17</id>
            <title>AI探索站 06月06日</title>
            <link>https://m.okjike.com/originalPosts/684277e74e98b7acd544ba17</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/684277e74e98b7acd544ba17</guid>
            <pubDate></pubDate>
            <updated>Fri, 06 Jun 2025 05:08:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    最近大家都在聊 AI 加持下的 vibe coding，我来聊聊作为资深开发者最近高强度使用 AI 的一些感受：<br /><br />一句话总结，AI 让不会写代码的人具备了“直接造辆车”的能力，而让资深开发者一个人就有了“独立建造航母”的可能。<br /><br />### 项目重构<br /><br />最近使用 claude-4 对我之前的一些代码进行了重构。原因是原来的实现中，为了降低编写时的心智负担，会使用一些性能偏低但是易于书写的代码。比方说自动锁管理、ARC、使用 array 数据结构代替 queue。<br /><br />然而用 AI 实现就没了这些负担，我先让 AI 为原始实现编写完整测试用例，确保原代码行为明确，然后让 AI 对整个 class 进行重构，追求极致性能，写完新代码后再重新运行测试保证行为一致。<br /><br />就这样，我轻松完成了部分核心数据结构的重构。尽管重构后的代码量几乎翻倍，但逻辑清晰、复杂度可控，换来的则是约 20% 的性能提升。<br /><br />核心是，AI 编写代码不怕苦不怕累，没有必要为了简化代码而牺牲性能。人类工程师目前主流习惯是牺牲部分运行性能以换取开发效率。<br /><br />### AI 编程语言<br /><br />这牵扯出的另一个观察是，什么编程语言对 AI 更友好，我的观察是可读性越高、行为越明确的语言效果越好。语法糖等简化编码技术，反而不利于 AI 使用。（AI 在发现一些奇怪的行为是运算符重载导致的不知道会不会跟我一样跳脚骂街）<br /><br />而像 SwiftUI 那些优势仅在开发效率上的技术，在 AI 时代更显得有些生不逢时。反正都是 AI 写，AI 用 UIKit/AppKit 实现不过是代码长一点而已，在可控性和行为明确性方面更适合 AI 自动化维护，性能也高的多。<br /><br />### AI 的资深<br /><br />虽然 AI 的编码技能，比起资深的工程师其实可能还是会有差距，但是要论知识丰富程度，则远非任何个体可比。<br /><br />这个优势体现在当我要去实现一些技术盲区时，原本的流程大概是：先读几本书，再对照比较一系列 RFC，再请教下相关领域的朋友确认自己已经理解。或者先按照自己的想象做个最小工程实践，然后再根据各种问题一点点填坑。<br /><br />比方说最近在实现 IPv6 ND 协议栈，一些特定的 RA 消息构造在某些操作系统上就是无法生效，原本这可能要耗费我几天的时间去研究，阅读各种文献甚至 kernel 源码实现，而现在只需请教 AI，就能非常准确地找到答案。<br /><br />AI 的这种资深，在你对某个技术的表层足够了解，但是缺乏经验和细节信息时，能够极快的帮你补全。<br /><br />### 极强的 debug 能力<br /><br />我的项目里有一个藏了很久的问题，在特定情况下会出现 TCP 性能下降，由于并没有产生任何明确的报错，这让修正这个问题变得异常麻烦。<br /><br />我原本是单纯向 AI 描述了我的使用场景和问题表现，AI 提出了几种猜想，大部分我看一眼就知道不靠谱，剩下几个试了下也并无效果。索性，我直接把 100MB 的抓包结果丢给了 o3 让他分析。它在几分钟内就精准指出了问题所在，甚至给出了改进建议。这种调试能力在人类团队中几乎无法复现。<br /><br />如此庞大的数据量，人工分析非常困难。即使借助各种工具，仅学习用法、配置环境就已令人头大。（因为 TCP 流控分析的各种工具链基本都是上个世纪的项目）<br /><br />现在我已经习惯了这种 vibe debug，遇到什么问题，直接把 verbose 日志和问题描述丢给 AI，大概率就能直接找到问题，这其实也是得益于 AI 的不怕苦不怕累的精神。<br /><br />### Peer review<br /><br />作为独立开发者，我的 code review 一直以来只能靠自己，但是自己写的 bug，很多时候自己是看不出来的🙈，现在我只需将 git diff 的结果交给 AI，就能请它帮我 review。<br /><br />同样的，我也会 review AI 给出的结果，AI 当然也会犯错，高级低级的都有。但是比起人类同事来说，AI 没有 ego，能很好地接受反馈并立即调整；很多人类做不到，或至少过程很曲折。<br /><br />### 职业影响<br /><br />就目前 AI 的能力来看，无疑是对初级开发者就业市场产生了巨大的压力，对于资深工程师来说，反而是一种赋能。（我目前还是能为找到 AI 的错误并指导它而沾沾自喜，但也不知道还能持续多久。）<br /><br />这比较让人担忧的是，这可能导致职业断层，因为初级开发者根本没有机会得到训练机会而成长。<br /><br />不过这已经早已不仅仅是软件工程师所面临的问题，本质上来说，所有脑力工作者的职业都受到了巨大威胁。像咨询、律师等职业，还可以依靠私域信息门槛维持。而像医生这样完全依赖公域信息的职业，初级职位也同样完全可以被 AI 替代了，当然最终取决于患者的接受程度。<br /><br />我最近一次体检后的报告喂给 o3 进行解读，他给出的信息量、准确性、建议，均远超全科医生给出的解读。不仅仅是因为 AI 的信息更全面，AI 可以为报告中每一项异常数据，检索最新研究与各国医疗指南，并整合后给出建议，甚至由于 GPT 已经了解我的生活习惯，能更优针对性的给出意见。而这种工作量对于人类医生来说是不可接受的（当然大多数情况下也确实没有必要）。<br /><br />很多人对 AI 医疗的顾虑是：AI 犯错了怎么办？然而其实人类医生也会犯错，而且就现在的 AI 水平来看，AI 犯错的概率应该已经比一般人类医生低了。当然最优解还是兼听则明，把 AI 的意见告知医生，也把医生的反馈告知 AI，基本最后都会达成一致。对于一些不重要的小问题，仅 AI 意见完全足够。<br /><br />### AI 的限制<br /><br />当然 AI 也不是万能的，甚至可以说局限性相当明显。claude-4 虽然非常强，但是随着 context 的增长，注意力溃散的非常严重，后面基本就像喝多了一样。<br /><br />当前的最佳实践是：尽量保持 context 精简，聚焦具体任务，依靠人力来拆解复杂目标。<br /><br />比方说先用一个 context 确定具体需求，再开一个 context 将明确好了的需求转换为具体任务列表，再把任务单独交给一个个 context 去具体实现。这样效果会好很多。<br /><br />仔细一看，这不就是人类的团队协作模式嘛 😂<br /><br />这让我想起不久前由 GPT o1 和 DeepSeek R1 的思维链引发的 AI 能力巨幅提升。其实在思维链能力出来之前，就可以靠 prompt 指引 AI 一步步思考，取得类似的效果，甚至催生了 prompt 工程师职业。然而直接在模型层面将这种能力整合后，prompt 引导就非常多余了。<br /><br />那么目前编程实践中，如今常用的 context 切分技巧，我认为在不久的将来也可能被模型层原生支持，即 AI 自主可以通过切换 context 的方式维持注意力，保持高效。这可能带来 AI 能力的又一次飞跃式进步。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68393bb5d9288e4a516c8782</id>
            <title>AI探索站 05月30日</title>
            <link>https://m.okjike.com/originalPosts/68393bb5d9288e4a516c8782</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68393bb5d9288e4a516c8782</guid>
            <pubDate></pubDate>
            <updated>Fri, 30 May 2025 05:01:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    感谢@硬地骇客 的支持，把三五环和半拿铁的多数文稿整理了一下，投入到 ima.copilot 里面，可以对话了。<br /><br />之前跟 ima 的朋友交流，就聊到未来知识库的「整理」变得没那么重要，而「采集」变得更重要，独特的筛选标准，以及采集逻辑，是决定知识库的价值的。ima 里也有很多筛选自己喜欢的内容而形成的公开知识库。<br /><br />对内容创作者自己来说，沉淀好自己的内容也特别有意义，有的不存不用确实就容易丢了。哪怕对别人没用，自己时常反刍也很有帮助。
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>