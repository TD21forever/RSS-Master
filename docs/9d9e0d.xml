<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>AI探索站 - 即刻圈子</title>
        <link>https://m.okjike.com/topics/63579abb6724cc583b9bba9a</link>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68aeb3e2e5597c28d37b5611</id>
            <title>AI探索站 08月27日</title>
            <link>https://m.okjike.com/originalPosts/68aeb3e2e5597c28d37b5611</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68aeb3e2e5597c28d37b5611</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Aug 2025 07:29:38 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    写了一个 Nano Banana 详细的使用教程和我所有用法提示词合集。<br /><br />这里比较完整，想系统了解的可以看看：https://mp.weixin.qq.com/s/dIrEIIRXRpdEOPK2sj0DBw
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ae91555ff3276240788af8</id>
            <title>AI探索站 08月27日</title>
            <link>https://m.okjike.com/originalPosts/68ae91555ff3276240788af8</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ae91555ff3276240788af8</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Aug 2025 05:02:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    除了可以从照片生成穿搭平铺图以外，你还可以用 Nano Banana直接预览自己穿上对应穿搭的样子<br /><br />ID 保持的非常好，提示词：The character in Figure 2 is wearing the clothing and accessories from Figure 1.
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ae821114af706d82dbd41b</id>
            <title>AI探索站 08月27日</title>
            <link>https://m.okjike.com/originalPosts/68ae821114af706d82dbd41b</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ae821114af706d82dbd41b</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Aug 2025 03:57:05 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    🧙‍♀️我无数次表达过 GPT image 是真正的魔法。但难以置信的是，仅仅半年时间Gemini 2.5 flash image 就将这种魔法上升到了全新的层次：<br /><br />- 10倍的生成速度<br />- 惊人的品质和一致性<br />- 几乎完全免费<br /><br />值得再次强调是，获取这种魔法的关键就是亲自尝试——「对话」的能动性成为了唯一的门槛。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ae743cd2a36a8aa15b797a</id>
            <title>AI探索站 08月27日</title>
            <link>https://m.okjike.com/originalPosts/68ae743cd2a36a8aa15b797a</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ae743cd2a36a8aa15b797a</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Aug 2025 02:58:04 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    这个吊！Nano Banana 其实支持识别手绘内容<br /><br />所以你可以通过手绘图精准控制多个角色打斗姿态！<br /><br />提示词：Have these two characters fight using the pose from Figure 3.Add appropriate visual backgrounds and scene interactions,Generated image ratio is 16:9
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ae6ce22393a294a617da71</id>
            <title>AI探索站 08月27日</title>
            <link>https://m.okjike.com/originalPosts/68ae6ce22393a294a617da71</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ae6ce22393a294a617da71</guid>
            <pubDate></pubDate>
            <updated>Wed, 27 Aug 2025 02:26:42 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    NanoBanana 拯救拍照手残党<br /><br />觉得自己拍的照片不好看？不会调色？有无关的东西？<br /><br />交给 NanoBanana 一段提示词解决：This photo is very boring and plain. Enhance it! Increase the contrast, boost the colors, and improve the lighting to make it richer,You can crop and delete details that affect the composition.
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68ac3e201e5664293d2098db</id>
            <title>AI探索站 08月25日</title>
            <link>https://m.okjike.com/originalPosts/68ac3e201e5664293d2098db</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68ac3e201e5664293d2098db</guid>
            <pubDate></pubDate>
            <updated>Mon, 25 Aug 2025 10:42:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    前几天听完广密的播客意识到身边很多人确实从Cursor切换到Claude Code了，正想找找有没有稍微系统一点的教程，就看到吴恩达这个月联合Claude官方出了门课，花了两个小时一口气看完，强烈推荐！！<br /><br />原汁原味版：https://corporate.deeplearning.ai/courses/claude-code-a-highly-agentic-coding-assistant/lesson/66b35/introduction<br />B站字幕版：https://www.bilibili.com/video/BV1k1bBzTEF5/?spm_id_from=333.337.search-card.all.click<br />笔记精华版：https://mp.weixin.qq.com/s/Of1qAQuAQDrkyWIZ0ug-dw<br /><br />再次感恩AI时代赛博菩萨安德鲁大法师，当年要是没有他的DeepLearning.ai我甚至都入不了AI这一行<br /><br />补充一些不错的Claude Code相关文章：<br />《一个半月高强度 Claude Code实践反思》https://mp.weixin.qq.com/s/fP3kU-O5mbSO8ZS04gowCw<br /><br />《Claude Code掌舵人Cat Wu访谈》https://mp.weixin.qq.com/s/4cPLJClf0z-5SyaXwwTWpQ<br /><br />《Vivek Aithal：为什么Claude Code 如此好？》https://mp.weixin.qq.com/s/XGFzYMZRe69Sxc3KPFumeA<br /><br />《海外独角兽：Claude Code 如何带来 AI Coding 的 L4 时刻？》https://mp.weixin.qq.com/s/GB4t6GKOd4oU0Fsb-NfyEA
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68a929ede5597c28d3086ce4</id>
            <title>AI探索站 08月23日</title>
            <link>https://m.okjike.com/originalPosts/68a929ede5597c28d3086ce4</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68a929ede5597c28d3086ce4</guid>
            <pubDate></pubDate>
            <updated>Sat, 23 Aug 2025 02:39:41 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    🚨谷歌发布了一本 68 页的提示词工程的书籍，而且是免费的！<br /><br />你甚至不需要注册。<br /><br />它包含真实的数据、实验和最佳实践。<br /><br />以下是需要了解的最重要的要点：<br /><br />1，prompt不是比谁更聪明，而是要有系统性。<br /><br />谷歌提供的这本免费书籍对于任何从事LLM的人来说都是一座金矿。<br /><br />这是链接：https://www.kaggle.com/whitepaper-prompt-engineering<br /><br />2，提示词是一门科学，不是靠运气：<br />•谷歌的研究表明，结构化提示持续优于随意的表述。<br />•你的措辞可以让输出质量提升几个数量级。<br /><br />链式思维 &gt; 一次性回答：<br />•把任务拆成步骤（显式把思路写出来）会让模型更准确。<br />•不要只索要最终结果 → 要引导模型完成推理过程。<br /><br />3，具体性胜过模糊性<br />•“请用 5 个要点为 CEO 总结这篇文章”<br />要比 “总结一下这篇文章” 更好。<br /><br />清晰的角色、限制条件和上下文 = 更优质的输出。<br /><br />迭代是关键<br />•论文强调：不要停留在第一个提示词。<br />•精炼 → 测试 → 再精炼。<br />提示词工程更接近 调试（debugging） 而不是写作。<br /><br />4，提示词格式很重要<br />•换行、列表和分隔符可以减少幻觉并提升清晰度。<br />•把提示词当作给初级队友的指令，而不是随意的聊天文字。<br /><br />5，评估往往被低估<br />•谷歌建议构建一些简单的指标，用来比较提示词的质量。<br />•否则，你永远无法知道 v2 是否真的优于 v1。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/689b0806ba4e69f5b2cf3a2f</id>
            <title>AI探索站 08月12日</title>
            <link>https://m.okjike.com/originalPosts/689b0806ba4e69f5b2cf3a2f</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/689b0806ba4e69f5b2cf3a2f</guid>
            <pubDate></pubDate>
            <updated>Tue, 12 Aug 2025 09:23:18 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    遇到了一个有趣的问题，正好落在 AI 模型的能力边界处：试证明不可能把平面分成无穷个圆的无交并。<br /><br />在我尝试的所有模型里，只有 GPT 5 thinking model 成功做了出来（虽然花了点时间）。<br /><br />有趣的不是这个结论，而是观察它们的思路。所有失败的模型都有个共同点：它们的思考基本上是从文字到文字的。它们会调用自己脑海中各种已有的定理和知识，然后漫无目的地试图拼凑出一个证明，但所有这些定理，不管是拓扑的还是几何的还是测度的，对它们来说都是纯粹字面意义上的陈述。Qwen 的思考过程最典型：它滔滔不绝想了很久，但很显然从头到尾它都并不真的理解它在说什么。圆也罢，开集闭集也罢，Baire 纲定理也罢，对它来说都是纯粹的概念，给人的感觉是它甚至并不真的知道「圆是圆的」。<br /><br />微妙之处在于，这种「没有几何直觉的几何思考」在某些时候其实未必是一种劣势。现代数学早已挣脱了对三维现实想象的依赖，大部份数学思考本来也确实是在纯粹的概念思辨空间中进行（特别是当问题进入代数乃至范畴论的领域的时候，这时从概念到概念的思考就变成了一种必然）。有的时候，几何直觉甚至反而会成为一种束缚，特别是当思考高维空间的时候，基于低维现实的直观常常是有误导性的。在这些问题上，AI 的「盲目」反而带来了自由，使得它不必受困于视觉直觉。——当然，人类的视觉直觉可能会渗透进人类的文本语料里，在某种程度上「污染」AI，但这是另一个问题。<br /><br />然而对原问题来说，因为这是一个低维问题，直觉在这里不但有用，而且能大大缩短思考搜索的难度。在这一点上，一个把圆只作为抽象概念来理解的 AI 就会有巨大的劣势，因为它无法享受到几何直觉带来的跳步。这种直觉使得人可以一眼「看出」关键的构造，而这种构造在文本层面被搜索出来是困难的。<br /><br />考虑到 AI 的应用毕竟大多数情况下还是为了解决世界现实问题而不是思考高维几何，有几何直觉的 AI 会在大多数问题上显得聪明得多。于是一个现实问题是，这种直觉是只有依赖多模态的训练才能获取，还是可以通过精巧的文本训练就能实现？这有点像是 AI 领域的玛丽房间问题。这是一个经典的知识论思想实验：一个从出生就生活在黑白房间里、精通颜色物理与神经机制的科学家玛丽，当她第一次走出房间看到红色时，她是否获得了新的知识？<br /><br />今天大多数 AI 领域的困难都可以归结于此。人类是自己感官的奴隶，我们听到、看到、闻到，我们体会身体激素的涨落，我们想象、困惑、愤怒，然后试图把这一切投射在文字空间里。AI 则正好相反，它们在文字里理解这一切，但最终需要努力地——有时候是徒劳地——明白，一个圆在什么意义上是圆的。
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68956c79369da0a84be8c3a2</id>
            <title>AI探索站 08月08日</title>
            <link>https://m.okjike.com/originalPosts/68956c79369da0a84be8c3a2</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68956c79369da0a84be8c3a2</guid>
            <pubDate></pubDate>
            <updated>Fri, 08 Aug 2025 03:18:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    ✨ GPT-5 初体验<br /><br />作为ChatGPT 的深度同行者，我在GPT-5 发布这个重要节点，选择了最朴素的交流方式—— 模拟大学生询问它有什么特别之处？ GPT-5  以极快的速度输出了一张场景匹配表，隐约揭示出我们正站在新的分水岭。<br /><br />随后，想到了ChatGPT最新Study模式正风靡全球，想看看它会怎么介绍给新时代的中学生们；让GPT-5 写了欢迎信进行自我介绍；GPT-5 非常快、而5 Pro的缜密和深度令人印象相当深刻。<br /><br />回想起任天堂刚刚发布了一个独立游戏的直面会还没看，要不先让Agent 帮我梳理一下？  GPT-5 在 Agent 模式花了18分钟，列出了一份详实的清单：好家伙，21个新游、熟悉面孔不多；不过铲子骑士家出的新作《挖掘者米娜》很期待呢<br /><br />打开Open AI 的GPT-5直播发布会，气氛依旧温暖、平等并充满人文气， 其中关于 GPT-5 语音模式的对话意外相当亲切；于是拿起手机端也聊了几句，更智能、也更个人化（调皮），我知道以后的日子里更离不开它了。<br /><br /> GPT-5的发布会如老朋友般松弛，这与当年的苹果式发布会炫酷创新形成鲜明反差。任何一个保持开放、拥抱变化的AI 创造者来说，无论过去三年多么的波澜壮阔，也会在这个时刻有那么点怀旧；Open AI 也找来了早期员工分享2022年底正式发布前，内部还称呼这个产品还叫Chat with GPT的往事，不疾不徐、简约克制。<br /><br />然而，新时代的改变又是极为剧烈的：过去我们与AI对话，现在我们委托AI执行；每天数十个任务ChatGPT Agents ——做研究、写创意文案、完成海量设计和编码。这个全能助手已经彻底融入工作和生活，比移动设备更加亲密和自然。 <br /><br />我知道无法在短短几十分钟揭示GPT-5的全貌， 便让它以自己最擅长的方式—— Deep Research ——进行一次深度回顾。<br /><br />三年巨浪，我们从旁观者成为创造者。这不只是技术进步的见证，更是每个深度用户内心变迁的缩影。 相信你也能在这个回顾中找到自己的心路历程。Enjoy～<br /><br />《从ChatGPT到GPT-5：三年AI革命的全景回顾》<br />https://chatgpt.com/s/dr_6895633264248191913fdb1138eaf853
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://m.okjike.com/originalPosts/68943e5d723686893f13022d</id>
            <title>AI探索站 08月07日</title>
            <link>https://m.okjike.com/originalPosts/68943e5d723686893f13022d</link>
            <guid isPermaLink="false">https://m.okjike.com/originalPosts/68943e5d723686893f13022d</guid>
            <pubDate></pubDate>
            <updated>Thu, 07 Aug 2025 05:49:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                    感觉 OpenAI 最近开源的 gpt-oss 真的被严重低估了。它虽然不是最聪明的，但在使用场景和定位上是非常成功的（服了 Sam 老六）。<br /><br />首先最小尺寸的 gpt-oss:20b 绝对是你在 Mac 或者家用电脑上就能跑的最好模型之一（对我来说是“唯一”）。它是那种少有的“真正能用”的本地模型，对话体验非常好，持续对话下来非常稳定、不会出现输出混乱崩溃的问题。大部分早期能跑的本地模型是不具备的。<br /><br />它的尺寸刚刚好，不管是模型文件还是运行显/内存都非常小（大约是 12～16GB），大部分电脑都能使用。在我的 M4 Mac 上能做到 70token/秒 的输出，在我的 19 年老 Intel 的 Mac 上能做到大约 1 token/秒的输出。整体性能上非常出色。<br /><br />最为重要的是，虽然硬件要求非常低，但它的智能表现出乎意外的好。我把个人最近一些非编程类的问题同时发给 gpt-oss:20b 和元宝（DeepSeek R1），不管是回答速度还是回答质量，我更喜欢 gpt-oss:20b 多一点。这不是严谨的对比测试，我也只尝试了五、六个问题，但考虑到硬件要求，这样的回答效果已经让我感到满意了。<br /><br />如果你的对话需求非常简单，或者想要一个完全离线、隐私自由的本地模型，gpt-oss:20b 绝对是一个简单可靠的选择。<br /><br />其次是最大尺寸的 gpt-oss:120b，OpenAI 号称接近 o4-mini 的水平。理论上它也能在 Mac 或者家用电脑上运行起来。我记得大约需要 60～80 GB 的显/内存，对我的 Mac 来说非常吃力。如果电脑硬件足够强、或者并行几个 Mac Mini 跑起来问题应该不大，这就能拥有一个接近 o4-mini 水平的离线本地模型了。<br /><br />gpt-oss:120b 另外一个被低估的意义是在价格上。在 together.ai 上这个模型的价格低至 $0.15 / $0.60，相比之下 DeepSeek V3（注意是 V3 不是 R1）的价格是 $0.27 / $1.10。也就是说，这么一个号称性能接近 o4-mini 的模型，价格只需要 DeepSeek V3 的一半！！据我所知这样的价格已经是 LLM 中的最低档，类似价格的是 gpt-4o-nano 和 gemini 2.5 flash-lite。<br /><br />我现在非常怀疑 OpenAI 这波就是冲着 DeepSeek 来的……<br /><br />另外我还发现了 gpt-oss 非常容易破解，比其他开源模型简单很多。容易到什么程度呢，我可以在大约三次对话内让它回答各种非法问题。不过这个话题不适合在这里讨论了……
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>