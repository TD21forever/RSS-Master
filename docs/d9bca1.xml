<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <channel>
        <title>InfoQ 话题 - AI&amp;大模型</title>
        <link>https://www.infoq.cn/topic/AI</link>
        
        <item>
            <id>https://www.infoq.cn/article/u9llmlvM1dUTiCLcf2mg</id>
            <title>硅谷视野+中国实践，汇聚全球顶尖技术的 AI 科技盛会，更有生成式 AI 黑科技上手体验、蔚来试驾等你来！| AICon</title>
            <link>https://www.infoq.cn/article/u9llmlvM1dUTiCLcf2mg</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/u9llmlvM1dUTiCLcf2mg</guid>
            <pubDate></pubDate>
            <updated>Thu, 09 May 2024 09:52:28 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 人工智能, AICon, 企业
<br>
<br>
总结: 大模型的崛起为企业带来新的增长机遇，尤其是中小型企业找到更优解决方案，提升产品体验并引发颠覆性创新。各行各业都在悄然改变中，利用大模型设计装修方案、运用于营销推荐、改变交互方式等。如何将大模型应用于企业是焦点，AICon将探讨大模型的开发与应用实践，为工程师、产品经理、数据分析师等人群提供深入探讨的机会。 </div>
                        <hr>
                    
                    <p>大模型的崛起为众多企业带来了新的增长机遇，尤其是中小型企业找到了更优解决方案，提升了产品体验甚至引发了颠覆性创新。各行各业都在悄然改变中，建筑业利用大模型设计装修方案，金融领域则运用它进行营销推荐，汽车业也在改变交互方式等等。甚至股神巴菲特都将人工智能与核武器相提并论，可见大模型的出现备受瞩目。</p><p></p><p>如何将大模型应用于企业，是许多人关注的焦点。InfoQ 将于 5 月 17 日至 18 日举办 AICon 全球人工智能开发与应用大会暨大模型应用生态展·2024，此次会议面向工程师、产品经理、数据分析师等人群，将深入探讨大模型的开发与应用实践。我们总结了本次 AICon 的五大精彩亮点，帮助你快速 get 为什么这场活动不容错过！</p><p></p><p></p><h4>看点 1：Keynote 阵容豪华</h4><p></p><p></p><p>在本次的 Keynote 环节，我们邀请了多位学术界、业界大咖为大家分享前沿认知，目前已经确认的演讲有以下内容：清华大学电子工程系教授、系主任兼无问芯穹发起人汪玉将探讨可持续智能的概念，引领听众深入思考大模型高能效系统的未来前景；Lepton AI 联合创始人兼 CEO 贾扬清将带我们回顾互联网到人工智能的转型过程，展望云产业与 AI 融合的新时代；而数势科技创始人兼 CEO 黎科峰将分享大模型时代下的数据分析新趋势，探讨大模型技术在企业数字化转型中的关键作用；此外，北京智源人工智能研究院副院长兼总工程师林咏华将带领大家探索大模型背后的荆棘之路。</p><p></p><p>与此同时，亚马逊云科技的全球生成式人工智能产品营销总监，曹志斌博士，将发表题为《The Next Wave：Explore the Strategy of Generative AI》的深度演讲。他将详尽剖析下一波生成式人工智能技术浪潮中，我们应采纳的前沿策略与核心应对机制。最后，腾讯杰出科学家兼混元大模型技术负责人之一刘威将揭示腾讯在混元大模型技术与应用实践方面的最新进展，为大家打开人工智能领域的创新路径。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/02/024b2cb22eab7990535f6b617e48f8cf.jpeg" /></p><p></p><p></p><h4>看点 2：60 余场大模型产品设计 、技术实践及前沿洞察</h4><p></p><p></p><p>AICon 一共设置 14 个专题论坛，其中包括 AI Agent、RAG 检索与生成、Copilot 应用构建、大模型训练以及推理优化、基础设施构建、LLMOps、多模态大模型、大模型 + 行业创新应用、AI 前沿探索以及大模型全球化机会和挑战等，来自 Google、微软、字节、阿里、科大讯飞、智谱、月之暗面、MiniMax 等行业头部企业的 60 余位嘉宾将带来精彩分享。</p><p></p><p>其中，AI Agent 论坛专家团队包括微软的卢建晖、清华大学的李鹏、华为云的陈星亮、机器姬的刘智勇、阿里巴巴的严明、喜马拉雅的吕睿韬、以及阅文集团的马宇峰。卢建晖将介绍如何利用 SLM 结合边缘设备构建 AIoT Agent；李鹏将探讨面向开放域的大模型智能体；陈星亮将分享华为云在企业生产场景中 AI Agent 的技术实践；刘智勇将讲述家用具身智能机器人推理和训练 Agent 的落地应用；严明将分享模型协作智能体到个性化智能体的技术应用实践；吕睿韬将探索喜马拉雅在音视频创作中 AI Agent 的创新应用；马宇峰将介绍基于 Agent 的内容生产辅助技术。</p><p></p><p>而在大模型 + 行业创新应用论坛，九位嘉宾即将分享在不同场景下的大模型技术应用与实践经验。他们探索了教育领域的大模型翻译与写作辅导、法律领域的智能合同与法律服务、居住领域的图像生成与装修灵感图生成，以及金融领域的办公智能化应用等，展示了大模型在不同行业中的广泛应用和实际成果。现在大会两天日程已 100% 上线，更多详细议题内容参考如下完整日程↓</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/aa/aa986cdd1daad9d7cd9f617536a28ebb.jpeg" /></p><p></p><p></p><h4>看点 3：深度交流之晚场圆桌 &amp; 三场高端闭门交流会</h4><p></p><p></p><p>我们将于大会第一天 5 月 17 日晚 18:30 至 20:00 举办一场公开的圆桌交流，主题是《AI 智能体落地的挑战与应对策略》。这次交流对所有 InfoQ 粉丝免费开放！我们邀请了四位业内专家，他们将与大家分享他们的经验和见解，并与听众互动探讨。你可以通过扫描下方二维码报名参加。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ab/ab14239eb412f82af5358835703774f2.png" /></p><p></p><p>与此同时，我们策划了三场高端闭门交流会，主题分别是《大型语言模型在硬件类产品的应用落地与解决方案探讨》《行业大型语言模型与技术融合的路径探索》《人工智能在金融场景中的技术应用与实践》。这是一个难得的机会，名额有限，机会不多，作为出席嘉宾，每位都需要参与发言。</p><p></p><p>为确保与会者的专业性和质量，闭门交流会采取审核制度，我们希望您在企业任职副总裁以及 CXO ，且在对应领域拥有丰富经验。另外，成功参与本次闭门讨论的专家将获得 AICon 两日通票，解锁日程页面上所有内容。如果您有意参与，请通过下方报名二维码选择对应的闭门交流会。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/06/066d824f4601c6cf00f206568fe3bfcb.png" /></p><p></p><p></p><h4>看点 4：中国 AGI 市场发展研究报告 2024</h4><p></p><p></p><p>AGI 究竟是什么？AI Agent 如何助力人工智能走向 AGI 时代？中国 AGI 市场规模几何？四层结构是如何释放 AGI 技术潜力的？目前又在营销、金融、教育、零售、企服等场景又有哪些典型应用和案例？</p><p></p><p>带着这些问题，InfoQ 研究中心深入分析了当前大模型在各行业的应用案例，同时与众多行业和技术专家进行了深入的对话和交流，并将相关成果以 《中国 AGI 市场发展研究报告 2024》 的形式呈现给大家，旨在为行业决策者和技术开发者提供一份宝贵的参考和指南，帮助大家更好地理解大模型在行业中的实际应用和未来发展的趋势。</p><p></p><p>《中国 AGI 市场发展研究报告 2024》将在 5 月 17 日 AICon 现场隆重发布。这不仅是一场科技的盛宴，更是一次思想的碰撞。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/31/3101af70d9c6f9e94d91b84eda8fec0b.png" /></p><p></p><p></p><h4>看点 5：大模型应用生态展 = 猎奇 AI 智域×重磅产品秀场</h4><p></p><p></p><p>AI 和大模型正在渗透并赋能千行百业，与每一个人息息相关。本次 AICon 除了延续高浓度的技术内容外，我们还特别设置了大模型应用生态展，带大家一起猎奇 AI 智域，探索生成式 AI 的未来可能。</p><p></p><p>在汇聚了神秘科技力量的未来集市中，时间与空间交织成一张由 AI 编织的网，各种各样的 “智域” 存在其中，分别代表着 AI 技术在不同场景的领先应用。参观者将穿梭于这些智域，沉浸式体验科技如何塑造未来生活。</p><p></p><p>参展商们将在这里秀出他们的 AI 产品力，提供真实的产品形态和可动手操作的体验设备。这里先给大家剧透一下：讯飞将带来星火大模型 sparkdesk 问答机器人，你可以上手 AI PPT 生成 / 文档问答体验，感受 AI+ 办公所带来的生产力变革；Rokid 将设置灵境虚拟展，你可以体验虚实结合的空间计算游戏，比如保卫农场、完美弧线、飞镖大赛等；商汤则会带来主打“自动生成代码”的代码小浣熊和“聊着天就把数据分析做了”的办公小浣熊产品，并且有望把前不久刚推出的小浣熊·代码大模型一体机带到现场；更有“造车新势力×智驾领航者”蔚来汽车神秘展车“参会”，并为现场观众提供限量试驾名额，敬请期待！（温馨提示：希望申请试驾的同学提前准备好本人电子 / 纸质驾照）</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/87/875afefbf5532c3792f15d27cddfd993.png" /></p><p></p><p>除此之外，本次 AICon 现场展区还会有一辆满载亚马逊云科技生成式 AI 黑科技的大巴车抵达。你可以在 AICon 现场探索亚马逊云科技生成式 AI 的过去、现在和未来；体验全新发布的生成式 AI Demo ；在构建者游乐场脑洞大开，操纵 Amazon Bedrock 生成应用程序 ……</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/0d/0d1c2d15b122853b715c3f21e1ab93bc.jpeg" /></p><p></p><p>同步预告，5 月 29-30 日，亚马逊云科技中国峰会将在上海世博园正式开启！免费报名现场体验最前沿构建产品，感受创新技术！参会请添加官方小助手微信【aws102466】进行咨询。</p><p></p><p>在现场的【OpenTalk】区域，多位专家大咖将与你面对面分享、交流，目前已经确定的议题包括：疯狂之旅——进击的开源大模型、基于混合检索赋能 RAG 和 Agent 应用、商汤大模型在应用场景的落地实践、数据开源如何赋能全球 AI 开源开放生态、讯飞星火大模型应用生态创新实践等。</p><p></p><p>我们还在展区策划了【Workshop】区域——智能编码工具体验区！我们邀请了多家智能编码产品厂商提供现场体验机会，无论你是资深软件工程师还是代码新手，都能在这里找到提升编码技巧的灵感和工具。在体验区内，你将有机会亲手试用多家智能编码工具产品，体验如何通过自然语言处理技术自动生成代码，以及利用 AI 进行代码审查和优化。别错过这个学习、探索和创新的绝佳机会。期待在体验区与你相见！</p><p></p><p>准备好了吗？5 月 17 日，AICon 将在北京盛大启幕！门票即将售罄，扫描下方海报二维码购票咨询，和我们一起探索 AI 的未来！期待与你在会场交流～</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/ae/aedcaedf352584660498cd0ad6d87fdf.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Xq1pF1uAs4MdcoCTpR0m</id>
            <title>“驯服”不受控的大模型，要搞定哪些事？| 专访达观数据副总裁王文广</title>
            <link>https://www.infoq.cn/article/Xq1pF1uAs4MdcoCTpR0m</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Xq1pF1uAs4MdcoCTpR0m</guid>
            <pubDate></pubDate>
            <updated>Thu, 09 May 2024 07:34:40 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: GPT, 大模型, 可控性问题, RAG
<br>
<br>
总结: 大模型在各行各业受到热烈追捧，但可控性问题成为落地难点。通过RAG技术等方式提升大模型的可解释性和可操作性，同时结合知识图谱等方法提高可控性。知识图谱和大模型相互补充，而大模型与知识图谱不会相互替代，二者在不同场景下发挥作用。 </div>
                        <hr>
                    
                    <p>作者&nbsp;|&nbsp;华卫</p><p>采访嘉宾｜王文广，达观数据副总裁</p><p>&nbsp;</p><p>GPT&nbsp;爆火一年多后，无论在国内、外，“几乎所有领域都需要用大模型重构”的论调已深入人心。中国&nbsp;200&nbsp;多家厂商掀起的“百模大战”、层出不穷的千亿、万亿大参数模型、性能效果与应用方向的飞速迭代，无一不在表明大模型被各行各业拥抱的热潮力度。但在更多行业对大模型跃跃欲试之际，也有许多现实的落地问题浮现出来，可控性问题就是其中之一。</p><p>&nbsp;</p><p>在&nbsp;5&nbsp;月&nbsp;17&nbsp;日即将召开的&nbsp;<a href="https://sourl.co/faYrKr">AICon&nbsp;全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"上，InfoQ&nbsp;邀请到了达观数据副总裁王文广做演讲分享，他将从大模型相关技术和幻觉问题为切入点，探讨如何利用知识图谱、RAG&nbsp;和大模型融合的技术路线提高大模型的可解释性、可操作性和可控性。会前，InfoQ&nbsp;对王文广老师进行了专访，听他先聊一聊大模型的不可控之处和对技术路径的应用判断。</p><p>&nbsp;</p><p>以下为访谈实录，经编辑。</p><p>&nbsp;</p><p></p><h1>大模型“不受控”在哪？</h1><p></p><p>InfoQ：说到可控性问题，现在大模型在哪些方面的输出是“不受控”的？</p><p>王文广：大模型输出的内容是根据用户输入的prompt去生成的，是由模型本身能力来决定的，如果要从细节上来控制模型的输出是不可能的。也就是说，大模型本质上是不可控的。实际应用来讲，大模型的不可控可以从两方面来讲：一是输出的内容与用户预期一致就是是可信的，跟预期不一致就是所谓的幻觉；二是可控性在使用时未必一定是需要的，比如说写小说写剧本等，即使天马行空也没什么大问题。</p><p>另外对中国的用户来讲，还有些场景下，可控性是要求很高的。比如有时候会要求必须一字不差地按照给定的内容输出时。但可控性与幻觉又是不同的概念，幻觉是跟事实不符，可控性则是跟预期是否一致。</p><p>&nbsp;</p><p>InfoQ：可控性问题是大模型目前落地的最大阻碍吗？业内现有的大模型产品达到什么样的效果？</p><p>王文广：不能完全说是障碍，要分场景的，只是在部分可控性要求高的场景下是障碍，比如制造业、金融领域的应用等。也就是说，对输出结果的精确度要求越高，可控性的影响越大。</p><p>我认为大模型追求的目标并非可控性，而是模型自身的能力。它的智能化水平与可控性并没有很强的关联，越强的大模型，未必可控性一定做得越好，但可控性可以用别的方法去做。</p><p>&nbsp;</p><p>InfoQ：从安全和合规层面来说，整个行业如何能够共同推动大模型的可控？</p><p>王文广：这个主要还是要由大模型的提供商来解决这个问题，要保证输出的内容适应各地的法规、习惯、隐私和道德要求。</p><p></p><h1>主流的三种应对方式</h1><p></p><p>&nbsp;</p><p>InfoQ：要解决可控性问题，需要在大模型的哪些方面努力？</p><p>王文广：这个有比较多的方法，大家用的最多的是&nbsp;RAG（检索增强生成）技术，把需要的东西检索出来，然后通过提示词的方法输入到模型里。还有的会采用分析神经网络里的激活链路的方式，这个比较难且成本非常高，所以可能真正用得不太多。</p><p>&nbsp;</p><p>InfoQ：目前行业内在可控性问题的解决上，普遍采用哪些方式？</p><p>王文广：普遍用的就是&nbsp;RAG&nbsp;，特别是在应用里，但&nbsp;RAG&nbsp;本身也会有几方面的细分内容。一是搜索引擎，用这一方法去找到答案的大致范围，然后再通过提示词输入到大模型里，让它给出答案；二是向量数据库，用向量的方法去检索内容，但相比搜索引擎来讲，其可能也存在检索效率和精度等问题。因为搜索引擎起点蛮高的，要做好一个搜索引擎并不容易。</p><p>另外就是在产业用得比较多的知识图谱，它的好处对业务有很多预定义的结构，能够更方便地找到精确答案，然后再利用大模型把答案生成一段合理文本来回答。</p><p>主流来讲就是这三种方法：搜索引擎检索、向量检索和知识图谱增强。应用来说，偏通用的领域前两者比较多，在专业领域知识图谱更好一些。</p><p>&nbsp;</p><p>InfoQ：知识图谱能为大模型可控带来多大的提升？在曹植大模型上的运用效果如何？</p><p>王文广：知识图谱和大模型是一个互补的关系。从原理上来讲，大模型本质上我们称之为归纳推理的结果，而知识图谱更多是演绎推理；从实用角度来讲的话，大模型是概率输出，无法精确控制，同时即使出错也无法进行编辑，知识图谱恰好能做修改的事，可以在里面写确定性的逻辑。知识图谱的劣势是构建成本高、有很多结构化的成本、逻辑推理要求能够理解业务，而这正是大模型所擅长的，比如说可以用大模型去做知识图谱的构建、语言的理解。两者的结合，刚好可以实现一个高度智能化且能够落地应用的系统。知识图谱和曹植大模型融合在效果上是非常好的，被金融、制造、能源等广泛的行业客户所接受。</p><p>&nbsp;</p><p>InfoQ：RAG能为大模型可控带来多大的提升？在曹植大模型上的运用效果如何？</p><p>王文广：最大的提升方向是，用这一方法去提升大模型，相当于把开放性的题目变成选择题。在曹植大模型的落地中，大量才用了与知识图谱融合的方法。</p><p>&nbsp;</p><p>InfoQ：对于RAG本身的局限之处，在大模型可控的应用实践中如何避免？</p><p>王文广：要做大模型落地，RAG技术是不可避免会遇到的，用别的技术方法只会更难或者效果达不到预期。具体的局限之处要看方法，RAG&nbsp;的三个方向各自都有其难点所在。搜索引擎的局限在于复杂性，搜索引擎是一个庞大的复杂系统；向量检索乍一看非常简单，但可控性非常差，遇到问题没法去更改，在落地的时候往往会发现，细节是魔鬼，越到后面越没法用；知识图谱和搜索引擎一样是很复杂的知识体系，学习起来都很复杂，而且一个知识图谱往往是针对不同的业务去做的，很难构建起全面的知识图谱。</p><p>我们现在的做法，是在一个系统里把这三种方法都用起来，每一种方法都有弱点，那就用别的方法去补充。如果只会其中一种方法，顶多就60分吧，其实挺难做好的的。</p><p></p><h1>单靠大模型，永远达不到预期</h1><p></p><p>InfoQ：大模型与知识图谱之间有不少重叠的应用能力，二者会相互替代吗？</p><p>王文广：我觉得它们永远不会相互替代。举例来说，人类已经很聪明了，但需要精确的专业知识时还是需要去查百科全书。对大模型来讲也是一样的，它也不可能记住所有东西，特别是专业领域的知识，所以我经常说，知识图谱是大模型的百科全书；并且，大模型也需要更新，越大的模型更新越慢，训练也需要时间。所以大模型总需要某种方法来补充信息，知识库就是一个很好的选择。所以，我经常说，书籍是人类进步的阶梯，知识图谱就是大模型（人工智能）进步的阶梯，哈哈。</p><p>&nbsp;</p><p>InfoQ：大模型是否能反哺知识图谱的构建与发展？基于大模型的知识图谱能统一吗？</p><p>王文广：最直接的影响是，现在有了大模型以后，知识图谱的一些研究方向已经不再做了，比如问答。因为大模型在这些方面做得挺好，互相组合去做就可以了。随之带来的影响就是，大家可以有更多精力做知识图谱的其他方向，比如说推理，这可能也是未来知识图谱会融合大模型去做的一个研究方向。</p><p>&nbsp;</p><p>InfoQ：现阶段以及将来有哪些技术可以助力提高大模型的可控性？</p><p>王文广：目前来讲我觉得主要就是刚刚提到的三个方法，还有就是大模型本身能力的增强，比如训练一个针对特有领域的技术，可用但成本比较高，而且在语言模型里面好像大家做得不太多，可能还是效果没那么好。</p><p>&nbsp;</p><p>InfoQ：您认为大模型在可控性上达到业界和大众的普遍认可，还需要多长时间？</p><p>王文广：我觉得单靠大模型很难的，也许永远都达不到大家的预期，必须结合前面说的这几种方法。因为大模型再牛，如果语料里没有相关内容（比如刚刚发生的事情），肯定是答不好的。举一个例子，现在（2024年5月8日）问不带检索增强的大模型有关嫦娥六号的内容，肯定全是一本正经胡说八道。</p><p>&nbsp;</p><p>InfoQ：在即将到来的AI&nbsp;Con上，您准备向听众分享哪些方面的内容？</p><p>王文广：我主要会讲两部分，也是大家比较关心的方面。一是具体怎么去解决可控性，我们会主要把搜索引擎、知识图谱和向量数据库组合在一起；二是我们实际在做的案例，因为现在大模型最大的问题就是怎么落地。</p><p>&nbsp;</p><p>嘉宾介绍：</p><p>王文广，现担任达观数据副总裁，高级工程师职称，浦东新区“明珠计划”菁英人才，曾获得广东省科技进步奖二等奖，上海市计算机学会科技进步奖二等奖和上海市浦东新区科技进步奖二等奖。人工智能标准编制专家，《知识图谱：认知智能理论与实战》作者，参与编撰《智能文本处理实战》，《新程序员&nbsp;*&nbsp;人工智能新十年》顾问专家和文章作者，专注于知识图谱、通用人工智能&nbsp;AGI、大模型、AI&nbsp;大工程、NLP、认知智能、强化学习、深度学习等人工智能方向。</p><p></p><p>活动推荐：</p><p><a href="https://sourl.co/faYrKr">AICon全球人工智能开发与应用大会&nbsp;暨&nbsp;大模型应用生态展</a>"将于5月17日正式开幕，本次大会主题为「智能未来，探索AI无限可能」。如您感兴趣，可点击查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8e/8efe5a129189783edbe3fc9ae3550a49.png" /></p><p></p><p>会议即将开幕，扫码可预约主题演讲直播，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/EjZvtBssmlX0bGHHjBwI</id>
            <title>“我们坚持开源！”阿里云发布“地表最强”中文大模型：半年一迭代、性能翻倍？</title>
            <link>https://www.infoq.cn/article/EjZvtBssmlX0bGHHjBwI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/EjZvtBssmlX0bGHHjBwI</guid>
            <pubDate></pubDate>
            <updated>Thu, 09 May 2024 06:48:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 通义大模型, 阿里云, 开源模型, 企业合作
<br>
<br>
总结: 通义大模型在阿里云生态中迎来重大升级，包括发布新版本、开源模型、推出企业合作等举措，成为中国企业最受欢迎的大模型。 </div>
                        <hr>
                    
                    <p>5月9日，在通义大模型发布一周年之际，阿里云大模型生态迎来一次重大升级，主要有“四个最”：</p><p>&nbsp;</p><p>通义千问2.5正式发布，“模型性能全面赶超GPT-4&nbsp;Turbo，成为地表最强中文大模型”；Qwen1.5-110B参数开源模型在多个基准测评收获最佳成绩，超越Llama-3-70B，成为开源领域最强中文大模型；ModelScope魔搭成为中国最大的开源社区；通义大模型通过阿里云服务企业超9万，成最受中国企业欢迎大模型。</p><p>&nbsp;</p><p>同时，阿里云对通义大模型的品牌也进行了升级，正式将“通义千问APP”更名为“通义APP”，集成通义大模型全栈能力，免费为所有用户提供服务。阿里表示，通义APP将把通义实验室前沿的文生图、智能编码、文档解析、音视频理解、视觉生成等能力“All in one”，成为每个人的全能AI助手。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b5/b53cee5c7ba0a274657f8dde008a8153.png" /></p><p></p><h2>大模型系列更新</h2><p></p><p>&nbsp;</p><p>从初代模型升级至2.5版本的路，阿里云仅仅走了一年。</p><p>&nbsp;</p><p>去年10月底，阿里云 CTO 周靖人在 2023 年云栖大会上，发布了参数量提升到千亿级别的通义千问 2.0。当时，阿里表示，目前通义千问的综合性能已经超过 GPT-3.5，相比4月发布的1.0版本，通义千问2.0在复杂指令理解、文学创作、通用数学、知识记忆、幻觉抵御等能力上均有显著提升。</p><p>&nbsp;</p><p>如今，相比通义千问2.1版本，通义千问2.5的理解能力、逻辑推理、指令遵循、代码能力分别提升了9%、16%、19%、10%。在权威基准OpenCompass上，通义千问2.5得分追平GPT-4 Turbo，这也是该基准首次录得国产大模型取得如此出色的成绩。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/8f/8f5ffc9f37aa1ba0838d1c47bc80c0e9.png" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>在多模态模型和专有能力模型方面，通义千问视觉理解模型Qwen-VL-Max在多个多模态标准测试中超越Gemini Ultra和GPT-4V，目前已在多家企业落地应用。</p><p>&nbsp;</p><p>通义还发布了最新款开源模型：1100亿参数的Qwen1.5-110B，该模型在MMLU、TheoremQA、GPQA等基准测评中超越了Meta的Llama-3-70B模型。在HuggingFace推出的开源大模型排行榜Open&nbsp;LLM&nbsp;Leaderboard上，Qwen1.5-110B冲上榜首。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/4c/4cfe4c00d8eb2053e62b44b9b87eba9d.png" /></p><p></p><p>“生态非常重要，我们会坚持开源体系、坚持我们的开源模式。”周靖人透露。</p><p>&nbsp;</p><p>“现在已经是 2024 年了，我相信开源对整个全球技术领域的贡献是毋庸置疑的。”周靖人说道，“阿里云不是简简单单的开源，我们是要开源最强的模型。”</p><p>&nbsp;</p><p>去年8月，通义宣布加入开源行列，随之启动马不停蹄的开源狂飙，沿着“全模态、全尺寸”开源路线陆续推出十多款模型。小尺寸模型如0.5B、1.8B、4B、7B、14B，可便捷地在手机、PC等端侧设备部署；大尺寸模型如72B、110B能支持企业级和科研级的应用，都曾登顶Open LLM Leaderboard榜首。</p><p>&nbsp;</p><p>“阿里云是全球唯一一家既持续做模型开发，又做大量模型开局模式的企业。”周靖人表示，Llama 3 等在一定程度上把竞争拉得很高。坦诚地讲，不是所有闭源的公司都能做过开源模型。做闭源的，至少要能够超过开源模型水准，才能今天有机会参与。另一方面，实践中，大模型能力并不是越强越好，还要考虑成本等方面因素，所以阿里云希望把选择权给到企业和开发者。</p><p>&nbsp;</p><p>通义千问代码大模型CodeQwen1.5-7B则是HuggingFace代码模型榜单Big Code的头名选手，具备优秀的代码生成能力、长序列建模能力、代码修改能力和SQL能力，还是国内用户规模第一的智能编码助手通义灵码的底层模型。</p><p>&nbsp;</p><p>现场，阿里还宣布推出通义灵码的企业版，满足企业用户的定制化需求，帮助企业提升研发效率。</p><p>&nbsp;</p><p>据介绍，通义灵码熟练掌握Java、Python、Go、JavaScript、TypeScript、C/C++、C#等200多种编程语言，可以辅助写代码、读代码、查Bug、优化代码等。根据官方数据，2023年10月发布至今，通义灵码的插件下载量已超350万，每日推荐代码超3000万次，被开发者采纳的代码超亿行。</p><p></p><h3>“最受中国企业欢迎的大模型”</h3><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/12/121b60266f3aa5e8f245e489e71e41c0.png" /></p><p></p><p>&nbsp;</p><p>根据阿里云公布的最新数据，通义大模型通过阿里云服务企业超9万，通义开源模型累计下载量突破700万。通义落地应用进程加速，现已进入PC、手机、汽车、航空、天文、矿业、教育、医疗、餐饮、游戏、文旅等领域，成为最受中国企业欢迎的大模型。</p><p>&nbsp;</p><p>周靖人表示，各行各业、各个企业都是特殊的，非常明确的就是让基础模型直接对接业务需求是很难的，因此现在的大模型落地一定要结合业务的场景。</p><p>&nbsp;</p><p>小米旗下的人工智能助手“小爱同学”已与阿里云通义大模型达成合作，强化其在图片生成、图片理解等方面的多模态 AI 生成能力，并在小米汽车、手机等多类设备落地。此外，微博、众安保险、完美世界游戏等企业也宣布接入通义大模型，将大模型应用于社交媒体、保险、游戏等领域。</p><p>&nbsp;</p><p>更早之前，新东方、同程旅行、长安汽车、西部机场集团、亲宝宝等企业也都与通义大模型达成合作。中国科学院国家天文台人工智能组基于通义千问开源模型开发了新一代天文大模型“星语3.0”，这是大模型首次应用于天文观测领域；陕煤建新煤矿等十余座矿山推出由通义大模型支持的新型矿山重大风险识别处置系统，成为大模型在矿山场景的首次规模化落地。</p><p>&nbsp;</p><p>截至目前，通义大模型通过阿里云服务企业超过9万、通过钉钉服务企业超过220万。与此同时，海内外大量中小企业和开发者以下载开源模型的方式使用通义，在HuggingFace、魔搭ModelScope等开源社区，通义开源大模型的累计下载量超过了700万。</p><p>&nbsp;</p><p>在阿里云体系里，把模型能力和业务场景结合起来的纽带就是百炼平台。本次大会上，百炼升级成为阿里云承载云+AI能力的重要平台，提供一站式、全托管的大模型定制与应用服务。开发者可通过“拖拉拽”5分钟开发一款大模型应用，几小时“炼”出一个专属模型，把精力专注于应用创新。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/b8/b8f973e4c1cf7b517f69530b00c460d7.png" /></p><p></p><p>周靖人介绍，当下企业应用大模型存在三种范式：一是对大模型开箱即用，二是对大模型进行微调和持续训练，三是基于模型开发应用，其中最典型的需求是RAG，以企业数据对大模型进行知识增强。围绕这些需求，百炼打造了模型中心和应用中心，提供最丰富的模型和最易用的工具箱。</p><p>&nbsp;</p><p>百炼提供了提示词模版、拥抱开源框架，包括提供开放灵活可配置的检索增强应用服务、提供实时API等，此外还提供可视化流程，提供微调和评测。</p><p>&nbsp;</p><p>此外，百炼集成了上百款大模型，除了通义、Llama、ChatGLM等系列，还托管百川等系列三方模型，覆盖国内外主流厂商，联动魔搭开源社区，同时支持企业上架通用或行业模型，给开发者提供足够多的模型选择。</p><p></p><h3>结束语</h3><p></p><p>&nbsp;</p><p>问世一年多来，通义大模型发展出了文生图、智能编码、文档解析、音视频理解等能力，企业客户和开发者可以通过API调用、模型下载等方式接入通义，个人用户可从通义APP、官网和小程序免费使用通义家族全栈服务。</p><p>&nbsp;</p><p>未来，阿里云的AI之路会走得如何，我们也拭目以待。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/0lHZzGZbGuf0tk32qlkL</id>
            <title>2024春季火山引擎FORCE原动力大会，5月15日开幕</title>
            <link>https://www.infoq.cn/article/0lHZzGZbGuf0tk32qlkL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/0lHZzGZbGuf0tk32qlkL</guid>
            <pubDate></pubDate>
            <updated>Thu, 09 May 2024 03:58:08 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 2024年, 大模型, AI, 火山引擎
<br>
<br>
总结: 2024年春季火山引擎FORCE原动力大会聚焦AI主题，展示大模型在各领域的应用，发布火山引擎自研大模型和服务平台升级，探讨AI时代的转型机遇，助力企业数智化转型。 </div>
                        <hr>
                    
                    <p>2024年，大模型发展步入新阶段。5月15日，「2024春季火山引擎FORCE原动力大会」聚焦AI主题，以大模型应用为核心、以AI落地为导向，展示火山引擎在大模型、云计算领域的实践应用，携手汽车、手机终端、金融、消费、互联网等领域的专家和企业技术带头人，共同探讨AI时代的转型机遇。</p><p></p><p>大会共分为“AI增长焕新机、AI应用新范式、AI算力强护航”三个篇章。会上将正式发布字节跳动自研大模型，火山引擎大模型服务平台——火山方舟也将迎来重大升级。同时大会还将带来各行业场景下的大模型最佳实践，详细解读火山引擎模型生态战略。来自招商银行、蒙牛、OPPO、哈啰集团等客户企业的嘉宾，也将结合自身场景化应用和落地实践，与现场来宾共话AI时代机遇，让AI这束科技之光，穿透数智化转型迷雾，照亮新时代下企业转型前路。</p><p></p><p>大模型技术的发展与落地应用已步入新阶段。火山引擎将致力于通过领先的云与智能技术，打造企业数智化转型引擎，助力AI更快落地、更易于企业使用，帮助企业持续创新，激发增长潜能。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/16/44/16632994bfbf7edyyd911e5bc0128f44.jpeg" /></p><p></p><p>欢迎扫描海报二维码或点击阅读原文，预约观看2024春季火山引擎FORCE原动力大会直播！</p><p></p><p>5月15日 智能未来 由此探索</p><p></p><p>阅读原文链接：<a href="https://www.coze.cn/store/bot/7361268736772407307?panel=1">https://www.coze.cn/store/bot/7361268736772407307</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/qFI3IVzCFjeHWJlTkMoE</id>
            <title>东软集团：生成式 AI 时代，如何布局 AI 人力资源战略？</title>
            <link>https://www.infoq.cn/article/qFI3IVzCFjeHWJlTkMoE</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/qFI3IVzCFjeHWJlTkMoE</guid>
            <pubDate></pubDate>
            <updated>Thu, 09 May 2024 03:48:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 人工智能, 培训行业, AI 技术, 数字化人才
<br>
<br>
总结: 人工智能作为科技行业的热门趋势，对各行各业产生影响。培训行业需要适应AI带来的变革，提升人才水平。数字化人才发展大会探讨了AI时代的机遇和挑战，强调了AI技术的重要性。通过AI技术，企业商业模式、组织结构和人才需求都将发生变化。 </div>
                        <hr>
                    
                    <p></p><p>人工智能作为当下科技行业最火热的趋势，在各行各业的影响力都在快速升温。面对 AI 潮流的冲击，有人看到了广阔的机遇，也有人意识到了它给传统模式和方法带来的挑战。而为了迎接 AI 带来的变革，行业无疑需要水平更高的人才队伍支持，这也就给培训行业提出了更高的要求。</p><p></p><p>如何应对这样的需求，培训行业从业者如何在时代变革期间把握好机遇？日前，极客时间企业版与培训杂志联合举办的 DTDS 全球数字人才发展大会的“<a href="https://www.infoq.cn/minibook/dvqvwPZRtp3wFZ0BZdSN?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">AIGC 时代的数字化人才升级专场</a>"”，特别邀请到东软集团人力资源部上海分部部长滕新阳，发表了题为《AGI 时代，培训人如何抓住新机遇》的主题演讲。</p><p></p><p>本文整理自其演讲，内容经 InfoQ 进行不改变原意的编辑。</p><p></p><p>东软集团是中国第一家拿到 PCMM（人力资本成熟度）五级认证的公司，公司人力资源体系内部有大量体系文件和资料，形成了很成熟的知识库。对于 AI 而言，知识库、数据的积累非常重要，所以去年 AI 热潮刚刚开始，我们就马上锚定了这一技术，开始思考怎样利用好 AI 的能力。</p><p></p><p>就像扑克牌可以当作教具、魔术道具或者赢钱工具一样，工具是很重要，但使用工具的人能不能让它产生价值是更重要的事情。东软在做 AI 的过程中遇到了很多坎坷，了解了很多人和很多想法，我也用三句话来总结我的感受：</p><p></p><p>1. 不要用昨天预言明天，大家要更关注思考逻辑而非分享者内容的呈现。</p><p>2. 巨石崩裂时，有的人看见了恐惧，有的人看见了光。我希望大家通过我的分享能找到自己的价值定位和机遇。</p><p>3. 弱小和无知不是生存的障碍，傲慢才是。我们要以成长型的眼光来看待我接下来分享的应用场景。</p><p></p><p>本次分享的内容围绕三个关键词展开，那就是生成式 AI 技术时代，我们“为什么变（Why）”，“变什么（What）”和“怎样变（How）”，而核心的落脚点在于“变（Change）”。AI 不同于其他一些曾经被热炒的概念，它的热度从去年到现在是一直在持续的，说明它一定带来了一些本质性的变化，这也就引出了我们的第一个主题。</p><p></p><p></p><h2>生成式 AI 时代，为什么变？</h2><p></p><p></p><p>价值是一个链条，是会流动的，也分成表层价值和底层价值。什么叫表层价值？比如说在当年手机没流行之前研究 BB 机怎么能跟手机互联互通的人，后来的技术发展证明他错了。所以如果你只想到 AI 能够帮我做课，做数字人讲师，我觉得这是表层价值，相比之下我们更应该关注底层价值：我为什么要用 AI 做课？为什么企业需要大量课程？要关注本质发生了怎样的变化。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/958a383d9dfbf327d1cccc77d632749b.png" /></p><p></p><p>回顾人力资源发展史，在 17 世纪资本主义发展初期，谁家有钱就能供得起更多家族成员为自己务工，我只要提供吃住就可以，生意就可以做很大，但进入到电气化时代后，机器开始替代手工，那么我就需要更多非家族的成员，人力资源也因此开始强调科学管理，注重人员的产出效率。这个时候白领也出现了，为了管理他们的产出诞生了战略型人力资源的概念。</p><p></p><p>到了信息化也是我们现在的时代，规模化开始被定制化和经济化服务所替代，市场高度细分。这时我们开始追求价值，淘汰掉那些可能做不好的人。那么未来，随着人工智能时代的到来，我们会又会发生怎样的变化呢？</p><p></p><p>首先，人机协作的组合可能会替代或增强我们现有的劳动力和劳动关系，这也是为什么 HR 一定要关注 AI 技术的原因。第二，我们会有高影响力的人力资源组织，人力资源会泛职能化，而企业会更多需要一些人力资源的 AI 设计师这样的角色。其实在每个时代，人力资源的变化都代表那个时代当时缺少的东西，机械时代下它追求的是效率，其实代表它缺少好的流程；电气化时代下它追求的是质量，缺的是好的领导者；而在信息化时代我们追求价值，反过来讲缺的是数据，因为数据是可以精确量化价值的。而在未来的 AI 时代，我个人认为可能会追求创新，代表我们在这个时代下缺一些新质生产力。</p><p></p><p>从因果和时间关系推演，你就会发现我们是被迫的，从来都不是主动的。所以我们为什么要关注技术？因为技术和我们之间是有因果链的。新技术不是先改变人力资源，而是先改变了企业、商业模式，有了这个商业模式，肯定会有人把它扩大，就希望有更多的人过来帮我赚钱。所以他有了组织之后就有了对人才的要求，人力资源管理也自然而然会受到影响，开始变革。所以技术、组织、人才之间是链状关系。</p><p></p><p>反过来讲，HR 在技术的迭代过程中往往会在浪尖上，所以我们要做两件事情，第一就是你的思维要在浪尖上，第二是在浪拍下来之前学会冲浪，平稳着陆。这就是我们所说的为什么要改变。</p><p></p><h2>什么会变？</h2><p></p><p></p><p>接下来我们说什么改变了？我们刚才提到了商业组织的商业模式、组织和人才。</p><p></p><h4>&nbsp;商业模式</h4><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5c9be91fc5589fbbb91e50bfede896c4.png" /></p><p></p><p>那么首先商业模式会不会发生变化？麦肯锡和埃森哲这里有两份报告告诉我们，企业至少在业务领域采用 AI 技术的比例从 20% 提升到了 50%，未来一定会有加速式的变化；同时营业收入中由 AI 推动的份额由 12% 提升到了 21 年的 25%，翻倍式的增长，所以企业现有的商业模式很有可能会因为 AI 的到来发生巨大的变化。</p><p></p><h4>组织结构</h4><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/5a/5a3f9a0377cb1c28ce36776f5eadabae.png" /></p><p></p><p>组织结构会不会发生变化？回顾历史，信息时代最重要的变化都是计算平台的迁移。比如当年 IBM 推出了第一台个人计算机 5150，它的交互模式是鼠标的拖拉拽。那么在那个时代下，你会发现 SAP、Workday 都提出了 EHR 的概念，叫人力信息化系统，也就是把员工数据进行分大类的管理。</p><p></p><p>比如说我企业的信息化系统提取出员工的档案，可以知道他多大年龄？是男是女？什么专业？在哪工作？在哪个岗位工作？月薪多少？但如果我接下来再问，你说他喜欢打篮球还是喜欢踢足球？他跟怎样的领导人在一起绩效产出高？在什么业务场景下离职率高？可能这个系统给不了你们答案。所以 EHR 系统是分大类的数据管理，打造的是科层式组织，在那个时代很多组织都是科层式的，不同层级间信息壁垒是非常强的。</p><p></p><p>iPhone 的诞生标志着计算平台从电脑进化到了手机，它带来的变化是数据的精细化。手机可以知道你的 GPS 定位，可以知道你的模糊搜索进而给你推荐，实现个性化等需求。所以 10 年左右出现了 DHR 的概念，就是基于更多细分数据让你能够更了解你的员工，并对这些员工进行管理。也就是说你应该知道你的员工的个性，知道他喜欢怎样的领导，在怎样的环境下绩效产出比较高，甚至欠缺了什么技能。</p><p></p><p>我个人认为它会打造菱形组织的概念。以前的科层式组织中，中层、高层底下有无数个细分的事业部；而菱形组织就更聚焦于上面的 vision，有人告诉你我们在这个方向能赚钱，下面还会有 RPA 来替代你的可重复工作，再用 AI 给你的选人、奖惩提供一些推荐和建议。中间就变成了无数个像蜂巢一样的菱形，比如说企业在找钱的道路上遇见问题，我觉得我行，那我做 PM，然后我跟 AI 去说，你给我在企业里面推荐，我需要 3 个程序员，两个财务、法务、招聘，请你帮我把合适合格的人选推荐给我，组成团队，解决了这个问题后团队就解散，人员等待下一个项目。这就叫菱形组织。</p><p></p><p>在第二代计算平台的时代，它被很多原生互联网大厂所接受。菱形组织的特点是卷，在组织中如果你不被邀请，要么你活不行，要么你人缘不好，你就会被这个组织刷下去，所以它天然就带卷的特性。</p><p></p><p>第三个计算平台，我认为标志是去年 Vision Pro 的发布，元宇宙、虚拟现实很有可能会给我们的组织带来 DAO （去中心化组织）的变化。我们能发现的趋势就是员工的个体占比越来越强，组织的框架管理越来越弱。从科层到菱形，你会发现超级员工在组织里更容易凸显出来。</p><p></p><h4>生产力结构</h4><p></p><p><img src="https://static001.geekbang.org/infoq/2b/2b91396890ce009eac38b1b2fc0f260c.png" /></p><p></p><p>生产力结构会不会发生变化？OpenAI 之前发布了行业暴露度报告，大家就开玩笑说，你会发现体力劳动者的暴露度都很低，白领暴露度都特别高，比如作家、口译等，那我们这些坐办公室的都会被 AI 替代。但实际上我们认为暴露度的概念并不是替代性，而是增强性（Enhancement），就是暴露度高的角色和岗位很有可能在未来出现超级员工的概念，出现超级员工的几率会特别高。这就代表企业培养人才，可能变相培养了超级竞对，到最后他自己出去加上 AI，三个人成立一家公司，跟你抢单、抢客户。</p><p></p><p>未来的 AI 时代可能会是 20% 的人和机器替代完成 80% 可重复、可套路、有结构的工作，而剩下的 80% 的人要靠人机协作增强去完成 20% 和人类相关的、非结构化的工作，而这个部分的工作会消费大量的时间和精力。所以未来哪个地方在 AI 到来时会比较安全？我们要找增强的部分，不要找替代的部分。</p><p></p><h4>人力资源</h4><p></p><p></p><p>人力资源的使命会不会发生变化？我们提了几个关键词，前两个是“效能增长为导向”和“高效的工作环境”，这是传统人力资源的核心，不赘述。后面三个关键词是区别，是差异化的。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f0/f0a7760495a6694e91048e8be834f394.png" /></p><p></p><p>首先，企业能不能打造数字包容的工作场所，这件事情很重要。这里面最大的障碍是中层，因为老板现在可以知道员工是怎样，不需要通过经理层去知道这件事情，所以中层会消失，因此它是最大的阻碍。关键点也在他们的身上，他们能不能创造数字包容的工作场所很重要。</p><p></p><p>第二个关键词是个性化员工体验创造，我们刚才提到了 digital HR 就是在讲数据的细分，体验创造就是帮你挖掘出来更好的、更加干净的数据。</p><p></p><p>第三个关键词叫关注员工的可持续发展。随着超级员工时代的到来，你不想教他两年，他就出去当你的竞对吧？所以现在这个情况下要关注企业和员工的共赢。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/774df2630ea1437f8ec3e945a62d1fa2.png" /></p><p></p><p>德勤去年就发布了报告叫未来人力资源，提到了管理的核心要从组织到人，要可持续发展、敏捷，要全域学习和流程优化我们 HR 的泛职能化架构，用体验架构释放价值和生产力。尤其是当下，我们面对 Z 世代的时候，如果你的体验架构做不好，很有可能员工下班兼职赚得比你工作还要多，那我为什么要把精力和价值产出放在你的这 8 小时里面？最后还有利用人机协作来提高质量和速度。</p><p></p><p>现在我们再来看“新质生产力”这个词。这里有生产力三要素：劳动者、劳动资料和劳动对象，它们会往更高素质、更高技术含量和更广范围这三个方向变革。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e7/e75cafa3d3bddab1cca6b79393e30b30.png" /></p><p></p><p>同样对于人力资源来说，变革也要从劳动者、劳动关系、劳动资料这三个点下手。以京东仓储为例，以前仓储的效率取决于小哥跑的多快，因为那个时候都是推小车，他跑的有多快决定了你这个单的效率。那个时候也有大屏，就是有个人看着屏幕拿个麦克风喊，几号，你跑慢了，快点跑。</p><p></p><p><img src="https://static001.geekbang.org/infoq/5c/5ca67244afcc839117d9de3f4acc1831.png" /></p><p>后来这些仓储的小哥有一天开始学 Java、学开发，然后就变成了我们今天的黑灯工厂，这就是京东的新质生产力。它的劳动者从人变成了机器，劳动资料从效率数据变成了利润数据。以前我追求的是效率，现在我从大屏上不看效率了，我直接看成交额涨了多少。劳动对象从货变成了客户，他从关注货的效率变成了关注客户的体验，所以他整个的新质生产力都得到了巨大提升。那么我们也要通过这三个框架来追求新质生产力。</p><p></p><h2>怎么变？</h2><p></p><p></p><h4>关键人才</h4><p></p><p></p><p>最后这部分来谈怎么变？我们在去年 4 月份就成立了阿波罗创新实验室，由几个很感兴趣的 HR 小伙伴组成的，他们都是纯 HR 背景，也就是说不会代码。所以大家不用担心说，是不是会代码的人才能继续去做 HR 的 AI 研发，并不是这样的。我们在做一件事情，叫数字生命研发专项，就是想打造一些数字员工。</p><p></p><p>结合我们前面谈到的框架，首先来看劳动者，就是企业在 HR AI 的转型过程中定义了两类人才，第一类叫重要人才，第二类叫关键人才。关键人才是打造差异竞争力的核心人才。比如说，航空公司的飞行员就是重要人才，没有他飞机就飞不了。但比如春秋航空主打廉价，它的差异化竞争力来自运营数据管理，而东方航空主打服务，差异化竞争力来自空姐，那么这块的人才就是关键人才。</p><p></p><p><img src="https://static001.geekbang.org/infoq/63/6396502ab1f70216f401946b086f5309.png" /></p><p></p><p>所以我们内部把 HR AI 转型的人才队伍分成了两类，关键人才是可以独立完成 AI 的，负责应用场景的创新和创造，可能不需要具备太强的业务场景经验；而重要人才就是那些很有经验的人，他可能不懂 AI，但他能在这个业务场景里快速抓到痛点。</p><p></p><p>我们也提出来一个创新理念叫原子级替代。你会发现创新这件事情其实可大可小，当你把现在企业内的人力资源现有活动拆解成不能再拆解的原子级活动，再看一下这个原子级活动能不能被 AI 替代或者增强，这其实是创新的本质。我们当时分析了生成式 AI 能做什么事情，本质上替代了一些怎样的工作，这些工作在哪些原子级活动里面会被替代或者是被增强了？（其实替代和增强的概念很简单， 100% 就叫替代， 80% 叫增强。）然后这些原子级活动反过来是被哪些上层的人力资源管理活动所征用？</p><p></p><p>我们在内部现在梳理出来 200 多个原子级活动，里面有部分是替代，有部分是增强。我们还在做人力资源自己的暴露度公式，比如这里面有 10 个原子级活动，有 8 个会被 AI 替代的话，那你的增强度就是 80%。</p><p></p><h4>关键场景</h4><p></p><p></p><p>强调完关键人才，我们讲关键场景。我们大家在找关键场景时，一定要有对现有应用的批判性创新思维才可以。并不是说这个东西拿过来用它就有价值，或者说大部分人这么用它就有价值。我们也找了一些关键的场景，列举了 HR 的数字员工体系。这个体系的第一个部分叫 Jarvis，是 AI 打造的个人超级助理，它可以收集到更多员工数据，让我们更加了解员工。第二个叫菩提，是员工的学习助手。最后一个叫魔镜，是一个 AI 推荐助手，可以给我们推荐谁适合什么岗位，谁最适合晋升，给我们提供全面的建议和指导。</p><p></p><p><img src="https://static001.geekbang.org/infoq/95/95421cd9fa9f3f839cfd3ecae48d108d.png" /></p><p></p><p>这次我们重点讲一下菩提，它是一个学习助手，就是对知识的增强，主动推送检索结果，不是让人找知识，而是让知识找人。它来自于我们对企业现有学习工具或学习方法的替代和批判。</p><p></p><p>员工在什么情况下喜欢学习？第一种情况，我遇见事儿，我知道学技能可以短时间内提升上去，能帮我解决这个问题。但企业给你的解决方案是什么呢？他想说你遇见事儿不知道怎么解决，两眼一摸黑的情况下，希望你能够在企业线上学习平台的搜索框里精准搜索出来想要学的内容，你觉得员工能做到吗？第二种情况，在我刚入职或者晋升的时候，我希望能够通过学习达到更高的层次。那么企业给的解决方案是甩给你职业技能晋升的路线或者岗职位体系，结果对员工来讲，随着企业年龄的增长，这个体系会越来越复杂。站在员工的角度来想就是，算了，我不晋升了，太复杂了。所以你会发现它是高成本投入和低学习回报的状态。</p><p></p><p>那么我们希望菩提能做到什么事情？就是如果你遇见一件事，我希望你把这个事告诉我，我来告诉你怎么解决它，然后同时给你建议，告诉你该学怎样的课程。学完这个课程我对你进行一定的考评，看一下你对技能掌握的怎么样。掌握的好给你认证，你有了这个认证，未来可能就会被魔镜检索到，就能去赚解决相应问题的收入。</p><p></p><p>第二，如果你想晋升，我们更希望的不是依据岗职位的能力告诉你应该怎么晋升，而是依据技能体系、技能树的概念告诉你，如果你想赚到更多的钱就要产出更大的价值，那么有怎样技能的员工能够输出这样的价值？这一点的灵感来自于游戏的提示，游戏里面都会有技能天赋树，小白点了不同的天赋点可能会变成战士、法师，他非常清晰地知道他的路线，甚至能提前规划他的路线。我当然希望我们的企业也能做成这样，所以我们就希望打造技能树。</p><p></p><p>比如说 Java 是技能树， 如果把 Java 分成 100 个知识点，那么把每个知识点做 level 1 到 level 10 的分类，然后每 level 出 30 道题，员工可能只需要答 10 道题，我就能知道你这个技能点是什么水平，比如说你已经到了 level 4。接下来你想得到提升，我对你的要求是 level 10 的话，给你半个月的时间，你要提升到 level 10。接下来菩提会推给你课程，这门课程你不用从零开始看，40% 开始看就可以了，因为菩提已经知道了你是 level 4 的水平。</p><p></p><p>你会发现在 AI 时代下这件事情做得很简单，以前需要很多专家研究很久，花费大量成本来出题定级，现在用具备世界知识的大模型可以迅速给出方案。所以这件事情变得成本很低，企业基于培训来做决策也更加清晰和简单了。以前可能也有老板拍脑袋做决策，比如他要 All in AI，因为他有前瞻性思维，但是对现状没有清晰的认知。如果你的技能树告诉他，我们的 AI 技能的人才储备度是红色的，不是绿色的，那他不会做这个决定。他一定会先把红色技能周围的技能点标出来，把这些人送过去培训 AI，学完之后把这个灯变绿色，再做 All in AI 的战略决策。所以这其实对管理者来讲是更好的抓手。</p><p></p><p>我们对菩提做了实验，发现在需求描述清晰的情况下，在推课的场景里， AI 推荐的准确率和人推荐的准确率对比差值小于 10%。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/419de99b3ef84e9e44b4155029e61c54.png" /></p><p></p><p>换句话来讲， AI 推的跟人推的没什么区别。但是这个差值随着课程量的增加而减少，这个同理应用到了我们魔镜的测试里。两千门课程，人和 AI 推荐的准确率差不多；两万门课程， AI 推得比人准。这就引来了另外的话题，我企业内的课程不够了怎么办？</p><p></p><p><img src="https://static001.geekbang.org/infoq/96/96236f9753d54562a2e454114cc859a6.png" /></p><p></p><p>我们现在在训练菩提，用三天的时间在全网检索了 17 万的课程，这些课程可能都来自于一些开源社区。然后我们现在让菩提自己再把这些课程进行缩减，把课程删减成三千门，然后再让人类专家做评分，找出哪些是东软需要的。</p><p></p><p>所以你会发现培训人在这个环节里有很重要的点，有了菩提，你的培训经理可能会比一线业务的项目经理更先知道你的团队遇到了怎样的技术障碍。比如说在单位时间内，你团队的成员都在检索 Python，都在检索区块链技术，那就证明你的客户遇到了这样问题。所以谁抓住了数据的起爆点，对它做出分析，谁就能得到更大的话语权。</p><p></p><h4>培训增强</h4><p></p><p></p><p>最后谈一下劳动资料。我们其实很早就投入了交互式游戏化技术。你会发现数据无外乎就是三步骤，数据挖掘、标记和分析，现在 AI 出现就多了训练这个步骤。</p><p></p><p><img src="https://static001.geekbang.org/infoq/dd/dd743566ed209823e52c063dee5fe205.png" /></p><p></p><p>这个流程里面做的最好的是游戏，游戏是最了解这些玩家的，如果王者荣耀想的话，它可以知道你的投资风险偏好，可以知道你喜欢怎样的女朋友，因为你在这游戏里有大量的行为数据被记录下来了。因此我们做了很多的游戏化创新，研发了自己的电竞式知识训练平台。我们不止在自己家做了实验，也在很多 500 强做了实验。我们得到的数据，在新员工培训里面，一家 500 强的公司员工在电竞平台上的主动知识答题率得到了非常强的提升。</p><p></p><p>有了这么大的交互量，我可以精准知道你的技能点是怎样的，然后我们还对这些人进行了行为上的一些分析，了解他们的行为偏好和适合的发展方向。比如说有的程序员在游戏里有大量社交行为，那么他将来就适合去售前咨询方向。很多喜欢社交的员工来自某些大学，那么如果你的业务需要这样的员工，将来就可以直接到这些大学撒网招人。而这一切数据都来自于为期 5 天的新员工培训。你会发现培训的影响力得到了非常大的增强，往后可以影响 HRBP 的建议，往前可以影响招聘决策的建议。</p><p></p><h2>小结</h2><p></p><p>以上就是我的一些分享。有一句话我很喜欢，就是“改变是缓慢的，但会在一瞬间完成”。我也期待今年我们的数字员工增加到 15-30 个后，能够直接降低 100 万的成本。这其实是蛮大的挑战，希望下次有机会能够为大家做更多分享，谢谢。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WGuPKOlrh2ujmRZMOw4H</id>
            <title>美国撤销英特尔、高通向华为供应芯片许可</title>
            <link>https://www.infoq.cn/article/WGuPKOlrh2ujmRZMOw4H</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WGuPKOlrh2ujmRZMOw4H</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 May 2024 06:24:12 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 美国, 华为, 半导体芯片, 出口限制
<br>
<br>
总结: 美国对华为实施更严格的出口限制，吊销华为从高通和英特尔购买半导体芯片的许可证，影响华为手机和笔记本电脑在美国的销售。 </div>
                        <hr>
                    
                    <p>周二(5月7日)，美国彭博社、英国路透社、英国金融时报等多家媒体报道称美国已吊销华为技术有限公司从高通和英特尔公司购买半导体芯片的许可证，进一步收紧了针对华为的出口限制。</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b8d5f2ad93c3f8c71da57acca6d700a0.jpeg" /></p><p></p><p>(截图来源：英国金融时报)</p><p></p><p>彭博社援引知情人士称，美国撤销了华为(Huawei Technologies Co.)从高通(Qualcomm Inc.)和英特尔(Intel Corp.)购买半导体的许可证，进一步收紧对华为的出口限制。</p><p></p><p>这些不愿透露姓名的人士表示，吊销许可证会影响华为手机和笔记本电脑所用芯片在美国的销售。同时，撤销许可证也会影响美国芯片的销售。美国商务部证实撤销了向华为出口的“某些许可证”，但没有透露哪些美企受到影响。</p><p>&nbsp;</p><p>据路透社，华为上个月推出的首款支持人工智能的笔记本电脑MateBook X Pro搭载了英特尔全新Core Ultra 9处理器，这引发了美国共和党议员批评商务部为华为出口“开绿灯”。</p><p>&nbsp;</p><p>路透社3月曾报道，英特尔暂时保住了向华为供应芯片的许可，得以继续向华为销售价值数亿美元的芯片。当时美国共和党参议员卢比奥（Marco Rubio）要求“立即”吊销英特尔所获许可。</p><p>&nbsp;</p><p>英国《金融时报》星期二报道了高通出口许可证被撤销的消息后，高通股价应声下跌 0.9%，英特尔的股价则几乎没有变化。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KKmacI3Ccj4HNYi7kXUr</id>
            <title>零一万物李谋：当大模型推理遇到算力瓶颈，如何进行工程优化？</title>
            <link>https://www.infoq.cn/article/KKmacI3Ccj4HNYi7kXUr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KKmacI3Ccj4HNYi7kXUr</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 May 2024 02:17:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: OpenAI, ChatGPT, 大语言模型, 算力需求
<br>
<br>
总结: 介绍了大语言模型在推理过程中所面临的算力需求挑战，以及针对这一挑战的优化技术手段和未来发展方向。 </div>
                        <hr>
                    
                    <p></p><blockquote>自 OpenAI 发布 ChatGPT 起，大语言模型的惊艳效果吸引了越来越多的人和资本关注到该领域，近年模型本身的参数量和序列长度也呈指数级增长，要面对的算力瓶颈问题接踵而至。在 AICon 全球人工智能开发与应用大会 暨 大模型应用生态展·2024 上，InfoQ 邀请到了零一万物资深算法专家李谋发布演讲分享，他将结合大模型的的算力需求和模型结构，详细介绍零一万物在构建 Yi 模型在线推理服务过程中所运用的优化技术手段。为了让听众了解更多的内容，我们提前采访了李老师，以下为内容纪要：</blockquote><p></p><p></p><p>InfoQ：您在演讲中提到了大模型的算力需求及其增长趋势，可以详细介绍一下目前大模型在推理过程中所面临的主要算力挑战是什么？针对这种快速增长的算力需求，您认为目前的技术和资源是否足以应对？</p><p></p><p>李谋： 大模型的计算主要分为训练和推理两个步骤，他们对于算力的侧重点不太一样。模型训练侧重整体吞吐 (throughput)，需要大规模，高扩展性，低能耗的分布式计算集群，而推理侧重延迟 (latency)，在算力方面需要强大的计算芯片，高速的内存访问技术。这种算力的需求在深度学习和大模型流行之后的近年来呈指数级增长，对于硬件厂商和电力供应厂商是巨大的挑战，目前也有不少芯片制造商针对大模型场景设计了专用芯片和硬件架构，相信短期的未来能够完美迎接这波挑战。</p><p></p><p>InfoQ：您觉得传统模型和大语言模型在结构上的不同之处是什么，推理优化手段是否有差异？</p><p></p><p>李谋： 传统模型，包括 CNN, NLP, ASR 等网络的特点是结构复杂，算子类型多，模型的变种也很多，不同的软件框架有自己的模型描述语言和模型结构。而大语言模型绝大多数基于 Transformer 网络结构，通过多个 Transformer Block 串联得到，其特点是网络结构简单，但参数量巨大，针对这些差异这两套模型在工程上也有不同的优化手段。</p><p></p><p>InfoQ：了解到分布式并行加速是一个在大模型推理中措施，零一万物在这方面是如何做的？</p><p></p><p>李谋： 简单来讲对于分布式并行的推理优化手段主要是张量并行 (tensor parallelism) 和上下文并行 (context parallelism)，分别从模型维度和输入序列维度对参数做切分，使用多个设备并行计算达到加速的目的。</p><p></p><p>InfoQ：在推理过程中，大模型的内存消耗通常是一个重要的考量因素。您对于内存管理方面有哪些优化策略或经验分享吗？</p><p></p><p>李谋： 大模型的内存消耗主要来源于模型权重本身的加载和 Transformer Block 中的 Key/Value 这 2 个矩阵，首先模型的低精度量化是一个常见降低内存使用量的优化手段，使用更低精度的数据类型往往也能得到正确性的推理结果。其次模型中 Key/Value 矩阵的分页内存管理 (PagedAttention) 也可以大幅度提升内存利用率，甚至在任务空闲的时候我们可以将 Key/Value 矩阵临时切换放置到其它内存区域，在需要的时候再切换回来，以时间换空间。</p><p></p><p>InfoQ：在面对算力瓶颈时，有时候需要进行折衷权衡，比如牺牲一定的模型精度以换取更快的推理速度。您是如何权衡和决策的？是否有一些通用的指导原则？</p><p></p><p>李谋： 从感知上来讲模型的参数量越大，其中的信息冗余程度也就越高，低精度量化在传统的小模型推理中已经是一个常见的优化手段了，对于更大参数量的语言模型更是如此。零一万物的低精度量化覆盖了训练和推理整个流程，所以对于推理来讲是无损量化，不需要校验这个过程。从生产环境的角度来讲，如果模型量化能够在保持主流任务评测精度几乎不降 (或降低零点几个点) 的情况下服务性价比提升 1 倍以上，我觉得是可以完全可以接受的。</p><p></p><p>InfoQ：大模型在推理过程中可能会面临的另一个挑战是延迟问题，特别是对于实时或者交互式应用来说，延迟是一个非常关键的指标。您是如何处理推理延迟的优化问题的？</p><p></p><p>李谋： 优化延迟比优化吞吐要棘手一些，首先最好的情况是有条件购买算力更强大的硬件，或者从硬件设计的角度上去降低延迟。软件层面上，比如对于 NVIDIA GPU 可以开发更高效的 CUDA Kernel，使用多卡并行等手段，当然这中优化往往有较大的人力和时间成本。</p><p></p><p>InfoQ：除了硬件加速器和分布式并行加速外，是否还有其他类型的加速技术或者优化手段可以用于缓解大模型推理的算力压力？</p><p></p><p>李谋：这个方面内容，技术点有点多，在 5 月 17 日的 AIConAICon 全球人工智能开发与应用大会 暨 大模型应用生态展·2024 上，我们会展开分享，欢迎关注。</p><p></p><p>InfoQ：针对不同规模和复杂度的任务，您是否采用了不同的推理优化策略？是否可以分享一些根据任务需求调整策略的经验？</p><p></p><p>李谋：不同复杂度的任务使用了不同数量，不同配比的硬件。举个例子，对于同一个模型 Yi-34B，我们部署了 2 套硬件集群 (低配版 / 高配版，算力和成本不同)，针对用户在线请求的具体输入长度来决定使用哪个集群服务，这样能兼顾用户体验，服务压力和服务成本。</p><p></p><p>InfoQ：针对目前大模型推理算力瓶颈的问题，您认为未来可能出现的技术突破或发展方向是什么？</p><p></p><p>李谋： 首先是针对大模型的场景的专用芯片，目前国内已经有一些相关的产品，但问题是这些专用芯片和软件配套体系在市场上没有形成良好生态，没有用户的使用和共识对于生态发展是个挑战。其次随着大模型和 AI 对算力需求的增长，伴随计算集群规模的增长，局部地区的电力供应可能会是一个问题，这可能会推动一些清洁能源和高效发电技术 (如风力发电，可控核聚变) 的发展。</p><p></p><p>嘉宾介绍：</p><p></p><p>李谋零一万物资深算法专家，毕业于哈尔滨工业大学，零一万物大模型在线推理服务负责人，历任阿里达摩院和华为云 EI 服务产品部技术专家。长期从事 AI 模型推理和训练全链路研发与优化工作，曾带领团队自研通用推理引擎与底层加速库，取得 Standford DAWNBench GPU 排行榜 TOP1 的推理性能排名。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/b4/b40577f364287d8b8d51a38373872039.png" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/KPrrukXhTwQyTC1wi9O5</id>
            <title>国产版Sora到来！视频大模型更上一层楼 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/KPrrukXhTwQyTC1wi9O5</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/KPrrukXhTwQyTC1wi9O5</guid>
            <pubDate></pubDate>
            <updated>Wed, 08 May 2024 02:05:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 技术创新, 开源, 多模态
<br>
<br>
总结: 大模型领域的快速发展使得了解最新技术动态和积极学习成为从业者的必修课。InfoQ研究中心每周更新大模型行业动态，为读者提供全面的行业回顾和要点分析。本周重点发现包括技术创新、模型优化、跨领域应用和科研探索。大模型持续更新，开源领域和多模态领域也展现出活跃态势。科研领域的PEFT技术和KAN神经网络架构提供了新方向。应用探索方面，新产品新应用/功能不断涌现。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，行业在技术创新、模型优化、跨领域应用和科研探索方面展现出活跃态势。Vidu视频大模型，多token预测模型以及gpt2-chatbot，均展现突破性性能。元象公司开源XVERSE-V大模型，通义千问发布千亿参数的Qwen1.5-110B，推动了AI技术的共享与进步。PEFT技术和KAN神经网络架构的提出，为大模型的训练和架构设计提供了新方向。Anthropic发布Claude&nbsp;iOS应用，GitHub推出Copilot&nbsp;Workspace，增强了AI在移动平台和开发环境中的实用性。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>4月27日，生数科技与清华大学联合发布了中国首个具有长时长、高一致性和高动态性特点的视频大模型——Vidu。Vidu不仅能够模拟现实世界的物理现象，还支持多镜头生成，并且具有高度的时空一致性。Vidu采用了该团队独创的Diffusion与Transformer融合架构——U-ViT，能够实现一键生成长达16秒、分辨率达到1080P的高清视频内容。4月30日，在LMSYS&nbsp;Chatbot&nbsp;Arena中，一个名为gpt2-chatbot的神秘模型引起了社交媒体上的热烈讨论。众多网友对gpt2-chatbot进行了围观和测试，发现其能力与GPT-4不相上下，甚至在某些任务上的表现超过了GPT-4&nbsp;Turbo。4月30日，Meta&nbsp;AI法国团队推出多token预测模型，在编程类任务上表现突出。与单一token预测相比，13B参数模型在HumanEval基准测试中解决了12%的额外问题，在MBPP基准上则多解决了17%的问题。此外，在小型算法推理任务中，多token预测也在分布外泛化方面取得了显著的成效。</p><p></p><h4>开源领域</h4><p></p><p>4月28日，元象公司发布了多模态大模型XVERSE-V，并将其开源，允许无条件免费商用。这一模型支持任意宽高比的图像输入。4月29日，通义千问首次发布了一个千亿级参数模型——Qwen1.5-110B，该模型在多项基准测试中刷新了当前开源模型的最佳成绩。该模型继承了Qwen1.5系列的Transformer解码器架构，并引入了分组查询注意力方法，这使得模型在推理过程中更为高效。Qwen1.5-110B模型支持32K上下文，表现出卓越的多语言处理能力，支持中文、英文、法文、德文、西班牙文、俄文、日文、韩文、越南文、阿拉伯文等多种语言。</p><p></p><h4>多模态领域</h4><p></p><p>4月27日，智子引擎发布了其最新的多模态大模型——Awaker&nbsp;1.0，Awaker&nbsp;1.0采用了创新的MOE架构，并具备自主更新能力。在视觉生成方面，Awaker&nbsp;1.0采用了智子引擎自研的视频生成底座——VDT，该技术在写真视频生成方面的表现超越了Sora，打破了大模型在实际应用中难以落地的难题。</p><p></p><h4>科研领域</h4><p></p><p>4&nbsp;月&nbsp;29&nbsp;日，来自美国东北大学、加州大学Riverside分校、亚利桑那州立大学和纽约大学的研究员共同深入探讨了参数高效微调（PEFT）技术在大模型中的应用及发展方向。PEFT技术为预训练模型提供了一种高效的适配方法，专门用于下游任务。该技术通过保持大部分预训练参数不变，仅对极少数参数进行微调，使得大模型能够更加灵活和高效地适应各种不同的下游任务。4月30日，麻省理工学院、加州理工学院、东北大学等团队联合进行研究出一种全新的神经网络架构KAN，该架构与传统的MLP架构截然不同，并且能够在数学和物理问题上以更少的参数达到更高的精确度。在函数拟合、偏微分方程求解，甚至处理凝聚态物理方面，KAN的表现都超越了MLP。此外，KAN在处理大模型时自然避免了灾难性遗忘的问题，并且很容易将人类的习惯性偏差或特定领域的知识融入其中。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>4月29日，GitHub推出了Copilot&nbsp;Workspace的技术预览版。这是一个由GitHub设计的开发环境，它基于多种Copilot代理，允许开发者使用自然语言与代理进行互动。在Copilot&nbsp;Workspace中，开发者可以进行头脑风暴、规划、构建、测试以及执行程序代码，并且能够查看从初步想法到最终代码实现的整个开发过程，这使得开发者能够更快、更轻松地发挥其创造力。5月1日，人工智能创业公司&nbsp;Anthropic首次推出旗下大模型产品&nbsp;Claude&nbsp;的移动端App&nbsp;iOS版本，只有11MB。功能包括：允许用户与网络聊天实现无缝同步，聊天可以跨设备持续进行；用户可随时随地使用照片、拍摄新照片、上传文件，同时支持实时图像分析和态势感知功能；对于所有订阅计划用户，包括Pro和Team，都可免费下载并开放使用这款应用程序。</p><p></p><h3>其他</h3><p></p><p>4月27日，八位具身智能领域的企业创始人、技术负责人和专家在中关村论坛年会未来「人工智能先锋论坛」上展开了一场关于具身智能的深度对话，讨论了关于具身智能发展路径，当前的技术水平，发展瓶颈和难题等内容；5月4日，斯坦福大学教授李飞飞正在筹划成立一家AI公司，并已成功完成了种子轮融资。该公司将专注于开发“空间智能”技术，目标是使人工智能能够像人类一样对视觉信息进行复杂推理和处理。此轮融资的投资方包括硅谷风投a16z和Radical&nbsp;Ventures。</p><p></p><p>报告推荐</p><p>Sora来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？答案尽在InfoQ研究中心近期发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，关注「AI前线」公众号，回复「季度报告」免费下载，一睹为快吧~</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df2037200d792e5be89596273fdcf950.png" /></p><p></p><p></p><p>报告预告</p><p>AGI究竟是什么？AI&nbsp;Agent&nbsp;如何助力人工智能走向AGI时代？在营销、金融、教育、零售、企服又有哪些典型应用和案例？欢迎大家持续关注InfoQ研究中心即将发布的《中国AGI市场发展研究报告&nbsp;2024》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c0207976c6592ac74b5109332dc9e1c.jpeg" /></p><p></p><p></p><h4>活动推荐</h4><p></p><p>AICon&nbsp;全球人工智能与大模型开发与应用大会暨通用人工智能开发与应用生态展将于5月17日正式开幕，本次大会主题为「智能未来，探索AI无限可能」。如您感兴趣，可点击「阅读原文」查看更多详情。</p><p></p><p><img src="https://static001.geekbang.org/infoq/d1/d1a0f9425899db37a9189885eeca9625.jpeg" /></p><p></p><p>会议即将开幕，购票或咨询其他问题请联系票务同学：13269078023，或扫描上方二维码添加大会福利官，可领取福利资料包。</p><p>阅读原文链接：<a href="https://sourl.co/iRVv2U">https://sourl.co/iRVv2U</a>"</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/yibHMlH6p23G31PlMA9R</id>
            <title>微软秘密开发首个千亿大模型，竟由OpenAI对手操刀！网友：你不要奥特曼了？</title>
            <link>https://www.infoq.cn/article/yibHMlH6p23G31PlMA9R</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/yibHMlH6p23G31PlMA9R</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 May 2024 06:45:21 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 微软, MAI-1, 5000 亿参数, Inflection
<br>
<br>
总结: 微软发布了自研的大型人工智能模型 MAI-1，拥有5000亿参数，可能在本月亮相，基于Inflection技术开发。 </div>
                        <hr>
                    
                    <p>整理 | 华卫</p><p></p><p>Phi-3 Mini 模型发布不到两周，微软又传出了自研千亿参数级别模型的消息。</p><p></p><p>自向 OpenAI 投资超 100 亿美元以换取重用其人工智能模型的权利以来，微软首次开始在内部自研全新且足够大的人工智能模型，或能与来自谷歌、Anthropic 和 OpenAI 的最先进模型相竞争。</p><p></p><p>这个新模型在内部被称为 MAI-1，由前谷歌人工智能领导者、 AI 初创公司 Inflection 的 CEO 穆斯塔法·苏莱曼 (Mustafa Suleyman) 负责监督。知情人士称，MAI-1 的参数规模将远远大于 Phi-3 等任何微软之前训练过的较小的开源模型。但这意味着，它将需要更多的计算能力和训练数据，因此成本会更高。</p><p></p><p>同时，微软此举表明，其现在正在人工智能领域追求“双重轨道”，目标是开发既可以廉价地构建到应用程序中又可以在移动设备上运行的“小语言模型”，以及更大、最先进的人工智能模型。苹果目前似乎也正在探索类似的路径，此前同样也发布了八款针对设备使用的小型 AI 语言模型。</p><p></p><p></p><h3>5000 亿参数级别</h3><p></p><p></p><h3>最早或于本月亮相</h3><p></p><p></p><p>据介绍，MAI-1 将有大约 5000 亿个参数或设置，可以调整这些参数或设置以确定模型在训练期间学习的内容。相比之下，OpenAI 的 GPT-4 有超过 1 万亿个参数，而 Meta 和 Mistral 等公司发布的小型开源模型有 700 亿个参数。</p><p></p><p>这表明，MAI-1 可以定位为 GPT-3 和 GPT-4 之间级别的一种模型，该模型将能够提供远远高于 Llama 和 Mistral 等开源模型、但或低于 OpenAI 旗舰版 LLM 的响应精度。</p><p></p><p>为了训练该模型，微软一直在分配大量配备 Nvidia GPU 的服务器，并编译各种来源的训练数据，包括 OpenAI 的 GPT-4 生成的文本和公共互联网数据，还可能会使用来自 Inflection 的训练数据和某些其他资产来支持 MAI-1。</p><p></p><p>目前，MAI-1 的确切用途尚未确定（即使在微软内部），其最理想的用途将取决于其性能。如果该模型确实具有 5000 亿个参数，那么在消费类设备上运行就太复杂了。这意味着，微软很可能会在其数据中心部署 MAI-1，在这里大语言模型可以集成到 Bing 和 Azure 等服务中。</p><p></p><p>微软可能最早在本月晚些时候的 Build 开发者大会上，根据未来几周取得的进展亮相 MAI-1。</p><p></p><p></p><h3>MAI-1 的研发是基于 Inflection？</h3><p></p><p></p><p>“尽管 MAI-1 是一种全新的、与 Inflection 之前发布的 Pi 分开的大型语言模型 ，但其可能建立在前 Inflection 员工带来的技术之上。”据两名了解情况的微软员工称。</p><p></p><p>从 OpenAI 官网的一份声明来看，Inflection 曾是 OpenAI 的竞争对手，但它现在把业务重点从聊天机器人 Pi 转向了向企业销售人工智能软件。曾担任过各种技术职务的肖恩·怀特 (Sean White) 已加入该公司，担任新任 CEO。</p><p></p><p>今年 3 月，微软以 6.5 亿美元收购了这家初创公司的大部分员工和知识产权，并聘请苏莱曼来领导一个新的消费者人工智能部门。该部门将面向消费者的产品（包括微软的 Copilot、Bing、Edge 和 GenAI）归入一个名为 Microsoft AI 的团队，而苏莱曼直接向 Microsoft 首席执行官萨蒂亚·纳德拉（Satya Nadella）汇报工作。</p><p></p><p>新部门标志着 Microsoft 的重大组织转变，其网络服务总裁 Mikhail Parakhin 将与他的整个团队一起向苏莱曼汇报工作。这也是微软利用生成人工智能热潮的最新举措之一。</p><p></p><p>纳德拉在一份声明中说：“我认识穆斯塔法已经好几年了，我非常钦佩他作为 DeepMind 和 Inflection 的创始人，以及一个有远见的产品制造商和追求大胆使命的开拓团队建设者。”</p><p></p><p>DeepMind 于 2010 年在英国成立，2014 年被谷歌以 5 亿美元收购，苏莱曼是该公司的三位创始人之一。在 DeepMind 就职时，苏莱曼曾因员工抱怨其咄咄逼人又过于激进的管理风格引发争议，于 2019 年被迫离开 DeepMind。之后谈到当时的员工投诉时，苏莱曼回应说：“我真的搞砸了。我要求很高，而且相当无情。我设定了一些相当不合理的期望，导致一些人的工作环境非常恶劣。我对此感到非常遗憾。”</p><p></p><p>几个月后，他转到谷歌总部，负责领导人工智能产品管理和政策。2022 年，他从谷歌离职，加入了硅谷风险投资公司 Greylock，并于当年晚些时候推出了 Inflection。</p><p></p><p>据悉，微软还将聘用 Inflection 的大部分员工，Inflection 的联合创始人兼首席科学家 Karén Simonyan 也将担任其 AI 团队的首席科学家。虽然微软没有明确调动的员工人数，但表示其中包括人工智能工程师、研究人员和大型语言模型构建者，他们设计并共同完成了“过去五年中为推动人工智能发展做出的许多最重要贡献”。</p><p></p><p>Inflection 的第三位联合创始人、LinkedIn 创始人兼执行主席 Reid Hoffman 将继续留在 Inflection 的董事会。</p><p></p><p>去年 6 月，Inflection 还完成 13 亿美元的一轮融资，由微软、英伟达和三位亿万富翁（Reid Hoffman、Bill Gates 和 Eric Schmidt）牵头投资。当时，微软首席技术官 Kevin Scott 还表示，“像 Inflection 这样雄心勃勃的人工智能公司，正在凭借易于使用并展示人工智能多种可能性的变革性产品引领行业发展。”</p><p></p><p></p><h3>结束语</h3><p></p><p></p><p>微软开发 MAI-1 大模型，也凸显了其试图独立于 OpenAI 等人工智能厂商去探索 AI 开发的意愿。</p><p>此前，微软一直致力于在其 Windows、Office 软件和网络安全工具等产品中推出人工智能助手，但多采取与外部企业合作的方式。</p><p></p><p>去年，微软向 ChatGPT 的制造商 OpenAI 投资了 130 亿美元，并将其技术快速集成到产品和数字体验中。目前，OpenAI 的技术为微软的许多生成式 AI 功能提供支持，包括 Azure、Copilot 和内置 Windows 的聊天机器人。</p><p></p><p>微软还投资了其他人工智能初创公司，包括向法国人工智能初创公司 Mistral AI 投资 20 亿欧元（约合 21 亿美元），将 Mistral AI 的大型语言模型（LLM）在 Azure 云计算平台上进行托管。</p><p>今后，这一切可能都会有所变化，微软或将开始推动自研大模型在各产品中的应用。据悉，苏莱曼领导的部门将接手把 AI 版本的 Copilot 集成到 Windows 操作系统中，以及开展增强生成式 AI 在其 Bing 搜索引擎中的使用等项目。</p><p></p><p>“微软正参与一场 AI 竞赛”，微软的一位 AI 工程师在前不久说，但在道德和保障方面，微软为速度走了捷径，匆忙推出产品而没有充分考虑接下来会发生什么。所有大型科技公司都可以访问大部分相同的数据，AI 领域没有真正的护城河。</p><p></p><p>参考链接：</p><p>https://tech.slashdot.org/story/24/05/06/1437231/microsoft-readies-new-ai-model-to-compete-with-google-openai</p><p>https://arstechnica.com/information-technology/2024/05/microsoft-developing-mai-1-language-model-that-may-compete-with-openai-report/</p><p>https://www.ft.com/content/5feedf3a-ff7a-4c89-9b1d-f9b48834ff4c</p><p>https://siliconangle.com/2024/05/06/microsoft-reportedly-developing-mai-1-llm-500b-parameters/</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/VUyEiu8A2CKRpoW6YGxI</id>
            <title>零一万物官宣中国职场“AI特助”万知：手机2分钟生成PPT，还让“100个领域专家”给自己“打工”</title>
            <link>https://www.infoq.cn/article/VUyEiu8A2CKRpoW6YGxI</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/VUyEiu8A2CKRpoW6YGxI</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 May 2024 06:18:22 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 万知, AI, 零一万物, 李开复
<br>
<br>
总结: 5月7日，零一万物宣布推出了一款为中国人量身定做的AI工作平台——万知，由李开复博士出任首席体验官。万知支持中英双语，可以做会议纪要、写周报，解读财报、论文等文件，还能帮助制作PPT。针对学习、工作中的三大需求，提供了联网版百科全书、双语量子速读和古希腊掌管PPT的功能。旨在解决用户工作场景中的真实问题，让每个人都能轻松使用AI个人特助万知。 </div>
                        <hr>
                    
                    <p>5 月 7 日，零一万物官宣了第一款为中国人量身定做的一站式 AI 工作平台 ——万知。作为“AI 个人特助”，万知可以做会议纪要、写周报，还可以解读财报、论文等各类文件，甚至帮你做 PPT。值得一提的是，万知支持中英双语，而且完全免费。</p><p></p><p>网页试用：www.wanzhi.com</p><p>微信小程序：万知 AI</p><p></p><p><img src="https://static001.geekbang.org/infoq/ad/adb7f124d44b569b34909a11ffcf362d.png" /></p><p></p><p>零一万物也重磅宣布，李开复博士亲自出任万知 C“E”O（Chief Experience Officer），也就是“首席体验官”。他将在这段时间里亲自推出一系列 AI 助力工作生活新范式的万知使用教程，同时，集结用户们在万知上提交的反馈，分享《万知首席体验官周报》，邀请广大网友一起体验 AI-first。</p><p></p><p></p><h2>“三大秘技”拿下工作</h2><p></p><p></p><p>学生和大部分上班族，在学习、工作中总是需要使用电脑、手机完成工作任务，其中不外乎 “找、读、写” 三大刚需。搜索和核实信息、阅读大量的文档资料进行数据分析，进而总结撰写所需要的内容，几乎是人人都熟悉、经常性重复的基础工作流程。迈入人工智能时代，万知针对这三个共性需求，提供了三大 AI 秘技：</p><p></p><p></p><h4>秘技一：联网版百科全书</h4><p></p><p></p><p>基于零一万物国际领先的 Yi 大模型海量知识库，万知在“兵家必争之地”的 AI 通用问答场景中，展现出了不俗的产品性能，面对复杂问题也能够快速总结给出“聪明”的高分答案，比起传统信息搜索，AI 更能快速直击精准又深度的解答。</p><p></p><p><img src="https://static001.geekbang.org/infoq/73/73e5c1e8796431999a6bd7c68d1f4886.gif" /></p><p></p><p>万知善用多种形式输出更有质量的内容，如：表格、简易思维导图，便于用户理解。得益于此，在更偏生活化的场景中，万知可以更好地扮演生活小助手等角色。同时，万知支持实时访问和整合互联网信息，为用户提供最新的数据和见解，这弥补了大模型知识库的滞后性、避免幻觉产生。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e9/e920320a80b5d643dc943ddb0f1adf95.gif" /></p><p></p><p></p><h4>秘技二：双语量子速读</h4><p></p><p></p><p>输出高质量问答离不开基座模型无损上下文的能力。在这方面，零一万物已经有了相当成熟的技术积累：零一万物在 200K 超长上下文窗口的 Yi 模型版本上进行迭代优化，可以轻松处理 40 万字资料。今年 3 月，零一万物进一步推出了基于全导航图的新型向量数据库笛卡尔（Descartes），为 RAG（检索增强生成）提供了高效极速的检索机制，0.1 秒判别用户意图，极速调度检索，给予用户高效的优质反馈。</p><p></p><p>Yi 大模型本身的超长上下文窗口搭配领先的 RAG 方案，共同构建起了万知“5000 页文档速读”的超长文档阅读能力。60 万字的英文小说《马斯克传》一度让不少 AI 助手 “宕机”，万知则能轻松解读。</p><p></p><p>在实测过程中，一篇 169 页的全英文财报，万知小程序可在 3 秒内对内容重点进行提炼，对于财报中的财务数据进行解读，并且按照财报叙述逻辑总结重点。在对话末尾，万知还为用户给出提示性质的追问，帮助用户加深对财报内容的理解。</p><p></p><p><img src="https://static001.geekbang.org/infoq/93/931f2d8514cfb18d7e6d51f73d3d7306.gif" /></p><p></p><p>与小程序相比，万知的 PC 网页版的功能更加全面。如在文档解读场景中，万知 PC 网页版在输入框内提供了“截图提问”按钮，用户可在左侧截取文档内容，针对性进行提问，提高回答效率。</p><p></p><p><img src="https://static001.geekbang.org/infoq/e1/e105c1f2d62b96155524d6e5bbe4084e.png" /></p><p></p><p></p><h4>秘技三：古希腊掌管 PPT 的“神”</h4><p></p><p></p><p>曾几何时，Boss 一阵夺命连环 call，打工人就要立马交付各种 PPT。现在，用户只需要输入相关 PPT 主题，万知就能生成文字大纲，经用户确认后就能一键生成 PPT。得益于 RAG 实时检索，万知 AI 能够基于“大脑知识 + 新鲜实事”生成 PPT 大纲，辅之以多种版式和 AI 配图，一份精美的 PPT 两分钟就能出炉。如果内容有瑕疵，你还可以在手机端复制链接，转到 PC 界面（同一个账户 PC 和手机端均共享历史文件）接力修改 PPT，用 AI 增强换图或本地上传功能优化到满意为止。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a0d79aaaee31f4b685e8f20a3cc5eb0b.gif" /></p><p></p><p>早上醒来看到老板临时来的 PPT 任务再也不用怕，即使在早高峰人挤人的地铁里，你也能在手机上利用碎片化的时间进行办公。</p><p></p><p><img src="https://static001.geekbang.org/infoq/de/def922bb8a37a1dc26a2aeaad97a70dc.png" /></p><p></p><p>目前，万知网页版内置了机构宣传、职场汇报、地产城市宣传、课程教案、项目汇报等多种模版。用户可以自定义页数、受众对象、演示场景的属性等条件来定制 PPT，输出中文、英文双语可选。同时，还可以上传 Word 材料让万知帮你转化成 PPT、或上传 PPT 参考模板进行个性化内容填充。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f2/f206c3d433b17320461fe1b7dd6e320d.png" /></p><p></p><p></p><h2>瞄准国内职场</h2><p></p><p></p><p>去年，零一万物在海外进行了产品试水。结合海外实践经验，零一万物将万知定位为“一站式 AI 工作平台”，将职业白领、大学生等高知群体确定为核心用户层，力求利用大模型能力解决用户工作场景中的真实问题，让每个人都能轻松用上万知这个 AI 个人特助。</p><p></p><p>根据万知团队贴近职场用户的观察，使用 AI-First 万知之后的个人工作效率平均有五成以上的提升，其中最大程度的效率提升来自：知识检索和整理、文档构思撰写。比如，针对某一些低专业判断的日常白领任务，比如众多发票分类总结、众多简历资格要点筛选，节约时间高达八成以上；针对文件撰写，以往需要 10 分钟的内容现在可以 1 分钟内搞定，一步步助力网友实践个人工作效率 x10 的目标。</p><p></p><p>基于大模型能力的 AI-first 应用将重构职场生态正在成为越来越多行业的共识。在产品设计的过程中，万知也贴合了中国职场的特色。</p><p></p><p>与海外办公场景多集中在 PC 上不同，中国职场内移动办公的需求更为普遍。在零一万物生产力产品负责人曹大鹏看来，万知小程序与 PC 网页端有着很强的协同效应，小程序对应碎片化的移动办公，PC 网页端则对应专业深度办公场景；小程序是用户输入需求的快捷入口，PC 网页端则可供用户做深入的细化调整。</p><p></p><p></p><h2>结束语</h2><p></p><p></p><p>经过 2023 年的混战，国内大模型产品都在争夺“中国版 ChatGPT ”的心智定位。回溯 2023 年初，ChatGPT 仅用 2 个月的时间便获得过亿用户，创下了互联网最快破亿应用的记录。从用户数据角度来看，中国目前还未迎来自己的“ChatGPT Moment”。</p><p></p><p>零一万物 CEO 李开复博士表示，大模型推理成本的下降会推动着中国 AI 大模型进入落地为王的阶段，今年会迎来“大模型应用爆发元年”。基于强大的 Yi 大模型的模型知识能力，搭配上以 AI-first 理念设计的工具软件界面，零一万物正试图打造适合国人的“ChatGPT Moment”。</p><p></p><p>“从目前国内的竞争环境来看，我认为未来不会是一家独大的局面。” 零一万物产品负责人曹大鹏表示，“零一万物作为大模型驱动的创业公司，我们会围绕生产力场景做产品创新。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/RAXU0v0i4wp8h3ox2EAL</id>
            <title>AI 与大模型如何影响企业基础平台和数据体系建设?</title>
            <link>https://www.infoq.cn/article/RAXU0v0i4wp8h3ox2EAL</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/RAXU0v0i4wp8h3ox2EAL</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 May 2024 03:40:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI 时代, 数据智能化, 大模型, 数据架构
<br>
<br>
总结: 在AI时代，数据智能化需要在多个层面进行优化调整，大模型对企业的底层基础设施提出挑战，数据架构需要适应新的应用场景。 </div>
                        <hr>
                    
                    <p>AI 时代的数据智能化需要在基础设施、算力、架构、数据训练等多个层面进行优化调整。在前段时间，InfoQ 邀请了 <a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">ArchSummit 架构师峰会</a>"上的专家小质科技 技术 VP 胡月军、天翼云资深研发专家 刘超，和 高级架构师 / 大数据基础架构负责人 王海华老师来直播，一起聊了聊他们团队在 AI 环境下更智能的处理数据和利用数据的。以下是直播整理。</p><p></p><h3>一、AI 大模型给各领域带来的影响和挑战</h3><p></p><p></p><p>InfoQ：随着 AIGC 的爆火，大模型的训练对于底层的算力基础设施提出了更高的要求，刘超老师在云计算领域拥有 10 多年的工作经验，可以谈谈您在这方面的观察吗？大模型究竟对企业的底层基础设施带来了哪些挑战？</p><p></p><p>刘超：我曾长期从事类似底层云计算相关的工作。一开始，我们都知道计算的核心是围绕着计算、网络和存储展开的。然而，在大模型时代，我们注意到了一些变化。现在，我们关注的焦点似乎转移到了算法、算力和数据方面。今天另外两位老师在数据方面比较专业，我是更加侧重基础设施层面，我认为在大模型时代，我所感受到的更多压力来自于算力方面。云计算的算力与智算的算力存在着一定的差异。以前，我们主要使用通用的 CPU 算力，它主要用于普通计算任务，比如虚拟机，容器以及运行在上面的电商业务。然而，在智能计算时代，我们需要更多的并行计算、向量计算，矩阵计算，这就需要一种算力，也就是我们常说的 GPU 算力。</p><p></p><p>除了 GPU 之外，还有其他一些不太显而易见的算力需求。比如大模型除了计算量大之外，大型模型在整个集群中的相互关联计算可能会导致一些协同性问题，例如海量的数据交互，这就需要更高吞吐量和更低时延的网络，再如大数据量的训练数据的并行高速读取，这与我们以前使用的对象存储有所不同。这些方面都需要额外的算力支持，有时我们称之为 DPU。因此，这些因素加在一起，带来了与以前那种大促或者大数据场景下分布式计算不同的挑战。这些是我感受到的这个挑战的一些方面。</p><p></p><p>InfoQ：进入 AI 时代，数据作为新型生产要素成为企业竞争力的核心，企业越来越重视私域数据的全链路智能管理，这给数据库技术也提出了新的要求，具体表现在以下几个方面：</p><p></p><p>胡月军：首先，我们先就目前企业数据系统的现状和问题进行讨论，再展望未来。目前我们了解到，许多企业的数据分布在不同的系统中，根据不同的应用场景而定。在业务规模较小的情况下，一些简单的查询和事务型数据存储在 MySQL 和 PostgreSQL 中。随着业务的扩展，一些企业会将数据存储到 MongoDB 中以获得更好的水平扩展性，MongoDB 以其文档模型支持半结构化数据存储和读取，但也存在一些问题，例如难以表达多对多的关系和更新时的事务问题。</p><p></p><p>另一类场景是关键字搜索，随着业务数据量的增加，通常会使用 Elasticsearch 作为查询引擎，但这种方案可能存在一定的延迟和更新不友好的问题。</p><p></p><p>还有一类场景是汇总分析，通常可以使用 ClickHouse 或 Snowflake 等工具实现，但在实时性和处理更新方面可能存在一些效率不足。</p><p></p><p>随着 AI、AIGC 等技术的发展，以及大模型对自然语言理解能力的增强，语义搜索逐渐成为许多企业必备的功能。然而，由于大模型主要基于公域数据训练，私域数据往往无法充分利用。解决这个问题的方法之一是通过微调大模型，将私域数据融入其中，但这种方法成本较高，因为需要重新训练模型。</p><p></p><p>另一种方法是将大模型与内部的私有数据知识库结合起来，通过向量化等技术检索内部知识库，以提供语义搜索的答案。这种方法通常称为 RAG 方法。然而，这种方法也面临着数据更新和结构化数据整合等方面的挑战。</p><p></p><p>综上所述，企业为了解决各种不同场景下的问题，通常会采用不同的技术系统，这会增加开发成本和维护成本。此外，由于数据存储在多个系统中，存储成本较高，而数据流转可能会带来一致性和实时性方面的问题。</p><p></p><p>此外，随着 AI 和语义搜索的引入，以及向量化数据的使用，如何与现有的结构化数据进行协同，以提高语义搜索的准确性也是一个挑战。回归到用户对数据的本质需求，主要包括数据的一致性、正确性、实时性以及高性能的存储，查询和挖掘能力。要解决以上问题，我们需要不妥协地面对正确性、性能和实时性等物理极限的需求。</p><p></p><p>InfoQ：从数据分析领域的视角来看，大模型技术将如何改变企业的数据架构，以及用户的日常数据分析体验？</p><p></p><p>王海华：在我的理解中，大模型对数据架构的影响主要表现在两个方面。首先，随着大模型的引入，数据架构需要适应新的应用场景，例如模型的训练、调优和推理，这会对数据的质量组织形式提出更高要求。过去，数据仓库可能更多地服务于结构化数据的报表和算法应用，而现在需要考虑非结构化数据的语料，例如音频、图片和视频等。因此，数据架构需要逐渐演进，从以前的数据仓库向更灵活的架构，如数据中台和布仓一体，转变。</p><p></p><p>其次，大模型的使用可能导致数据元数据的不统一性。例如，在实时和离线数据源之间可能存在不一致性，而新的算法场景又会引入新的元数据需求。因此，统一元数据对数据架构的重要性不言而喻，它能够为不同场景提供统一的数据视图，便于大模型的推理和训练使用。</p><p></p><p>另外，王海华老师提到了数据分析领域。大模型的引入可能会提升数据分析的效率和智能化水平。传统的数据分析工具往往需要复杂的操作，例如编写 SQL 查询和定制开发，而大模型具有强大的语义理解和逻辑推理能力，可以降低数据分析的门槛，使业务用户能够更轻松地进行数据分析。</p><p></p><p>此外，大模型的智能分析能力还可以通过逻辑推理和智能代理技术实现自动化的数据分析。用户可以通过简单的提问，获取复杂数据分析结果，从而提高工作效率。</p><p></p><p>总的来说，大模型技术的引入将带来数据架构的变革和数据分析效率的提升，为企业数据应用带来更多的便利和创新。</p><p></p><h3>二、企业如何应对 AI 大模型盛行带来的新变化</h3><p></p><p></p><p>InfoQ：在 AI 大模型的背景下，企业构建智算平台涉及哪些核心的技术要点？在实践和落地过程中，需要特别注意哪些问题 / 可能会踩到哪些“坑”？</p><p></p><p>刘超：首先，智算平台与通用计算存在一些不同之处，主要体现在管理、计算、存储和网络方面的改变。管理方面，调度器和工作节点之间的互通操作不会有太大变化。但在计算方面，由于大模型对 GPU 的使用效率要求较高，通常会采用裸金属技术，直接访问 GPU 卡，而非虚拟化。裸金属服务器之间的虚拟网络需要通过 DPU 完成。GPU 之间的数据互通需要更加低时延的网络，多会使用 RDMA 网络，InfiniBand 来实现。在存储方面，针对大模型计算的高并行下载需求，也需要提升网络的性能，也会采用高性能网络，可以使用 RoCE 网络，也可以使用 InfiniBand 网络。</p><p></p><p>在软件层面，需要配备 GPU 算力调度技术，满足大规模计算任务的运行，需要配备并行文件系统，能够满足大模型训练的高并行下载需求。因此，在构建智算平台时，需要对整个计算、网络和存储架构进行全面的重构，以满足 AI 计算的需求。</p><p></p><p>此外，构建智算平台也会带来一些挑战。企业可能需要接触一些新的技术栈，采购新的硬件设备，这些通常相对昂贵。一方面，企业需要进行大量的新硬件的适配工作，例如新的组网，驱动，插件，内核模块等，另一方面企业需要进一步优化各个方面，包括调度、GPU 复用、网络协议，存储协议等，以最大程度发挥这些昂贵硬件的作用。</p><p></p><p>总的来说，构建智算平台是一个复杂的过程，硬件复杂，软件平台复杂，部署复杂，优化复杂，使用好能发挥出算力也复杂，对任何企业的技术和资源都是很大的挑战。</p><p></p><p>InfoQ：天翼云在自身实践或如何赋能企业完成转型这方面有什么经验和实践？</p><p></p><p>刘超：在过去一段时间里，我们都注意到大模型层出不穷，呈现出爆发性的趋势。作为云厂商，我们主要服务 ToB 客户，在对客户落地大模型方面积累了一些实践经验。</p><p></p><p>当前大部分企业实践大模型有一个特点，即从零开始训练一个完整的大模型成本过高，因此很多企业更倾向于在现有模型的基础上进行调优。这种模式在实践中比较普遍。</p><p></p><p>在将模型落地时，企业通常需要进行全面评估，而不是像以前对待其他新兴技术例如容器微服务一样，进行简单地尝试，因为成本比较高。这种评估涉及到多个方面。</p><p></p><p>首先是对算力需求的评估，包括计算、网络和存储几个方面。计算方面的评估需要考虑现有模型的参数规模、数据量的大小，采用的调优算法等因素，以确定所需的 GPU 卡的数量和训练时间成本。数据存储的评估，需要考虑数据量和文件系统的选择，以保证训练数据的读取速度。网络方面的评估需要确定是否需要重新建设高性能低延时的 RDMA 网络以支持大模型的通信。</p><p></p><p>其次企业在建成大模型平台之前还需要对数据进行评估，包括数据量和数据的质量。数据质量的评估尤为重要，因为低质量的数据可能会导致浪费时间和金钱。</p><p></p><p>最后还需要评估平台建设的复杂度需求，是否需要构建一个 GPU 卡调度平台，其上是否需要构建一个深度学习平台，再往上是否要构建一个模型训练平台。</p><p></p><p>在落地的过程中，我们通常会和客户协商从这几个方面出发，让客户更有效地利用资源，建立一个高效的计算平台。</p><p></p><p>InfoQ：可以结合一下实际场景分享一下我们是在利用大模型进行智能数据分析落地所存在的挑战的吗？我们在智能数据分析技术建设实践方面有哪些新的思考和尝试？</p><p></p><p>王海华：关于大模型引入后，如何应对数据架构和数据分析方面的需求和挑战。首先，我们应该从数据分析方面入手。在大模型时代，数据架构方面出现了一些问题，包括私域数据和统一数据带来的挑战。我先从数据分析方面谈起，因为我们在智能数据分析方向做了一些实践工作，正如 28 米分享的主题所述。回到的话题，我们是一个类似货运版滴滴的业务场景，目前已经发展到中等甚至以上级别的互联网公司规模。</p><p></p><p>公司非常重视数据，我们已经全面收集和存储了大量数据，并通过数据应用形式为经营决策、精准营销、风控和地图 LBS 等场景提供支持。然而，随着大模型的引入，我们也意识到智能数据分析应该提供更深层次的洞察力，实现智能化。目前，我们的智能化水平还有待提高，数据分析仅仅是提供了数据，但在某些场景和 AI 方面的智能化还不够深入。</p><p></p><p>有了大模型后，我们发现可以将其与数据分析相结合，例如我们运营团队每天进行大量的业务策略调整和效果分析，包括业务数据的归因，以及订单和用户增长等方面的监测。对他们来说，这些场景非常重要。因此，针对这些关键场景，我们希望提供一个低门槛的入口，让他们可以轻松获取相关信息和洞察力，而不需要依赖复杂的数据产品或深度分析报表。</p><p></p><p>基于这个需求，我们启动了一个名为“速查”的项目，旨在建立一个智能数据分析的统一入口。我们希望逐步简化现有的数据产品，将其整合为一个统一的智能入口。同时，我们希望通过这个入口提供简洁、易用的数据查询和洞察力，以及整合各种数据分析产品的能力，实现用户语义化的输入和简洁的输出。</p><p></p><p>然而，在简化过程中，我们面临着数据质量和数据指标的挑战。我们希望将分散的数据逐步整合为一个统一的指标体系，并提供高质量的数据和元数据。通过这种方式，数据查询和洞察力的获取将变得更加轻松。此外，我们也意识到大模型在推理能力方面存在局限性和幻觉，而在数据场景中，对数据正确性的要求非常高。因此，我们需要在数据层面上做出一些思考和尝试，以确保数据的质量和指标的准确性。</p><p></p><p>另外，我们需要清楚地认识到，大模型的能力可能是临时的，并随着时间的推移而发生变化。因此，我们需要在解决问题和应对挑战时做出明智的判断，并寻找正确和高效的解决方案。我相信这些思考和尝试不仅适用于数据分析场景的落地，也适用于其他所有领域的应用落地。</p><p></p><p>InfoQ：在具体实践过程中，如何提升智能化程度同时确保数据分析平台的准确性、稳定性？</p><p></p><p>王海华：实际上，他在询问所使用的模型类型，并能否提供几个场景的例子。当前，大模型的应用状态非常多样化，外部供应商如 OpenAI、Google，国内的文心、通义千问等，提供了多种选择。对于模型的选择，我们应该根据业务探索阶段的需要，尽可能选择行业最顶尖的模型，而不是平均水平的模型。在探索阶段，确保数据安全和合规的前提下，选用性能最强的模型能够最大化业务效果。而在上线后的服务期，我们需要考虑更多因素，如数据安全和成本。模型越大，推理成本越高，因此需要在稳定性和性能之间寻找平衡。</p><p></p><p>在我们公司，有两类模型。一类是简单场景下的商用模型，例如阿里的通义，在许多场景下表现良好。另一类是私有化部署的模型，适用于数据敏感的场景，例如司机录音总结和一些涉及个人隐私数据的应用。对于需要领域微调的场景，我们会优先选择私有化部署并进行微调的模型，因为商用模型可能无法满足这些定制化需求。</p><p></p><p>最后，在上线服务时，除了考虑业务效果，安全性也是至关重要的。我们需要确保大模型不会产生违规内容，如涉黄、涉暴、涉恐等，这需要审核和风控审核的能力。对于内容风控方面，传统方案往往效果有限，因此考虑使用大模型来提升安全性和审核效率。综合考虑成本、安全性、效果以及定制微调等维度，选择适合自己场景的模型是一个综合性的决策。这是我的回答。</p><p></p><p>InfoQ：在数据库领域，融合大数据以及 AI 能力的 Data Warebase 核心技术成为一种趋势，在实现传统 TP/AP/Text Search 能力的同时，还可以通过向量计算和与大模型的结合来更精准的实现企业私域数据智能化和 Universal Search，从而帮助企业更好地利用数据资产，实现业务创新。</p><p></p><p>胡月军：海华老师介绍了在提升数据分析和智能体验方面的工作。他们尝试提高精准度，并在数据治理方面做了工作，以解决大模型和数据结合时的问题。刘超老师和其他老师在大模型的开发和应用方面有丰富经验，为数字平台的支持和应用提供了宝贵经验。在企业使用数据时，通常涉及多个系统，因此需要一种统一的数据产品，能够融合数据库、大数据技术和 AI 能力，以解决不同场景带来的问题。我们公司在 Data Warebase 方面做了大量工作，能够实现传统的 TP/AP 搜索和语义搜索能力，同时结合大模型和向量计算，提供企业语义搜索的能力。</p><p></p><p>我们的 Data Warebase 具有以下特点：为了实现 TP/AP 搜索和语义搜索的能力，我们在索引方面做了大量工作，包括行列, bitmap 索引和向量化索引的支持。此外，在向量化执行、分布式事务和优化方面，我们也做了大量工作，以保证系统的正确性和性能。另外，我们采用了存算分离的架构，能够更好地分离计算和存储，实现极致的弹性体验，同时节约成本。</p><p></p><p>为了解决多系统问题，我们需要系统具有较强的自适应能力，包括索引选择，查询并发度确定、事务提交一阶段、两阶段等方面。此外，我们对 SQL 和 PG 生态进行了兼容，方便用户使用已有工具，而不需要修改。总的来说，我们的 Data Warebase 致力于更好地应对 AI 带来的挑战，通过整合和训练数据，提升效率。我们相信，未来的 AI 会更好地理解索引，并与结构化数据一起使用，从而提升精准度。我们认为，Data Warebase 将成为解决用户数据存储和计算需求的统一系统，为企业私域数据智能化和各种计算场景的结合提供满足能力。</p><p></p><p>InfoQ：天翼息壤和慧聚，云骁的适用场景有什么区别是什么？</p><p></p><p>刘超：首先，云骁的侧重点在于智算平台，主要用于 GPU 资源的调度和底层 Infra 的管理，基于的就如上述的 GPU 裸金属，DPU，并行文件系统、RDMA 网络等新型算力技术。慧聚则侧重智算的服务层，提供任务调度处理，可以帮助用户进行模型训练或微调，可以大幅降低大模型训练、微调、部署、推理的门槛。</p><p></p><p>息壤类似于一个算力云市场，符合国家政策中“东数西算”的战略，各地的算力注册到线上平台，用户可以根据需求选择合适的集群执行计算任务。息壤的处理方式与传统的虚拟机及容器算力不同，它实现了全方位的多地多集群调度，相较于单数据中心单集群的平台而言，息壤的优势更明显。</p><p></p><h3>三、ArchSummit 会议上的议题亮点</h3><p></p><p></p><p>InfoQ：王海华老师在数据架构领域拥有丰富经验，曾在滴滴、饿了么和拼多多等公司都有过相关经历。在这些公司，您是如何应对不同规模和需求的数据平台架构的？特别是在数据平台架构和保证数据复杂度、大模型性能和准确性方面的应对。最后，您能否对此进行一些总结或概述呢？</p><p></p><p>王海华：理解您的问题是关于不同规模下数据架构的不同之处，以及如何应对这些挑战。确实，我有几家互联网公司的经验，它们可能处于不同的阶段，有的在快速增长期，有的已进入成熟期，甚至可能同时处于不同阶段的状态。这会对数据架构提出不同的要求。</p><p></p><p>我想分享一些我个人的见解。比如在快速增长期，比如电商和外卖领域，每月的业务量都在快速增长，带来了算力和存储的飞速增长。这首先对数据架构提出了两点挑战和需求：一是需要极强的扩展性，因为业务容量可能会在短时间内翻倍甚至几倍增长。如果扩展性不足，可能会遇到瓶颈，无法支持快速增长的业务，可能引发稳定性和性能问题。幸运的是，在大数据技术的生态系统中，许多组件在扩展性和容错性方面做得相当不错。我们可能只需要做一些补充就能满足需求，这是一个幸福的地方。当然，如果我们使用自研技术，就需要考虑技术深度，确保扩展性和容错性。</p><p></p><p>第二个挑战是业务的快速增长可能会带来许多新需求和项目，需要数据平台和架构快速支持。有时候，我们可能会发现在业务能接受的情况下，做到尽善尽美可能会牺牲一些，因为资源是有限的。我们引入了一些配置化和技术补充来应对这些需求。比如在 Doris 的早期阶段，我们就引入了它来支持实时写入和高效的数据查询。尽管 Doris 当时并不十分成熟，有时会出现问题，但我们通过澄清和解释，与业务保持了良好的沟通。</p><p></p><p>【活动推荐】</p><p></p><p>将在 6 月 14-15 日举办的深圳 <a href="https://archsummit.infoq.cn/2024/shenzhen/schedule">ArchSummit 架构师峰会</a>"上，胡月军和刘超老师出品的专题，将邀请 vivo、天翼云、网易、火山引擎、eBay、货拉拉、Uber 的专家来分享各自在大模型算力、AI &amp; Data 结合方面的实践话题，感兴趣的可以点击查看会议详细的议题内容。目前会议门票售价 9 折期间，购票人数越多优惠力度越大，欢迎来现场和演讲嘉宾、同行交流。</p><p><img src="https://static001.infoq.cn/resource/image/a7/d3/a7169c8f216f8af139e2f6886de5b8d3.jpg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/JtwOOwiVXHioT6OiWDEr</id>
            <title>MediaTek举办天玑开发者大会MDDC2024，携手产业伙伴共创生成式AI新生态</title>
            <link>https://www.infoq.cn/article/JtwOOwiVXHioT6OiWDEr</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/JtwOOwiVXHioT6OiWDEr</guid>
            <pubDate></pubDate>
            <updated>Tue, 07 May 2024 03:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: MediaTek, 天玑开发者大会2024, 生成式AI技术, 天玑AI先锋计划
<br>
<br>
总结: MediaTek在2024年5月7日举办了天玑开发者大会2024（MDDC 2024），以“AI予万物”为主题，深入研讨生成式AI技术为移动生态带来的变革与全新机遇。会上，MediaTek联动天玑平台合作伙伴，共启“天玑AI先锋计划”，共同定义生成式AI手机，分享了生成式AI端侧部署的解决方案“天玑AI开发套件”以及全场景的创新应用。 </div>
                        <hr>
                    
                    <p>MediaTek 在2024年5月7日举办了天玑开发者大会2024（MDDC 2024），本届大会以“AI予万物”为主题，深入研讨生成式AI技术为移动生态带来的变革与全新机遇。会上，MediaTek 联动天玑平台合作伙伴，共启“天玑AI先锋计划”；联合业界生态伙伴发布《生成式AI手机产业白皮书》，共同定义生成式AI手机；分享了生成式AI端侧部署的解决方案“天玑AI开发套件”以及全场景的创新应用。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/f3/0c/f36b28d628a47e147046b8f84ba0e60c.jpg" /></p><p></p><p>此外，大会还展示了基于先进的 MediaTek 星速引擎技术所构建的丰富游戏生态和先进体验。MediaTek 天玑9300+旗舰5G生成式AI移动芯片也正式亮相，以卓越的全大核架构设计和生成式AI能力，助力终端设备旗舰体验再升级。</p><p></p><p>MediaTek 董事、总经理暨营运长陈冠州表示：“生成式AI彻底革新了终端应用的使用价值，智能终端是生成式AI普及的关键载体。MediaTek 凭借在边缘计算领域的深厚功底和丰富经验，每年赋能20亿智能终端设备，在人机交互、生产力、娱乐体验等方面带来了前所未有的创新。通过天玑平台的优势，以及‘天玑AI先锋计划’，MediaTek 将融合产业生态伙伴的力量，高效地赋能开发者，加速建构从云端到终端的AI新生态，推动生成式AI技术在智能终端上的应用普及，让更多用户享受到全新的高端生成式AI体验，加速万物AI时代的到来。”</p><p></p><p>为促进产业共创实现生态共赢，MediaTek 在天玑开发者大会上联合阿里云、百川智能、传音、零一万物、OPPO、荣耀、vivo、小米启动“天玑AI先锋计划”，该计划面向全球开发者，致力于整合 MediaTek 与产业生态伙伴资源为勇于探索和创新的开发者提供开发资源、技术支持和商业机会，助力开发者在搭载天玑芯片的终端设备上打造创新的用户体验。</p><p></p><p>会上，MediaTek 与 Counterpoint 携手阿里云通义千问、百川大模型、虎牙、酷狗音乐、零一万物、OPPO、Soul、腾讯AI Lab、腾讯混元、vivo，联合发布《生成式AI手机产业白皮书》，共同定义了“生成式AI手机”的概念和典型特征。《生成式AI手机产业白皮书》详细地阐述了生成式AI与智能手机深度融合的趋势，深入探讨了生成式AI手机生态中芯片厂商、手机厂商、大模型厂商、开发者的AI战略，以及生成式AI手机的软硬件科技全景，并展望了生成式AI手机的发展趋势。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/96/f8/965dc973edf8348a60a8f90a83yy51f8.jpg" /></p><p></p><p>MediaTek 天玑AI生态战略由芯片、模型、应用构成，MediaTek 与产业伙伴、开发者共同探索和发展端侧AI技术，推动终端设备和应用从触控（Graphical Touch UI）向由生成式AI赋能的智能体（Agent UI）加速演进，为用户提供更出色的交互体验、多模态内容生成能力、个性化服务以及革新的应用生态。MediaTek推出“天玑AI开发套件”，包括快速高效的GenAI最佳实践、覆盖全球主流大模型的 GenAI Model Hub、高效提升性能的 GenAI 优化技术和 Neuron Studio 一站式视觉化开发环境等四大模块，为广大开发者提供“快、全、强、易”的专业开发体验，赋能终端生成式AI应用开发全流程。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/ca/32/ca5f74ee8e0d20ae645ce0f75dbfc132.jpg" /></p><p></p><p>其中，GenAI 最佳实践通过模型量化、模型编译和模型推理技术加速了大模型的终端部署，可从数周加速到一天；GenAI Model Hub 适配行业前沿主流的大模型，为开发者高效构建生成式AI应用提供丰富的大模型资源。天玑AI开发套件还支持推测解码加速、LoRA Fusion 等先进的 GenAI 优化技术；Neuron Studio 集成开发环境可提供一站式可视化的开发环境，跳出传统的代码开发环境，带来“所见即所得”般更易用的开发体验。目前，天玑AI开发者套件已覆盖智能手机、智能汽车、物联网、个人电脑等智能终端设备，为全场景生成式AI应用开发赋能。&nbsp;</p><p></p><p><img src="https://static001.infoq.cn/resource/image/b9/39/b9ffyyb94b3262b2fd239294900c1f39.jpg" /></p><p></p><p>MediaTek 是端侧生成式AI先锋，为行业提供了完善的端侧生成式AI开发者解决方案，与全球生态伙伴同行，持续推动全场景生成式AI应用的普及和发展。目前，MediaTek 已携手阿里云通义千问 、Cocos、Honor of Kings、虎牙直播、酷狗音乐、美图、全民K歌、RWKV、Soul、腾讯AI Lab、小红书等数十家生态伙伴共创体验更精彩的生成式AI应用。</p><p></p><p>MediaTek 星速引擎全面升级，借助自适应技术和硬件光线追踪技术软件开发套件，通过精准的性能管理、光线追踪效果优化、网络质量监测等关键技术，助力游戏开发者打造画面更逼真、流畅，触控和网络响应速度更快，续航更持久的全优体验。此外，MediaTek 还积极与 Arm、Google、Khronos，以及 Unreal Engine 团队在内的产业合作伙伴共同进行前沿技术的探索，持续赋能高速拓展的天玑游戏生态圈。</p><p></p><p>MediaTek 延续天玑旗舰的突破精神，发布天玑9300+旗舰5G生成式AI移动芯片，以先进的全大核架构设计和生成式AI引擎带来更优异的旗舰体验。</p><p></p><p>MediaTek 天玑9300+ 采用全大核CPU架构，八核CPU包含4个 Cortex-X4 超大核，最高频率可达3.4 GHz，以及 4 个主频为 2.0GHz 的 Cortex-A720 大核，为高端手机用户和游戏玩家提供卓越体验。此外，天玑9300+ 拥有强大的生成式AI能力，率先在端侧支持AI推测解码加速技术，同时支持天玑 AI LoRA Fusion 2.0 技术，提供更高效和个性化的生成式AI体验。天玑9300+支持前沿主流的生成式AI大模型，可为用户提供文字、图像、音乐等端侧生成式 AI 多模态创新体验。同时，还支持 AI框架 ExecuTorch，可加速端侧生成式AI应用的开发进程。天玑9300+搭载星速引擎自适应技术，显著降低游戏在高画质运行时的功耗表现，让满帧畅玩更持久。MediaTek 星速引擎网络质量监测系统可实时监控游戏网络的连接质量，支持游戏更高效运用Wi-Fi、蜂窝网络并发技术，为在线游戏提供持久流畅的网络连接体验。</p><p></p><p><img src="https://static001.infoq.cn/resource/image/bf/87/bf70463839f25a82b8068d091f524487.jpg" /></p><p></p><p>自 “天玑”品牌诞生以来，MediaTek 持续通过大力投资技术与开放合作，构建了蓬勃向上的天玑移动生态。今天，先进的生成式AI正在改变移动产业，MediaTek 始终坚信科技能够改善人类生活，并致力于以技术创新驱动产业共融、共创、共赢，将携手更多生态伙伴构建基于生成式AI应用的新时代。</p><p></p><p>（*以上公司名称排序，均以公司名称拼音的首字母顺序排列）</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/ezKOSppmsVFiMmWuJUyA</id>
            <title>爆火Rabbit R1翻车，被爆套壳安卓；特斯拉FSD入华进入倒计时；Sora创意大片主要靠人工后期｜Q资讯</title>
            <link>https://www.infoq.cn/article/ezKOSppmsVFiMmWuJUyA</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/ezKOSppmsVFiMmWuJUyA</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 May 2024 06:46:59 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: Rabbit R1, 安卓, 争议, OpenAI
<br>
<br>
总结: Rabbit R1是一款爆火的AI硬件，被曝出实际运行安卓系统，引发争议；同时，OpenAI向所有ChatGPT Plus用户开放了“记忆”功能，展示了人工智能技术的不断进步。天涯社区即将恢复访问，经历了资金筹集和技术实施的过程。此外，余承东卸任华为终端BG CEO一职，华为内部人事调整引发关注。 </div>
                        <hr>
                    
                    <p></p><blockquote>爆火 AI 硬件 Rabbit R1 翻车，被爆套壳安卓；天涯社区将恢复访问；余承东卸任华为终端 BG CEO；OpenAI 向所有 ChatGPT Plus 用户，开放“记忆”功能；Sora 爆火大片背后真相；向男友泄露机密？小红书辞退涉事员工；马斯克再挥裁员大刀！特斯拉整个超级充电团队被解散；字节跳动内部反腐；马斯克闪电访华，特斯拉 FSD 入华进入倒计时？苹果挖走大量谷歌员工，全力布局 AI 打造神秘团队；快手北京游戏事业部全被裁了；谷歌解雇数百名核心员工，将部分职位转移到印度和东欧；BASIC 语言诞生 60 年；八家美国报纸起诉 OpenAI 和 Microsoft 侵犯版权；万年“8GB”时代！Mac 起步内存停止升级……&nbsp;&nbsp;</blockquote><p></p><p></p><h2>科技公司</h2><p></p><p></p><h4>爆火AI硬件Rabbit&nbsp;R1翻车，被爆套壳安卓</h4><p></p><p>5月1日消息，科技博主Mishaal&nbsp;Rahman发文称，Rabbit&nbsp;R1内部运行Android系统，其整个界面都由安卓应用提供支持。</p><p></p><p>几个月前，Humane、Rabbit&nbsp;两家初创公司陆续推出他们的人工智能设备&nbsp;——售价700美元的Ai&nbsp;Pin&nbsp;和售价199美元的&nbsp;Rabbit&nbsp;R1。最初，一些人认为这些设备将开创可穿戴人工智能的新时代。然而，几个月过去了，对于这两款设备的争议逐渐增多。然而争议不妨碍&nbsp;R1&nbsp;热卖。</p><p></p><p>本周二，Rahman&nbsp;曝光了知名生成式&nbsp;AI&nbsp;硬件&nbsp;Rabbit&nbsp;R1&nbsp;是一个安卓应用程序，立即引来了科技圈的关注，毕竟年初该公司CEO吕骋高调宣传搭载的是全新操作系统Rabbit&nbsp;OS，<a href="https://player.vimeo.com/video/901031775?h=faae8fdece">希望“摆脱</a>"<a href="https://player.vimeo.com/video/901031775?h=faae8fdece">当前智能手机使用的操作系统”。</a>"</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c93eeaec4dfb81a72dc0bc614c248ca.jpeg" /></p><p></p><p>同时，他还表示他们团队修改了Rabbit&nbsp;R1的launcher&nbsp;APK，从而可以将其直接转换到安卓应用程序，甚至可以在谷歌Pixel&nbsp;6a手机上运行。如今，甚至有用户已经在Rabbit&nbsp;R1运行安卓App了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c0/c0bb5db8a439c0cea1601cb86c0f3463.jpeg" /></p><p></p><p>所以简单来说，虽然&nbsp;R1&nbsp;上的操作系统可以是一个定制化安卓系统，甚至是一个定制化的&nbsp;Android&nbsp;ROM，但其实只是包含在一个&nbsp;.apk&nbsp;文件中的&nbsp;App，且可以在其他地方安装。</p><p></p><p>在&nbsp;GitHub上，还有人表示收到了泄露的代码，声称该设备只是运行几个自动化脚本：“这就是为什么他们只支持四个应用程序：Spotify、Midjourney、Doordash&nbsp;和&nbsp;UberEats。”“更糟糕的是，他们将用户会话存储在自己的计算机上，而没有任何额外的安全层。这既是对用户隐私的公然无视，也是一种极其糟糕的工程实践。”“可悲的是，对于任何了解该团队的人来说，都不应该感到震惊。毕竟两年前他们还在兜售&nbsp;NFT。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/a0/a0ad2ea960ddc94abf42e06039b1610f.jpeg" /></p><p></p><p>周三，吕骋发布了一份声明，辩称&nbsp;R1&nbsp;的界面并非一个安卓应用，并解释说他们调用了运行在云端的大模型（这一点实际也没有人质疑过）。</p><p></p><p>“Rabbit&nbsp;OS&nbsp;和&nbsp;LAM&nbsp;运行在云端，并使用了经过定制的&nbsp;AOSP&nbsp;和底层固件修改。Rabbit&nbsp;OS&nbsp;是专为&nbsp;R1&nbsp;设备定制的，我们不支持第三方客户端。使用盗版&nbsp;APK&nbsp;或网页客户端会带来重大风险；黑客可能会窃取你的数据。因此，我们建议不要使用这些盗版&nbsp;Rabbit&nbsp;OS&nbsp;应用。在今天OTA之后，我们实施了多项云验证改进来验证设备/客户端请求。”几个小时后，Rahman&nbsp;在推特上表示，他的&nbsp;Pixel&nbsp;6&nbsp;版本的&nbsp;Rabbit&nbsp;连接不上了，这似乎证实了吕骋关于新用户验证要求的说法。</p><p></p><h4>天涯社区将恢复访问</h4><p></p><p>4月30日，@天涯社区在微博发布《关于天涯社区网络平台恢复访问进展及新天涯进展的公告》。</p><p></p><p>公告称经过公司积极采取各种筹资措施，**目前恢复网络平台访问所需的专项资金已落实，由于各方认可的筹资方案近日才形成，正在走必要的流程，加上技术的实施，正式恢复访问仍需要一些时间。**在确保数据安全合规的情况下，最初的恢复访问将从参与天涯会员计划的用户开始，再面向公众提供访问服务。</p><p></p><p>具体内容如下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/55/5549332a931d51eba632be21446eedf7.webp" /></p><p></p><p>值得注意的是，3月1日，天涯社区曾发布公告称，计划于5月1日前恢复访问。2月，天涯社区传出“破产”的消息，天涯社区创始人邢明就此事回应南都记者称，系一员工被欠薪而向法院提起破产申请，公司没有受到太大影响。</p><p></p><h4>余承东工作重心转移，卸任华为终端BG&nbsp;CEO</h4><p></p><p>4月30日，据报道，华为内部当日发布人事调整文件，宣布余承东将卸任华为终端BG&nbsp;CEO一职，但仍保留终端BG董事长职位。原华为终端BG首席运营官何刚接任华为终端BG&nbsp;CEO。媒体从多位华为人士处确认，此次调整属实。</p><p></p><p>余承东自2011年开始担任华为终端公司CEO，历时近13年。接棒者何刚则是早在1998年便加入华为，2012年出任华为消费者BG手机产品总裁。他在任期间，华为终端业务收入从2012年的1601亿元，增长至2020年的巅峰4829亿元，旗下手机业务也曾一度登顶全球智能手机市场单季度销量首位。</p><p></p><p>目前，余承东在华为任董事会成员、常务董事、消费者&nbsp;BG&nbsp;CEO、智能汽车解决方案&nbsp;BU&nbsp;CEO、智能终端与智能汽车部件管理委员会主任。尽管余承东卸任CEO，但他仍保留终端BG董事长职位，这意味着他仍将在华为的战略决策中发挥重要作用。业界普遍认为，这次人事调整可能是华为为了更好地适应市场变化和公司长远发展规划所做的考虑。</p><p></p><h4>OpenAI向所有ChatGPT&nbsp;Plus用户，开放“记忆”功能</h4><p></p><p>4月30日凌晨，OpenAI在社交平台宣布，向所有ChatGPT&nbsp;Plus用户开放“记忆”（&nbsp;Memory&nbsp;）存储功能。用户通过开启该功能，可以使ChatGPT记住那些冗长、繁琐的内容，而不必每次打开对话框进行重复的提问。</p><p></p><p>这对于写小说、长期健康追踪、企业规划/管理、社交媒体营销、教育等行业帮助巨大。开启记忆存储的ChatGPT，才是真正的AI助手。记住长期对话历史记录需要大量的AI算力和内存资源。在实际应用中，大模型会受到硬件和延迟等因素的限制，难以支持高效的长期记忆。</p><p></p><p>所以，OpenAI开放的“记忆”存储功能对于那些需要长期写作、跟踪某一特定领域的用户来说非常有帮助。</p><p></p><p><img src="https://static001.geekbang.org/infoq/c8/c8957fd0933012e2503b6bf59ad3c07c.png" /></p><p></p><h4>Sora爆火大片背后真相：人工特效参与，被指误导大众</h4><p></p><p>今年&nbsp;2&nbsp;月份，OpenAI&nbsp;发布了人工智能文生视频大模型&nbsp;Sora，并放出了第一批视频片段，掀起了&nbsp;AI&nbsp;生成视频浪潮。目前，Sora&nbsp;仍未进行公测，只有一些视觉艺术家、设计师、电影制作人等获得了&nbsp;Sora&nbsp;的访问权限。他们发布了一些&nbsp;Sora&nbsp;生成的视频短片，其连贯、逼真的生成效果令人惊艳，并且，OpenAI似乎在暗示这些样片都是完全由Sora生成制作。</p><p></p><p>其中，加拿大多媒体制作公司&nbsp;Shy&nbsp;Kids&nbsp;发布了视频短片《Air&nbsp;Head》，在社交媒体上迅速引起广泛关注。最近，知名视觉特效总监&nbsp;Mike&nbsp;Seymour&nbsp;采访了&nbsp;Patrick&nbsp;Cederberg，就《Air&nbsp;Head》制作过程、技术难点等信息展开了提问，并在&nbsp;fxguide&nbsp;上发布一篇文章介绍了&nbsp;Sora&nbsp;在视频实际制作过程中发挥的作用和存在的问题。</p><p></p><p>据悉，这部制作精美的短片AI的参与程度要比预期中的小，实际上为了达到最终效果，他们结合了传统电影制作技术和后期制作剪辑，投入了3个人力，花费约&nbsp;2&nbsp;周的时间。其中，Sidney&nbsp;Leeder&nbsp;担任制片人，Walter&nbsp;Woodman&nbsp;担任编剧和导演，而&nbsp;Patrick&nbsp;Cederberg&nbsp;负责后期制作。</p><p></p><p>例如，气球的颜色在每次生成中都会改变、镜头中会出现一些瑕疵等等，要想获得最佳表现效果，仍需大量后期制作。“这就像是得到一大堆镜头，然后尝试以一种有趣的方式剪辑到旁白中”，Patrick&nbsp;介绍道。对于最终出现在影片中的&nbsp;90&nbsp;秒镜头，Patrick&nbsp;估计他们生成了“数百个&nbsp;10&nbsp;到&nbsp;20&nbsp;秒的片段”。他补充说：“我猜原始素材和最终成品的比例大概是&nbsp;300:1。”这些制作内幕也让我们感受到AI从文本一键生成我们理想中的大片依然只是美好的想象。</p><p></p><h4>向男友泄露机密？小红书辞退涉事员工</h4><p></p><p>小红书4月29日发布关于员工利益冲突、泄露公司商业秘密案件的通报。</p><p></p><p>根据网传一张小红书内部廉政通报截图显示，小红书一部门内容产品组员工杨某因在职期间存在利益冲突未申报且应他人要求多次泄露公司商业秘密，被给予辞退、永不录用，并在公司全员实名通报处罚，并列入行业黑名单。</p><p></p><p><img src="https://static001.geekbang.org/infoq/77/7748c9077be61cd88f37f195d723e3e3.jpeg" /></p><p></p><p>通报称，该员工杨某入职小红书前，就职于某互联网公司。在上家公司任职期间，与该公司副总裁刘某建立恋爱关系。2023年1月30日，杨某入职小红书，其在接受了公司的阳光申报培训，且明知该公司与小红书互为竞对关系的情况下，仍未向公司说明其与刘某的利益冲突关系。</p><p></p><p>入职小红书后，杨某多次应其男友刘某的要求，通过个人微信、飞书文档、共享企微账号等途径向其泄露小红书核心商业机密，泄露的信息中包含大量小红书业务月报、战略目标文档等核心商业机密。</p><p></p><p>通报还表示，公司有权采取包括但不限于提起民事诉讼、追究刑事责任等一切必要法律行动维护自身权益。</p><p></p><p>通报中的杨某男友刘某疑似是B站一员工，他在朋友圈发文回应此事称，过去大半年自己已淡出B站一线管理工作，并且计划留任至五一后离开。他透露，今年春节前，小红书毛总曾邀请自己加入小红书，还主动提到知道自己女友（杨某）也在小红书任职，明确表示不受影响。但因自己尊重行业里的公司，并没有考虑加入小红书。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a8/a89c81f3898b6106ba087bd4eeab8564.webp" /></p><p></p><h4>马斯克再挥裁员大刀！特斯拉整个超级充电团队被解散</h4><p></p><p>据报道，当地时间4月29日，马斯克在公司内部发送一份备忘录宣布，特斯拉超级充电团队负责人丽贝卡·蒂努奇(Rebecca&nbsp;Tinucci)和新产品负责人丹尼尔·何(Daniel&nbsp;Ho)将携整个团队离职，该团队约有500名员工。</p><p></p><p>美股4月30日收盘时，特斯拉股价大跌近6%。市值一夜蒸发了343.46亿美元（约合2487亿元人民币）。此前，负责特斯拉电池、电机和能源产品工程和技术开发的高级副总裁德鲁·巴格利诺(Drew&nbsp;Baglino)也于4月辞职。</p><p></p><p>马斯克在内部信中解释称，多年来，特斯拉发展迅速，在全球各地设立了多家工厂。但与此同时，某些领域的角色和工作职能也出现了重复。在公司为下一阶段的增长作准备时，降低成本并提高生产率就显得极其重要。</p><p></p><p>值得注意的是，在2022年8月举行的特斯拉2022年股东大会上，马斯克曾宣布，未来，特斯拉预计在全球建设10~12家超级工厂，每座工厂规划年产能预计在150万到200万辆之间，10年后有信心实现超过1亿辆的交付量。</p><p></p><p>今年2月，特斯拉就已开始推迟对一部分员工的绩效考核，随后开始传出特斯拉裁员的消息——裁员人数可能高达20%。而此次马斯克发布的内部信内容，无疑印证了此前的裁员消息，只是裁员比例不是20%，而是10%。</p><p></p><h4>字节跳动内部反腐：61起违法违纪案件，4人涉嫌刑事犯罪被立案</h4><p></p><p>4月28日，一则关于字节跳动内部反腐的消息快速出现在各大网媒的热点榜。字节跳动企业纪律与职业道德委员会对外通报了61起员工违法违纪案件，涉及的员工已全部遭到辞退，其中4人更是因涉嫌刑事犯罪被公安机关立案侦查。</p><p></p><h4><img src="https://alidocs.oss-cn-zhangjiakou.aliyuncs.com/res/YvenvKjx0r7AqoyZ/img/da8bfa3a-fa8e-4970-8079-5afeb8f70bf5.png" /></h4><p></p><p>字节跳动此次通报的违法违纪案件数量之多、涉及范围之广，引起了业界的广泛关注。这些案件中，有员工利用职务便利非法收受好处费，有的违规引入服务商并从中获利，还有的滥用工作权限进行牟利。这些行为不仅违反了公司规章制度，更触及了法律的红线。</p><p></p><p>字节跳动在此次反腐行动中表现出了极强的决心和力度。对于涉事员工，公司不仅予以辞退，还取消了他们的期权以及年终奖评定资格，并同步至阳光诚信联盟及企业反舞弊联盟。</p><p></p><h4>马斯克闪电访华，特斯拉FSD入华进入倒计时？</h4><p></p><p>美国东部时间4月29日，美股特斯拉涨15.39%，报194.2美元/股，**创3月4日以来盘中新高，也是2020年3月份以来最大单日涨幅，市值增加826亿美元(约合5974亿人民币)。**同时当日有消息称，特斯拉将使用百度地图提供的高级辅助驾驶地图，用于中国版FSD（Full&nbsp;Self-Drive，完全自动驾驶）。</p><p></p><p>北京车展正在如火如荼地举行中，新能源汽车巨头特斯拉没有参展。但4月28日，特斯拉首席执行官埃隆·马斯克突然出现在了北京。28日晚间，特斯拉发文称，将继续在中国深耕，在人工智能、电动车、储能等领域与行业共同发展，加速清洁能源与自动驾驶技术落地，把美好愿景变成现实。</p><p></p><p>据央视新闻消息，特斯拉CEO马斯克突然访华。业内认为，这加速了特斯拉FSD入华的进程。“特斯拉App已将原选装FSD（完全自动驾驶能力）页面提示的‘稍后推出’变更为‘即将推出’。”更有观点认为，特斯拉FSD入华或将加速中国自动驾驶商业化的应用。“FSD入华肯定能加快自动驾驶相关体系的规范和优化。</p><p></p><h4>多家芯片厂商宣布涨价，最高涨幅20%！</h4><p></p><p>4月29日消息，据媒体报道，在经历了一年多时间的半导体市场需求下滑，迫使芯片厂商竞相降价以争夺订单的惨淡景象之后，自去年四季度以来，半导体市场需求开始回暖。近期，多家国产芯片厂商开始相继宣布涨价，涨幅最高达到了20%。</p><p></p><p>浙江亚芯微电子有限公司于4月23日向客户发出“调价通知函”，宣布对全系列产品单价上调15%-20%不等，立即执行。涨价理由是国际贵金属市场价格近期涨幅平均超过15%，有的材料最高峰达到50%，且有持续上涨的趋势，导致原材料成本不断上涨。</p><p></p><p>深圳市创芯微电子股份有限公司也于4月23日发出“调价通知函”，宣布自5月1日后将对部分产品价格进行上调。涨价理由同样是由于金属等原材料价格的逐步上涨，半导体行业报价不断上涨，产品成本也在不断上涨。据市场研究机构TechInsights预计，2024年全球半导体销售额将增长24%。</p><p></p><h4>苹果挖走大量谷歌员工，全力布局AI打造神秘团队</h4><p></p><p>据媒体近日报道，苹果公司从谷歌挖来了数十名人工智能专家组建成一个团队，在开发新的人工智能模型和产品方面与竞争对手竞争。</p><p></p><p>据悉，自2018年苹果挖来John&nbsp;Giannandrea担任其主管机器学习和AI战略的高级副总裁以来，该公司已经从谷歌挖走了至少36名AI领域的专家。这一数字远超过苹果从其他公司挖走的AI人才数量，凸显出苹果对谷歌AI团队的重视。</p><p></p><p>这些被苹果吸引的AI专家，在加入苹果后，被分配到全球各地的人工智能和机器学习团队中，为苹果开发新的人工智能模型和产品提供强大的技术支持。而在瑞士苏黎世，苹果则建立了一个神秘的“苏黎世视觉实验室”。</p><p></p><p>尽管苹果在AI领域的竞争对手如微软、谷歌和亚马逊等都在不断吹嘘对AI技术的巨额投资，但苹果长期以来对其AI计划一直保持着低调。不过，随着苹果在AI领域的投入不断加大，外界对苹果AI产品的期待也越来越高。</p><p></p><p>据行业人士预计，苹果对AI技术的全新尝试，可能会在即将到来的6月份全球开发者大会(WWDC)上首次亮相。这将是一个重要的时刻，外界将关注苹果将如何展示其在AI领域的最新成果和布局。</p><p></p><h4>全数被裁？快手北京游戏事业部全被裁了</h4><p></p><p>近日，快手北京游戏事业部全数被裁的消息冲上热搜。其实早在今年三月份，快手就正式解散快手北京游戏事业部了，只有少数人转岗至其它项目组。</p><p></p><p>快手北京游戏事业部经历 了一段曲折的发展历程。在2019年7月，他们相继上线了《西行 记：燃魂》 和测试了《代号：北三》。然而，前者的研发周期超过三年，耗资数亿，却未能回收成本。据多方渠道得知，该项目上线后仅两到三个月，北京游戏事业部中台的近70名成员便被裁撤。再往前追溯，2023年年初的一轮裁员，使得当时的北京游戏事业部已不足百人。</p><p></p><p>接连两轮裁撤，让快手北京游戏事业部几乎原地消失。</p><p></p><p></p><blockquote>关于这次裁员，快手为被裁撤员工提供了两个赔偿方案供选择：方案一：&nbsp;被裁员工可以选择&nbsp;N+1&nbsp;的赔偿，其中&nbsp;N&nbsp;基于员工过去&nbsp;12&nbsp;个月的平均月收入。此外，员工还有另一个选择：在两周后离开公司，且在此期间无需到公司上班。方案二：&nbsp;员工也可以选择仅获得&nbsp;N&nbsp;个月薪水的补偿。此外，员工还有一个额外的选项：在原定两周离职期限的基础上，再多待一个月，社保也多交一个月，期间同样无需到公司上班。不过，这个选项不包含&nbsp;+1&nbsp;个月薪水的额外补偿。据了解，大多数员工倾向于选择第二种方案。这样，他们可以有更多的时间来做缓冲，精心准备面试材料，同时也不用担心社保中断的问题。</blockquote><p></p><p></p><h4>谷歌解雇数百名核心员工，将部分职位转移到印度和东欧</h4><p></p><p>5月2日消息，据外媒消息，谷歌在重组中裁掉了至少&nbsp;200&nbsp;名核心团队员工，其中包括将部分职位转移到印度和东欧等地。据谷歌公司网站称，核心部门负责构建公司旗舰产品背后的技术基础，并保护用户的在线安全。</p><p></p><p>此前，谷歌安全工程副总裁&nbsp;Pankaj&nbsp;Rohatgi&nbsp;在电子邮件中告诉他的团队，“为了优化我们的业务目标，我们正在将工作扩展到其他地点，这将导致一些角色的消除和拟议的角色消除。”这其中包括了Python开发团队、技术基础设施、安全基础、应用平台的核心开发人员和各种工程角色。</p><p></p><p><img src="https://static001.geekbang.org/infoq/fc/fc4deed45690c3ad2b118e3835fde9b2.png" /></p><p></p><p>其中，解雇Python语言团队在开发者群体中引起了相当大的反响。其实4月中下旬就有消息称，为了&nbsp;GenAI，谷歌解雇了整个&nbsp;Python&nbsp;团队。但由于缺少发布者的背景信息，这条消息没有引起太多的注意。但随后的消息显然证实了“谷歌解雇&nbsp;Python&nbsp;语言团队”不是谣言。</p><p></p><p>有趣的是，4月26日，谷歌母公司&nbsp;Alphabet&nbsp;公布了第&nbsp;1&nbsp;季度财报：营收为&nbsp;805.39&nbsp;亿美元，较上年同期的&nbsp;697.87&nbsp;亿美元增长&nbsp;15%，创&nbsp;2022&nbsp;年初以来最快增速——同样是发布财报，Meta&nbsp;股价暴跌&nbsp;15%，谷歌却由此正式跻身进入“2&nbsp;万亿俱乐部”。</p><p></p><p>因此很难料想到：在如此坚定投入&nbsp;AI&nbsp;的背景下，谷歌突然解雇了整个&nbsp;Python&nbsp;基础团队？！</p><p></p><p>更多阅读：《<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651203885&amp;idx=1&amp;sn=36b2d6d9883462b231177c1778c879aa&amp;chksm=bdbbd17e8acc5868cdbe22cc3f09405a108bc71d21f852d924553db63d5f2aebe0adaf68c08d&amp;token=1541326405&amp;lang=zh_CN#rd">谷歌裁掉整个&nbsp;Python&nbsp;团队！PyTorch&nbsp;创始人急得直骂人：“WTF！核心语言团队无可替换”</a>"》</p><p></p><h2>IT&nbsp;业界</h2><p></p><p></p><h4>BASIC&nbsp;语言诞生&nbsp;60&nbsp;年</h4><p></p><p>1964&nbsp;年&nbsp;5&nbsp;月&nbsp;1&nbsp;日早晨六点，计算机领域的一场悄无声息的变革在达特茅斯学院展开。数学家&nbsp;John&nbsp;G.&nbsp;Kemeny&nbsp;和&nbsp;Thomas&nbsp;E.&nbsp;Kurtz&nbsp;在通用电气&nbsp;GE-225&nbsp;大型机上成功运行了新开发的&nbsp;BASIC（Beginner's&nbsp;All-Purpose&nbsp;Symbolic&nbsp;Instruction&nbsp;Code）编程语言的第一个程序。</p><p></p><p>他们当时并不知道，新的语言将会推动计算的普及，在未来&nbsp;60&nbsp;年里激励一代又一代的程序员。BASIC&nbsp;是一种解释性编程语言，逐行运行，带有行号。程序可使用&nbsp;GOTO&nbsp;等命令在行之间跳转，可用于创建简单的循环程序。今天的大多数编程语言都使用不同类型的结构范式（如函数和面向对象编程），但&nbsp;BASIC&nbsp;易于掌握的语法及简单的英语关键字受到了新手的欢迎。</p><p></p><p>BASIC&nbsp;之前的语言如&nbsp;Fortran、Algol&nbsp;和&nbsp;COBOL&nbsp;都相当复杂，主要由专业人士使用。1975&nbsp;年保罗·艾伦&nbsp;(Paul&nbsp;Allen)&nbsp;和比尔·盖茨&nbsp;(Bill&nbsp;Gates)&nbsp;将&nbsp;BASIC&nbsp;语言带到了个人计算机上，创办了微软公司。沃茨在苹果电脑上开发了解释器&nbsp;Integer&nbsp;BASIC。</p><p></p><h4>Ubuntu&nbsp;24.10&nbsp;代号&nbsp;Oracular&nbsp;Oriole</h4><p></p><p>将在&nbsp;10&nbsp;月释出的&nbsp;Ubuntu&nbsp;发行版下一个&nbsp;24.10&nbsp;代号为&nbsp;Oracular&nbsp;Oriole。</p><p></p><p>随着按字母次序的形容词-动物组合而成的代号名称日益复杂，其重要性也日益下降，它更多体现为&nbsp;Ubuntu&nbsp;的包存档名称。作为&nbsp;LTS&nbsp;版本之后的下一个版本，而距离下一个&nbsp;LTS&nbsp;版本还有两年时间，意味着&nbsp;Ubuntu&nbsp;24.10&nbsp;的开发者可以更自由的发挥创意。</p><p></p><p>Ubuntu&nbsp;24.10&nbsp;预计将使用&nbsp;Linux&nbsp;6.11&nbsp;内核、GNOME&nbsp;47&nbsp;桌面环境、GCC&nbsp;14.1&nbsp;编译器，将对即将推出的英特尔&nbsp;Lunar&nbsp;Lake&nbsp;平台&nbsp;和&nbsp;Battlemage&nbsp;显卡、AMD&nbsp;Zen&nbsp;5、AMD&nbsp;RDNA4&nbsp;显卡等新硬件提供开箱即用支持。</p><p></p><h4>八家美国报纸起诉OpenAI和Microsoft侵犯版权</h4><p></p><p>4月30日，一场没有硝烟的战争在纽约联邦法院悄然打响。《纽约每日新闻》《芝加哥论坛报》《奥兰多哨兵报》《佛罗里达太阳哨兵报》《圣何塞水星报》《丹佛邮报》《橙郡纪事报》和《先锋报》8家报纸出版商，代表对冲基金Alden&nbsp;Global&nbsp;Capital，对微软和OpenAI提起诉讼。他们声称，这两家公司在未支付任何费用的情况下，非法使用了数百万篇受版权保护的文章，用于训练其先进的AI模型，包括广为人知的ChatGPT。</p><p></p><p>出版商们指出，OpenAI的GPT-2、GPT-3以及GPT-4模型，使用了他们报纸的文本数据集进行训练。微软则被指控复制报纸信息，用于必应搜索索引或作为AI助手的信息来源。更令人震惊的是，GPT-4在某些情况下，会输出与出版商作品内容近乎逐字逐句的副本。</p><p></p><p>OpenAI的发言人对此回应称，公司之前并不知晓Alden&nbsp;Global&nbsp;Capital的担忧，但目前正在与全球多个新闻机构进行积极对话，探索合作机会，化解担忧，并提供解决方案。而微软方面则保持沉默，拒绝发表评论。</p><p></p><p>这场诉讼的背后，是新闻机构对于商业模式的坚守。在互联网时代，新闻机构一直在努力寻找可持续发展的路径。然而，新的生成式AI技术，仅凭从互联网上提取的大量数据，就可能破坏新闻机构的商业模式。MediaNews的执行主编弗兰克·派恩强调，新闻机构不能允许OpenAI和微软继续无偿窃取他们的成果。</p><p></p><h4>万年“8GB”时代！Mac起步内存停止升级</h4><p></p><p>4&nbsp;月&nbsp;27&nbsp;日，苹果&nbsp;Mac&nbsp;基础款型号内存始终停留在&nbsp;8GB，而官方给出的答复是&nbsp;8GB&nbsp;足以驾驭上网、播放、轻量编辑等很多任务场景了。</p><p></p><p>David&nbsp;Schaub&nbsp;近日在&nbsp;Mastodon&nbsp;平台上发布分析论文，表示蒂姆・库克（Tim&nbsp;Cook）掌舵苹果公司之后，苹果基础款&nbsp;Mac&nbsp;的内存升级周期就已经停止了。过去在乔布斯时代，Mac&nbsp;的起步内存会每隔两年升级一次，但目前自&nbsp;2012&nbsp;年以来，苹果&nbsp;MacBook&nbsp;Pro&nbsp;和&nbsp;iMac&nbsp;一直提供&nbsp;8GB&nbsp;RAM，MacBook&nbsp;Air&nbsp;在&nbsp;2017&nbsp;年也升级至相同容量。</p><p></p><p>尽管用户有升级内存的需求，但苹果认为&nbsp;8GB&nbsp;内存已足够应对多数日常任务，并且公司更注重优化硬件与软件的集成度以及芯片与组件之间的通信效率，通过&nbsp;SoC&nbsp;架构的集成来提升性能。</p><p></p><p>该公司声称目前的发展重心更加放在芯片和组件的通信方面，其&nbsp;SoC&nbsp;架构通过密集集成来增强性能。其&nbsp;8GB&nbsp;RAM&nbsp;与竞争对手的&nbsp;16GB&nbsp;RAM&nbsp;相当。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Rp1AOy3jTGLOaiSco6zH</id>
            <title>面壁智能低调开源大模型“理科状元”！LeetCode 周赛超越80%人类选手，推理性能超 Llama3-70B</title>
            <link>https://www.infoq.cn/article/Rp1AOy3jTGLOaiSco6zH</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Rp1AOy3jTGLOaiSco6zH</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 May 2024 06:19:13 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, Eurux-8x22B, SOTA, LeetCode
<br>
<br>
总结: 面壁智能低调发布了大模型Eurux-8x22B，在复杂推理综合性能方面超越了Llama3-70B，刷新了开源大模型SOTA。Eurux-8x22B在LeetCode周赛中表现优异，超越80%的人类选手。面壁智能团队被称为“大模型界最强Buff厂”，他们利用Ultra对齐技术更新了UltraInteract数据集，提升了大模型推理能力。 </div>
                        <hr>
                    
                    <p>在4 月 18 日 Llama3 发布前两天，面壁智能低调开源了大模型 Eurux-8x22B。据悉，该模型在代码和数学等体现大模型核心素质的复杂推理综合性能方面超越 Llama3-70B，刷新开源大模型 SOTA，堪称“理科状元”。</p><p>&nbsp;</p><p>除了开源时间早于LlaMa3，Eurux-8x22B的激活参数仅有39B，推理速度更快，目前支持 64k上下文，相比之下 Llama3-70B 的上下文大小为8K。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/88/888ea8f43f6888ae24f951e6dcaead2d.png" /></p><p></p><p>&nbsp;</p><p>此外，Eurux-8x22B 由 Mistral-8x22B 对齐而来，综合性能不输 Llama3-70B。</p><p></p><p><img src="https://static001.geekbang.org/infoq/bb/bb8a19a8d5fb774ab248e1bab20d453f.png" /></p><p></p><p>&nbsp;</p><p>Eurux-8x22B 模型和对齐数据，全家桶开源：</p><p>&nbsp;</p><p><a href="https://github.com/OpenBMB/Eurus">https://github.com/OpenBMB/Eurus</a>"</p><p><a href="https://huggingface.co/openbmb/Eurux-8x22b-nca">https://huggingface.co/openbmb/Eurux-8x22b-nca</a>"</p><p>&nbsp;</p><p></p><h2>LeetCode 周赛超越80%的人类选手</h2><p></p><p>&nbsp;</p><p>复杂推理能力是体现大模型性能差异的最核心能力之一，也是大模型真正落地应用所需的关键能力所在。根据测评，Eurux-8x22B在代码和数学等复杂推理的综合性能方面刷新开源大模型 SOTA。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/07/078ef08fc7bebbb413b47e13dd744a0f.png" /></p><p></p><p>&nbsp;</p><p>具体而言，Eurux-8x22B在 LeetCode （180道LeetCode编程真题）和TheoremQA（美国大学水准的STEM题目）这两个具有挑战性的基准测试中，超过现有开源模型。</p><p>&nbsp;</p><p>那么开源大模型“理科状元”Eurux-8x22B在实际应用中表现如何呢？</p><p>&nbsp;</p><p>代码能力方面，面壁智能让其参加了近期的一场 LeetCode 周赛，这是一个检验人类程序员编程能力的真实竞技场。</p><p>&nbsp;</p><p>结果显示，Eurux-8x22B 的Python编程能力非常优秀，成功解决了四道算法题中的三道，其综合排名超越了80%的人类参赛选手，可以初步通过互联网大厂的程序员编程面试。下面是周赛中Eurux-8x22B对一道中等难度算法题的真实解答：</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/72b63d05ff4f252e1fd2788fdb691d99.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/76/768ebd7e70f211b87ec3d3a9873aa5a3.png" /></p><p></p><p>&nbsp;</p><p>除了代码能力优秀，Eurux-8x22B 解答数学题也是轻而易举。</p><p>&nbsp;</p><p>例如，给它一道高中排列组合题，Eurux-8x22B 首先给出了清晰的解题思路，然后一步步地拆解执行，再进行结果汇总，最后得到了正确答案。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4ff146859f51b0578529f428c95e392c.png" /></p><p></p><p>&nbsp;</p><p>再考察它一道代数题，Eurux-8x22B 直击要害，运用二项式定理，清晰简洁地给出了正确解答。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6d/6d36415f4cbb9f444d5aa23af5480bcb.png" /></p><p></p><p>接着给它一道向量代数题，Eurux-8x22B 也能轻松拿下：</p><p></p><p><img src="https://static001.geekbang.org/infoq/9e/9e6ab58c5dc29fb10edd8fef2e3cab44.png" /></p><p></p><p>&nbsp;</p><p>高考函数题可能是令很多人回忆起来就头疼的一类题，Eurux-8x22B 也能解答无误：</p><p></p><p><img src="https://static001.geekbang.org/infoq/2f/2f1ecfd6a76770f12ede2dbc5dad448e.png" /></p><p></p><p>&nbsp;</p><p>（注：Eurux-8x22B 没有针对中文语料进行额外的微调和对齐。）</p><p></p><h3>&nbsp;</h3><p></p><p></p><h2>大模型“上分神器”</h2><p></p><p>&nbsp;</p><p>面壁智能是国内极少数兼具大模型算法与 infra 能力的团队：匹配大模型作为系统工程的本质要求，打造了一条从数据原材料、到模型制作过程中训练与调校工艺环环相扣的全流程高效模型生产线，被戏称为“大模型界最强Buff厂”。</p><p>&nbsp;</p><p>本次 Eurux-8x22B 更快、更长、理科更好的全方位惊艳成绩，即来自面壁 Ultra 对齐技术（Ultra Series）更新：新增了大规模、高质量对齐数据集UltraInteract。</p><p>&nbsp;</p><p>UltraInteract是专门设计用于提升大模型推理能力的大规模、高质量的对齐数据集，包含了覆盖数学、代码和逻辑推理问题的12个开源数据集的86K条指令和220K偏好对，共有五十万（条）左右数据。而相比之下，LLaMA 3-70B模型则是使用了千万量级的对齐数据，这从侧面证明了 UltraInteract 数据集的优质性——数据质量胜过数据数量。</p><p>&nbsp;</p><p>面壁智能团队是如何构建高质量的对齐数据？</p><p>&nbsp;</p><p>严格质量控制和筛选。首先，面壁从多个开源数据集中抽样出难度较高、考察多样推理能力的86k复杂推理问题，并使用多个模型来采样答案。通过自动化格式检查和人工质量抽查结合的方式保证了答案格式的一致性和内容的正确性。</p><p>&nbsp;</p><p>逐步推理。对于每条指令，模型都会按照思维链（CoT）格式进行逐步推理（如下图①），生成格式统一但模式多样的推理过程。</p><p>&nbsp;</p><p>多轮交互。在模型给出推理过程之后，会自动与答案对比确定推理过程是否正确（如下图②），如果不正确，UltraInteract会使用另一个批评模型（如下图③）指出错误并给出改进建议，生成新的逐步推理（如下图④），再与策略模型进行多轮交互（如下图⑤⑥），直到答案正确或达到轮数上限为止。这一步有助于模型学会反思和改错能力，在实际表现中让其可以更好地和人进行多轮交互问答。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/3b/3b7454e645f68828cd9f38f52c745d2c.png" /></p><p></p><p>图注：UltraInteract两轮交互的过程</p><p>&nbsp;</p><p>首创偏好树结构。为了深入探究偏好学习在复杂推理中的作用，UltraInteract还为每个问题都构建了一棵偏好树（如下图所示），其中问题作为根节点，每个回复作为一个子节点，每一轮生成两个节点(一对一错相配对)。所有正确推理对应的节点都可以用于SFT，而配对的节点则可以用于偏好学习。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a9/a9c20dd8233f17e1f1563b714b9fd9b1.png" /></p><p></p><p>图注：UltraInteract（第三列）是当前唯一一个树状结构的对齐数据集</p><p>&nbsp;</p><p>除了UltraInteract数据集的大力加持，偏好对齐也对Eurux-8x22B的推理性能提升有所帮助。</p><p>&nbsp;</p><p>面壁智能团队发现，在推理任务中，提升正确答案的奖励值对于偏好对齐的效果十分重要，因为正确答案的空间比错误答案更有限，因此更加重要，模型在训练过程中不能偏离正确答案。然而，当前流行的DPO算法会使正确答案和错误答案的奖励值共同降低，因此在实验中效果不佳。面壁智能采用了另外两种偏好对齐算法KTO和NCA，取得了更好的效果，能在SFT的基础上进一步提升模型性能。</p><p>&nbsp;</p><p>此外，UltraInteract 数据集也在开源社区受到了广泛好评：</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/4b/4bea4ea097250593ab0b67b548d64738.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/b8/b8bb0de8c32370a781e3552311e8b6de.png" /></p><p></p><p>&nbsp;</p><p>据悉，面壁 Ultra 对齐技术此前已经“强壮”了全球超 200 个大模型，尤其擅长提升大模型“以小博大”能力。例如，在面壁 Ultra 数据集的加持下，Zephyr-7B 以更小规模，在不少指标上超越了 LLaMA2-70B-Chat，同时帮助“小钢炮”MiniCPM-2B 取得与 Mistral-7B 一较高下的惊艳表现。</p><p>&nbsp;</p><p>面壁智能表示，未来将持续开源高效大模型及其数据集，开源开放的精神最终将惠及所有人。</p><p>&nbsp;</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/y9zLo2WHVz94YMXk2O4O</id>
            <title>前 LangChain 员工爆料更强的 Devin 2.0 要来了？所以，“世界首个AI程序员”到底造假没？</title>
            <link>https://www.infoq.cn/article/y9zLo2WHVz94YMXk2O4O</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/y9zLo2WHVz94YMXk2O4O</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 May 2024 06:15:19 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AI程序员, Devin, 功能更新, 质疑
<br>
<br>
总结: 3月份，被称为“世界首个AI程序员”的Devin问世，引起广泛关注。Devin具备规划和执行复杂工程任务的能力，并能够学习和修复错误。近日，Devin 2.0 新功能即将上线，包括交互模式、编辑代码功能和Cookie功能。然而，Devin也遭遇了质疑，有人指出其存在造假行为。Scott Wu在采访中表示，Devin在Devops、数据分析等领域有潜力，但也存在不足之处。 </div>
                        <hr>
                    
                    <p></p><p>&nbsp;3月份，有着“世界首个AI程序员”的Devin横空出世，立刻就被大家追捧。据报道，Devin 可以规划和执行需要数千个决策的复杂工程任务，并回忆每一步的相关背景，随着时间的推移学习并修复错误。一时间，各个程序员们心里慌慌。</p><p>&nbsp;</p><p>近日，前 LangChain员工Andrew Gao在网上爆料了即将上线的Devin 2.0 新功能。</p><p>&nbsp;</p><p></p><p></p><p>&nbsp;</p><p>首先，启动交互模式以帮助 Devin 浏览网络。如果被卡在图片验证码之类的东西上，那么它非常有用。诚然，它有些慢（他们承认这一点），但它工作得足够好，能够做出点击动作。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e55bcb95b6e3d8c737d943e80e2f081.png" /></p><p></p><p>&nbsp;</p><p>其次，之前大家抱怨的使用 Devin 无法干预和编辑代码，现在可以通过启动 Web VSCode 来执行此操作。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b90c214739131708791ecae2c9422ed4.jpeg" /></p><p></p><p>另一个更新则是 Cookie，它让 Devin 能够使用用户的帐户登录网站，而无需向 Devin 提供用户密码。 PhantomBuster 也做了类似的事情。</p><p>&nbsp;</p><p>Andrew 举了个例子，他让 Devin 在 DoorDash 上订购鸡翅， Devin 很好地找到了店铺Wingstop、选择了鸡翅以及操作各种复选框......</p><p>&nbsp;</p><p></p><p></p><p>&nbsp;Devin 现在似乎更擅长编写网站：</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/67/676d6f6a56523a20676979db3093ec02.png" /></p><p></p><p>Devin 还新增了“机器快照”功能，机器快照可以让用户保存 Devin 的状态，这样当服务器关闭时，用户可以再次启动。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ce/ce02b517a92aa003e7ac9d246cdb240e.png" /></p><p></p><p>Devin 还支持与GitHub 集成，可以让 Devin 进行提交。</p><p>&nbsp;</p><p><img src="https://static001.geekbang.org/infoq/ed/ed310eead40d1d6834f3d0c2ae297627.png" /></p><p></p><p>不过需要注意的是，Devin 背后公司Cognition并未正式发布上述功能。</p><p></p><h2>创始人最新访谈，闭口不谈造假风波</h2><p></p><p>&nbsp;</p><p>对于Devin来说，最火的时刻有两个：一是3月13日刚发布时，二是两周多后被指造假时。</p><p>&nbsp;</p><p>就在上个月初，一位自称有35年软件工程师经验的网络博主卡尔质疑 Devin 造假，卡尔逐帧复现了Devin的演示视频并提出质疑，主要包括以下方面：</p><p>&nbsp;</p><p>Devin 被认为能够解决任意Upwork 任务。但在视频演示中，要求解决的问题与客户规定的要求（客户要求设置说明，而不是代码）不符；Devin 正在修复 GitHub 存储库源中的错误，但它所编辑的文件实际上并不存在于该存储库中，而且它修复的一些错误是无意义的，属于人类永远不会犯的类型。推论：Devin 一定是在修复它自己创建的文件中的错误，但没有明确指出；EC2 部分不需要进行任何编码，因为存储库中的自述文件包含完成任务所需的所有说明，只需一行调整即可正常工作，即使存储库是旧版本。这就是为什么客户要求提供有关如何在 EC2 上运行的说明，而不是一些编码要求。 Devin 似乎没有阅读 README，也不明白它只需要执行几个预先存在的 Python 脚本。视频中的输出看起来任务很复杂，有很长的计划和许多显示工作已完成的复选框，但实际上这项工作毫无意义且多余；Devin 的代码更改很糟糕，例如编写自己的低级文件读取循环而不是正确使用标准库；虽然视频看起来Devin 很快就完成了任务，并且视频创建者能够在大约30 分钟内完成所请求的任务，但聊天中的时间戳显示该任务持续了多个小时，甚至持续到第二天；Devin 执行无意义的 shell 命令，如“head -n 5 foo | tail -n 5”。</p><p>&nbsp;</p><p>卡尔认为，Cognition Labs夸大了Devin的能力，视频描述和推文中存在谎言，造成混乱和误解。卡尔建议，不要在未经适当研究的情况下盲目重复和放大网上发现的主张。</p><p>&nbsp;</p><p>“几乎没有任何人工智能产品能在经过大肆宣传后的几周后，依然表现让人满意。”有网友评价道。</p><p>&nbsp;</p><p>虽然人们非常期待Cognition能对这些质疑进行回应，但截至目前该团队都没有做出解释。我们只能在4月中旬，Scott的推特中隐约看到他对Devin 缺点的态度：今天的 Devin 还远非完美。Devin 经常工作，但也经常犯错误、编写错误或陷入困境。</p><p>&nbsp;</p><p>5月2日，Scott Wu参加的不到30分钟的采访视频发布。Scott 在视频里表示，未来工程师并不会因为AI减少，反而会越来越多。首先，AI会对工程的需求变大，“很多问题可以用代码解决，也有很多问题可以用代码构建”；其次，Devin 不是决定做什么的人，使用它的人应该知道要构建什么、解决什么问题等，因此他认为Devin 只是让工程师更加纯粹。</p><p>&nbsp;</p><p>Scott 认为，Devin 更加擅长的领域在Devops和Dev设置方面。“Devin 第一个真正让我们兴奋的时刻是数据库表旋转、Kubernets 启动时。” 另一个很好的用例则是数据分析。Scott 强调，Devin是执行者，它的重点是如何准确理解需求后将其表述为代码并做到。</p><p></p><p><img src="https://static001.geekbang.org/infoq/f7/f731d173ee4baa50d011947c62ef607a.png" /></p><p></p><p>“他们给了他一切机会来回应对视频的批评，但他一直回避。他没有说任何实质性内容。这次采访并没有激发人们对他的公司的任何信心。”有网友在采访视频下评论道，甚至有人调侃称，“加密货币诈骗者接受加密货币诈骗者采访。”</p><p>&nbsp;</p><p>当然也有力挺的网友，“在这里看到这么多仇恨者真是太疯狂了。Scott 建立了一支非常优秀的团队，并正在开发一款革命性的产品。”</p><p>&nbsp;</p><p>根据Linkedin显示，该公司目前有超过35人的员工，上面各项动态依然停留在Devin 刚发布那天。</p><p><img src="https://static001.geekbang.org/infoq/90/9070336cf575f63253a667ac9edb6e61.png" /></p><p></p><p></p><h2>“无法透露更多细节”</h2><p></p><p>&nbsp;</p><p>&nbsp;</p><p>Cognition 公司拥有三位创始人：CEO Scott Wu、CTO Steven Hao 和盒首席产品官 Walden Yan。</p><p>&nbsp;</p><p>Scott Wu 自述自己 9 岁起开始编程，并且非常热爱将自己的想法变成现实的感觉。还有人挖出了 Scott Wu 在 14 岁时参加 MathCounts 比赛的视频。在比赛中，Scott Wu 回答奥数问题基本不需要多少思考时间，主持人念完问题，Scott Wu 马上能报出答案。</p><p>&nbsp;</p><p>Hao 此前曾担任 Scale AI 的顶级工程师，这同样是一家价值可观的初创企业，专司 AI 系统的训练工作。Yan 则刚刚从哈佛大学退学，他要求对此事保密，因为自己还没跟父母通过气。创始人还自述团队共有 10 枚 IOI 金牌。</p><p>&nbsp;</p><p>这样的团队已经获得了彼得·蒂尔的Founders Fund基金领投的2100万美元A轮融资。另外根据彭博社报道，前Twitter高管Elad Gil也参与了对Cognition AI的投资。&nbsp;</p><p>&nbsp;</p><p>但 Cognition 如何在如此短的时间内取得重大突破仍然是个未解之谜。</p><p>&nbsp;</p><p>Scott 拒绝透露太多关于该技术的底层细节，只表示他的团队找到了将 OpenAI GPT-4 等大语言模型（LLM）与强化学习技术相结合的独特方法。Cognition 方面也拒绝透露 Devin 在多大程度上依赖于其他现有大语言模型。</p><p>&nbsp;</p><p>Scott 在访谈中也依然表示不能透露更多关于Devin 如何运行的细节。</p><p>&nbsp;</p><p>所有涉及运行实现的部分，整个Cognition团队都三缄其口，增加了神秘感的同时也让外界对其更加怀疑，毕竟“Talk is cheap，Show me your code”已经成为大家共识。</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://twitter.com/itsandrewgao/status/1786617554724921641">https://twitter.com/itsandrewgao/status/1786617554724921641</a>"</p><p><a href="https://www.infoq.cn/article/WXRuf4M0fOibdRIEleJf?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search">https://www.infoq.cn/article/WXRuf4M0fOibdRIEleJf?utm_campaign=geek_search&amp;utm_content=geek_search&amp;utm_medium=geek_search&amp;utm_source=geek_search&amp;utm_term=geek_search</a>"</p><p><a href="https://news.ycombinator.com/item?id=40008109">https://news.ycombinator.com/item?id=40008109</a>"</p><p><a href="https://www.youtube.com/watch?v=OvBiqmcnjHY">https://www.youtube.com/watch?v=OvBiqmcnjHY</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/bVjdiotpLlL7NliKotTs</id>
            <title>架构师会被AI秒了吗？</title>
            <link>https://www.infoq.cn/article/bVjdiotpLlL7NliKotTs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/bVjdiotpLlL7NliKotTs</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 May 2024 03:53:43 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 重构, 大模型, 软件架构, 生产力
<br>
<br>
总结: 演讲嘉宾郭东白副总裁探讨了大模型时代对软件架构师的影响，强调了重构的重要性以及大模型对软件研发和生产力的影响。大模型的出现改变了传统软件开发中的角色，提高了测试速度，可能重新定义程序员的价值。在大模型时代，大模型成为主要生产力，对软件研发的成本和人性等方面产生重大影响。 </div>
                        <hr>
                    
                    <p>演讲嘉宾 | 郭东白 Coupang 副总裁</p><p>编辑 | 华卫</p><p></p><p>对于一名架构师，最能让其感到兴奋的词汇莫过于“重构”。只要有重构的需求，就意味着有工作可做。在大模型时代，全球范围内的大模型正在重新定义软件的面貌。在当前的世界正在重新定义软件的背景下，作为架构师应该如何应对？面临哪些机会和挑战？这些都是本演讲想要探讨的主题。</p><p></p><p>本文由 InfoQ 整理自 QCon 北京 2024 主论坛演讲《大模型时代的架构思维》，经郭东白老师授权发布。以下为演讲实录：</p><p></p><p>本演讲将从大模型讲起，分析大模型的本质及其影响；然后我们将探讨大模型如何影响软件研发，以及它对软件架构的冲击是什么；最后将讨论我们应该如何应对这些挑战和冲击。这是一个清晰的逻辑过程：首先理解问题，然后制定应对策略。</p><p></p><p></p><h1>大模型时代的新生产力</h1><p></p><p></p><p>大模型的基本原理很简单：你有一套训练数据，通过训练生成式模型，产生许多候选样本，然后人工挑选出较好的内容。接着，使用验收模型将人工挑选的过程自动化。这样一来，我们就形成了一个从生成式模型到验收模型的完整过程。当这个过程上线后，就可以形成一个循环，使得模型能够持续学习和进步。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a3/a3da07eb93e50363be1f7ca8880bff75.png" /></p><p></p><p>大模型的突破之处在于其处理大量数据的能力，这使得它能够涌现出类似语义理解的能力。这让我们可以感觉到，大模型似乎能够在完全不同的语境中，甚至是跨越传统意义上的语言界限，实现近乎无损的语义转换。这就是大模型突然变得如此热门的原因。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2e/2e7a647f993039c2776b2e28bb7b504e.png" /></p><p></p><p>事实上，我在一年前就已经在网易的一次演讲中提到了这些观点。我认为这个逻辑非常强大，它解释了为什么大模型对我们程序员或者说我们这个行业的冲击是最大的。大模型的冲击并不在于我们现在讨论的各种应用，而是在于它改变了传统软件开发中的角色。在传统软件中，生成式模型和验收模型都是由程序员和测试人员来完成的。但现在，生成式模型和验收模型都变成了机器模型，这就改变了程序员的位置。</p><p></p><p>如果你有一个从需求到代码，再到测试的映射过程，并且这些过程都能够进行自洽性验证，那么你可以在多种语言、不同的编译环境、测试环境和运营环境中形成多次约束，确保整个语义系统的正确性。这就是我们所说的闭环，而这个闭环在我们的领域是第一个形成的，它是一个非常强的语义闭环。因为它所施加的约束是严格的：代码要么无法运行，要么运行结果与预期验证结果不一致，这些都是容易出现的错误。在大模型出现之前，我们已经能够进行许多自动化测试，包括半自动化和半智能的测试。而大模型的出现使得测试过程变得更加迅速。拥有了这种能力之后，我们需要重新审视程序员的价值所在。如果产品和用户能够利用生成的文档来完全验证开发过程，那么在许多情况下，程序员的角色可能变得多余。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b1/b1642abddeb2ccea4c07af016b7f9398.png" /></p><p></p><p>从今天的角度来看，我去年写的幻灯片其实已经完全成为事实了。图灵测试已经通过，人类的简单意图可以在代码层面上做到无损，而且成本低了很多，提高了好几个数量级的速度。</p><p></p><p>对于程序员来说，我们正处于不同的时代轮换中。在大模型时代到来时，它应该成为主要的生产力。就像马车时代马的重要性一样，在汽车时代汽车成为主要生产力，在大模型时代，大模型就是为该场景提供主要生产力。这意味着，并不是说大模型要取代所有人，而是如果一个人加上一个模型就比三个人快，那就已经很好了。就像汽车和马车一样，马在当时也是非常重要的生产力。我还记得之前举过的例子，直到现代社会发明蒸汽机之前，马一直具有很高的价值。在不同的社会中，马的价值是人的价值的三倍。如果某样东西的价值是你的两倍，那么它必然会取代你。这是一个非常有意义的观点。</p><p></p><p><img src="https://static001.geekbang.org/infoq/76/762e53bd42d4e72e4464fe4f4a8e10db.png" /></p><p></p><p></p><h1>大模型会如何影响软件研发</h1><p></p><p></p><p>接下来我想谈谈大模型对软件研发的影响。在我的极客时间架构课程和书中，我描述了传统软件研发中架构师的角色，通常被限定在一个相对较小的范围内。实际上，一个真正的架构师应该拥有更广阔的视野，考虑的因素应当包括人员、目标、经济价值、环境、过程控制以及文化等多个方面。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9a/9a991e21b917b1dce81b8b7ef27b0ce3.png" /></p><p></p><p>当我们进一步审视这个问题时，我们可以思考 AI，特别是大模型，对这些架构要素的影响程度。在这些架构要素中，哪一个会受到最大的影响？在进行大规模的架构活动，比如重构时，我们必须为这个活动设定一个明确的目标。这个目标定义了我们努力的方向和期望达成的成果。我们需要明确这个目标是什么，并考虑如何衡量成功与否。</p><p></p><p>人性是一个极其重要的元素，正如韦青老师之前提到的，它在架构中扮演着关键角色。人性涉及到参与者，包括目标用户的人性。这些因素会如何影响你的软件架构呢？此外，软件架构始终是一个成本问题，在任何企业中，成本都是不可忽视的因素，还有当前的计算软件环境。</p><p></p><p>我的评估是，大模型对我们最大的冲击在于成本方面。这种冲击可能会持续今后几年。人们关注大模型并不仅仅是因为它们有趣，而是因为它们能够以更低的成本完成工作。例如，在数据标注任务中，与传统的人工标注相比，使用大模型来生成样本的成本更低，而且生成的样本与人工标注的样本具有相同的价值。这就使得人工标注变得不再必要，大模型的引入成为了一种替代人工的过程。</p><p></p><p><img src="https://static001.geekbang.org/infoq/41/41be6e1947b194d01ef443f9c56b24c1.png" /></p><p></p><p>从能力的角度来看，大模型对传统架构师的影响是客观存在的。我之前已经分享过相关的幻灯片，今天我们再次审视这个问题，即大模型的到来到底对哪些方面产生了冲击。其中，红色标记的部分显示，代码交付这一环节受到的冲击最为严重，这种冲击不仅限于未来，而是已经从现在就开始了。</p><p></p><p>在当前的软件开发环境中，即使是像兼职架构师这样的角色，也需要进行进度沟通等工作。然而，这些工作的价值已经不如以往。因为现在我们可以通过代码自动生成的总结来更准确地了解代码的改动情况，这种自动化的总结比人工编写的总结更为可靠。它能够清晰地展示出具体的代码更改，而不仅仅是描述完成了哪些任务。</p><p></p><p>在整个软件开发过程中，越是偏向个体研发的能力，受到的冲击就越大。换句话说，那些依赖于单兵作战的开发工作，如编写代码、调试等，更容易被自动化工具和智能系统所取代。而越往下，即越偏向于对整个开发场景的控制和管理，这些工作受到的冲击相对较小。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9d/9dbd7cd14011485c07b1bbf23181c9de.png" /></p><p></p><p></p><h1>大模型对软件架构的冲击</h1><p></p><p></p><p>当我们意识到这些冲击必然会发生时，作为架构师或至少是决策者，我们应该采取什么行动呢？我们是与软件架构相关的独立决策者，我们应该考虑做些什么？首先，我们需要思考我们要做什么决定，必须清楚我们到底是为企业创造了什么价值。我们需要分析出我们的价值，并在之前创造的价值不再存在时重新定位。作为架构师，真正的价值创造来自于弥补其他人决策盲区，这一点非常重要。你之所以被认为是架构师，是因为你能看到别人看不到的东西。</p><p></p><p>在大模型时代，我们必须认真思考：大模型时代是否存在决策盲区？这些决策盲区在哪里？大模型的出现是否会导致新的决策盲区？举例来说，钉钉正在研究 AI 模型，这些模型涉及数千个人工智能代理和人类之间的网络配对。例如，他创建了成千上万个这样的配对。在这些配对中，你该如何应对？你在这种情境下如何改变自己在这个新的决策环境中，你能弥补的盲区的价值定位是什么？我认为最重要的一点是，当前的大模型无法回答的问题。</p><p></p><p>大模型的盲区就是架构师创造价值的所在。实际上，关于我在《架构思维，从程序员到 CTO》这本书中提到的每一个架构要素，大模型都无法回答。第一个要素是大模型不知道你要解决什么问题。比如你要开发一个操作系统或做一个 AI 代理，你要解决什么问题是别人无法知晓的，作为架构师必须清楚地知道你的商业目标是什么。同样，人性方面，大模型的引入会改变某些人性机制。我们本来具有怎样的人性？现在突然引入大模型，会影响我们的人性。尽管大模型本身可能不具备人性，但是它的引入会给企业软件开发带来人性挑战。开发人员会怎么想？用户会怎么想？用户可能并不关心软件是由程序员编写还是由大模型生成的，但是企业人员可能会关心。经济价值会发生何种变化？收入和成本会发生巨大变化，因为作为架构师，你组织的架构活动中最大的成本是人力和时间。有了大模型，这两个成本都会发生巨大变化。你应该如何应对这些变化？还有环境和各种过程，我就不一一赘述了，但这些都是大模型无法回答的问题，因为大模型不了解你的工厂、你的企业，或者你的架构活动需要处理什么事情。因此，作为架构师，我们有机会为整个企业注入最佳的相关决策。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/9146c3dbef7dab93428d06dba17cb0e2.png" /></p><p></p><p>大模型对软件架构师的冲击很小，甚至会可能带来更大的市场需求。面对大模型可能带来的盲区，我持乐观的态度。尽管我的判断可能不完全准确，但我个人认为大模型对架构师职位的冲击相对较小，反而可能会带来更多的需求。在过去，一个拥有 100 多名员工的企业可能只需要两到三名架构师。但在将来，架构师的需求可能会增加，尽管与此同时，与架构师合作的程序员数量可能会减少，比如只有 10 到 15 人。这种情况可能导致架构师的角色变得更加普及，不再像以前那样是一个特殊的职业。但这并不意味着架构师的传统职责就消失了。</p><p></p><p></p><h1>架构师在大模型时代的价值定位</h1><p></p><p></p><p>正如我们在之前讨论的中提到的，架构师的职责依然存在，包括确保架构活动有正确的目标、构建符合人性的架构等。这些职责在大模型时代仍然至关重要。</p><p>作为架构师，我们需要思考在这个时代中如何定位自己的工作。我们需要继续遵循那些在大模型时代之前总结的生存法则，这些法则至今仍然基本正确。例如，架构活动必须有一个正确的目标，这是任何架构师工作的基础。同时，我们也需要考虑到构建没有人性考量的架构可能会面临的挑战。在大模型时代，架构师的角色和职责可能会发生变化，但他们的核心价值和重要性仍然不变。在有限的资源下最大化经济价值的重要性。虽然不敢说 100% 肯定，但我对此有很高的信心。毕竟，企业需要盈利和承担支出，我们的目标始终是最大化企业的经济价值。此外，软件架构必须适应环境的需求，这一点不会因为大模型的出现而改变。</p><p></p><p><img src="https://static001.geekbang.org/infoq/9c/9c12a4fd2c2dd9c81bb6be8565477df7.png" /></p><p></p><p>接下来，我想强调的是设定唯一且正确的目标，这是架构师生存法则的第一步。在大模型时代，这一点变得更加重要。我给它打了 5 分，表明其重要性。在大模型时代，目标的精确性至关重要。一旦更改目标，就会产生后验成本。大模型的开发涉及大量的训练成本，尤其是生成大量训练数据的成本，这可能非常高昂，有时甚至超过训练本身的成本。在启动大模型项目之前，必须明确你想要解决的问题和目标，清晰定义目标，并确定衡量成功与否的指标。对这些问题的深入思考和明确定义是架构师最大的贡献。</p><p></p><p><img src="https://static001.geekbang.org/infoq/3c/3c2ac12a8b20f699e6fb59d04ac41c47.png" /></p><p></p><p>在我们会场中，可能有超过一半的人参与了大模型相关的工作，而大约有三分之一的人可能从事与大模型相关的工作。这表明，大模型领域的参与度相对较高。在进行大模型相关工作时，最重要的是必须设定一个清晰、可量化、可观测且可持续优化的目标。这一点不仅适用于大模型，也适用于任何算法密集型的工作。然而，对于大模型来说，这一点尤其重要。我自己在参与大模型项目的过程中发现，为公司创造最大价值的关键在于清晰地定义目标。我回顾过去可能不够明智的决策时，往往发现问题出在目标设定不够精确上。因此，明确目标是最重要的事情。</p><p></p><p>法则 3 是在有限资源下最大化经济价值。在大模型的开发和应用中，成本是一个不可忽视的重要因素。首先，定义目标是至关重要的第一步，因为在大模型的尝试中，成本是非常高昂的。大模型不仅在实施阶段成本不菲，而且在训练样本的准备上也需要大量的投入。此外，训练过程本身的成本也不低。除了训练成本，线上计算成本，也就是进行推理的成本，也会因公司和应用场景的不同而有所差异，但通常来说这也是一个不小的开支。持续运维的成本同样不可小觑，因为随着场景的变化，模型可能会逐渐失效，需要重新训练。在某些情况下，这种情况可能会好一些，但成本仍然是一个需要考虑的因素。</p><p></p><p>作为架构师，在进行架构设计时，还需要考虑迁移成本。如果你的公司之前没有使用大模型，或者使用的是小模型，甚至是完全基于规则和代码的场景，那么向大模型场景的迁移将涉及巨大的成本。迁移成本包括了从旧系统到新系统的转换，可能还包括了人员培训和重新设计架构等方面的投入。</p><p></p><p>随着大模型的到来，它们有潜力带来增量收入，这意味着在考虑成本的同时，我们也需要考虑收入。通过与国内外做大模型的朋友们交流，我发现许多公司在投入大模型时并没有充分考虑其经济价值。他们往往是因为看到其他人在做，就跟风投入，而没有深思熟虑大模型究竟能为企业带来什么具体价值，以及这些价值是如何通过大模型创造的，更不用说如何在商业模式和成本结构中实现这些价值了。很多人在没有想清楚这些问题的情况下就进行了投入，这可能导致作为架构师的你在一个错误的决策中浪费了大量的时间和资源，从而未能创造真正的价值。这是架构师在决策时需要深思熟虑的问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ca/cab1d51e1b9c5f81d40a69077ff7f9de.png" /></p><p></p><p>如果我们作为架构师要考虑投入大模型的决策，我们必须考虑到实施成本、训练成本、计算运维成本以及迁移成本等所有相关成本。在我看来，多数企业应该考虑在大模型上进行投入，因为这些成本随着时间的推移，按照摩尔定律逐渐降低。特别是训练样本的生成成本，由于在许多场景中可以与其他大模型共享，因此训练样本的成本也有可能大幅降低。</p><p></p><p></p><p>从竞争的角度来看，我认为大多数场景下，企业应该对大模型进行投入。原因在于，更大的模型可能对训练样本的数量要求更低，企业可以在较少的数据基础上训练出有效的模型。此外，考虑到潜在的竞争优势，如果你的竞争对手不需要投入大量成本就能实现大模型的价值，而你却因为规模或其他原因无法做到，这将使你处于不利地位。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8c/8c81e34ade6daa914d978e7449bd6775.png" /></p><p></p><p>具体如何实施大模型的投入是一个需要深思的问题。在开始使用大模型时，我们可能会面临诸如“为什么我们需要做这个？”或者“我们的企业规模太小，如何投入大模型？”等问题。在当前的投资环境中，投资者可能对大模型的投入持谨慎态度，这进一步增加了决策的复杂性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/48/48694d5b18e2ea2609b4908dcbabbdac.png" /></p><p></p><p>在这个大模型时代，作为架构师，我们需要思考如何创造价值并确保自己不被时代淘汰。我在《架构思维，从程序员到 CTO》一书中提到了一个重要的概念——原子架构，或者说原子价值单元。意思是我们应该从最基本的价值创造单元出发来考虑大模型的投入。原子价值单元首先应该是可识别的功能。在大模型的背景下，这很容易理解，例如一个用户可交互的代理。用户使用这个功能后，我们能够获得一个精确的反馈值。如果无法度量出这样的反馈值，那么这个功能就不是用户可识别的。</p><p></p><p>最重要的是，我们需要明确大模型能为企业或项目真正创造什么价值。很多人在大模型项目中工作了很长时间，但仍然没有弄清楚这个问题。我们需要识别出大模型项目中最小的原子价值单元，即那些只能通过大模型才能带来的核心能力。只有找到了这个最基本的价值单元，我们才能确保大模型的投入能够为企业带来实质性的价值。在审视许多声称与大模型相关的应用场景时，如果你仔细研究，会发现它们实际上与大模型并没有太大关联。包括我们之前讨论的一些案例，很多场景并不真正涉及大模型，它们可能是基于传统的小模型，或者是传统的其他类型的模型，如 Transformer 架构等。如果你真正意识到大模型可能成为下一代的生产力，那么关键是要在企业中找到能够利用大模型突破的场景。如果选错了场景，再多的努力也可能是徒劳，因为你可能无法找到正确的目标和反馈链路，从而无法实现能力的提升。首先要做的是清晰地认识到，你的企业能通过大模型创造出什么样的独特能力，以及为什么这种能力只能由大模型带来。如果不是非大模型不可，那么很可能有其他成本更低或训练时间更短的途径。</p><p></p><p>一旦你找到了正确的场景，你可以迅速进行闭环实验。在国内，这是一种很常见的做法。实验不一定非要从头开始自己做，你可以利用现有的 API，比如 OpenAI 的 ChatGPT 或其他模型的 API，先进行尝试，看看是否可行。如果实验结果是肯定的，那么接下来你可以投入到特征工程中，确保你的模型能够最大限度地发挥作用。我们的经验表明，大模型对特征工程特别敏感，因此在这一环节需要做到最好。</p><p>接下来，你需要找到最小的原子价值单元，即在最小的场景中使用模型，以最小化总成本。在大模型中，合规性挑战也是一个需要考虑的因素，尤其是在国际上，对大模型的正确性和合规性的挑战可能会更加严峻，这可能是国内外在大模型应用上的一个重要差异。</p><p></p><p>在当前阶段，进行迁移的成本过高，因此在这个阶段进行迁移并不合理。如果你真的想要实施大模型，最好是在一个全新的场景中开始，而不是尝试将大模型应用到已经存在的场景中。这是因为传统的模型或非模型驱动的场景，如那些基于规则的系统或已经优化很久的小模型，迁移到大模型的成本可能非常高，而且成功的可能性不大。如果你尝试在这些场景中应用大模型，很可能会失败。因此，更明智的做法是寻找一个全新的场景来开发和应用大模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/91/91aa5e342e781dc5ca5b5613c2d4f87e.png" /></p><p></p><p></p><h1>总结</h1><p></p><p></p><p>对于程序员和架构师而言，大模型时代已经到来。作为架构师，我们的任务是弥补大模型的盲区，并确保它们能够在企业中发挥最大的价值。为了实现这一目标，我们需要关注以下三件事情。</p><p></p><p>确保有正确的目标：明确你希望通过大模型实现什么，以及这些目标如何与企业的整体战略相匹配。找出大模型真正能创造的价值：理解大模型的优势和潜力，并确定它们如何为企业带来具体的益处。找到企业应用场景中的最小价值单元：识别出大模型能够带来的最小且最有价值的功能单元，从这个点开始构建和实施。</p><p></p><p></p><p>从这个基础出发，我们可以更有效地将大模型融入到企业的架构实践中，确保它们能够为企业带来真正的价值。这是我自己的思考，并且我们已经在一定程度上实践了这些理念，认为这是将大模型成功引入企业的最佳路径。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/vgBIHuNqf37axDTV5eKs</id>
            <title>李飞飞首次创业：瞄准空间智能；巴菲特股东大会谈AI：与核武器相似；69岁比尔·盖茨被曝主导微软OpenAI联姻 | AI 周报</title>
            <link>https://www.infoq.cn/article/vgBIHuNqf37axDTV5eKs</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/vgBIHuNqf37axDTV5eKs</guid>
            <pubDate></pubDate>
            <updated>Mon, 06 May 2024 01:01:24 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 比尔·盖茨, 人工智能, OpenAI, 微软
<br>
<br>
总结: 比尔·盖茨仍然在幕后主导微软与OpenAI的合作，推动人工智能技术发展。 </div>
                        <hr>
                    
                    <p></p><blockquote>69 岁比尔·盖茨仍是幕后大佬，主导微软 OpenAI 联姻；爆火 AI 硬件 Rabbit R1 翻车，被爆套壳安卓；斯坦福人工智能领袖李飞飞打造“空间智能”初创公司；Sora 大片真相：人工特效参与，被指误导大众；马斯克来华之际，传再解雇特斯拉两名高管……</blockquote><p></p><p></p><p></p><h2>热门资讯</h2><p></p><p></p><p></p><h4>巴菲特谈人工智能：我认为人工智能的发明和核武器相似</h4><p></p><p></p><p>5 月 4 日晚 10 点 15 分开始，堪称全球最著名投资者、被誉为“股神”的九旬老人巴菲特，携 CEO 接班人、伯克希尔非保险业务的负责人 Greg Abel（阿贝尔），以及保险业务负责人 Ajit Jain（贾因）共同回答股东提问。</p><p></p><p>在一位坐在 4 区提问者问及巴菲特对人工智能的观点时，巴菲特先是打趣的回应：我后悔让 4 区的朋友提问了，早知道我就让 2 区的股东提问了。紧接着他回答：我对人工智能一无所知，但这并不意味着这个技术不重要。我们此前发明核武器的时候，已经“让精灵跳出了魔盒”，而且精灵已经回不了盒子了，这是很糟糕的。而我觉得人工智能也是类似的情况。我现在只是希望这只精灵能够做些好事。</p><p></p><p>对于 DeepFake，巴菲特表示：我有过一次让我有些紧张的经历，这是最近不久的事，我在屏幕上看到一个图像：那是我，是我的声音，我的女儿甚至都没能察觉出任何不同，但它传递的信息绝不是出自我。如果我对投资欺诈感兴趣，这将是有史以来增长最快的行业之一，因为它以一种新的方式被启用。当然，AI 也有可能带来好事。</p><p></p><p></p><h4>69 岁比尔·盖茨仍是幕后大佬，主导微软 OpenAI 联姻</h4><p></p><p></p><p>5 月 4 日消息，据 Business Insider 报道，自 2021 年以来表面上几乎完全离开微软的微软创始人比尔·盖茨，实际上一直在幕后默默策划微软的 AI 革命。</p><p></p><p>据微软现任和前任高管透露，比尔·盖茨仍然密切参与微软的运营，包括提供战略建议、评估产品、聘用高管，以及培养微软与 OpenAI 联合创始人兼 CEO 萨姆·奥特曼的重要关系。</p><p></p><p>2023 年初，微软推出了采用与 ChatGPT 相同技术的搜索引擎 Bing，向谷歌等竞争对手发起挑战。正是盖茨在促成 OpenAI 和微软的“姻缘”、启动这项计划中发挥了关键作用。如今风靡科技圈的 agents 和 Copilot 概念，也是盖茨长期设想的 AI 形态。</p><p></p><p>自 2016 年以来，盖茨一直定期与 OpenAI 会面。在微软与 OpenAI 建立合作关系后，OpenAI 的领导者定期在盖茨的华盛顿豪宅里向盖茨做报告，让他了解关键的基准和重大的障碍。</p><p></p><p>2022 年年中，盖茨私下向奥特曼和 OpenAI 提出挑战，要求他们创建一个能够通过大学先修课程生物学考试的模型。</p><p></p><p>同年 8 月，在盖茨家的一次晚宴上，奥特曼和 OpenAI 首次在公司外推出 GPT-4，纳德拉也是宾客之一。当它通过测试时，盖茨非常震惊，称这是“我一生中见过的最令人惊叹的演示”。随后盖茨写了一份详细说明微软应该如何使用 GPT-4 的备忘录。</p><p></p><p>去年秋天 OpenAI 发生夺权事变后，盖茨也积极施以援手。在 OpenAI 董事会突然解雇奥特曼后的几天内，盖茨主动联系了奥特曼，希望在奥特曼重新领导 OpenAI 的谈判中提供支持。</p><p></p><p>可以说，虽然纳德拉可能是为微软 AI 成功代言的“Public Face”，铺就了一条通往 3 万亿美元市值的金砖大道，但一手缔造微软软件帝国的盖茨，仍然一直是关键的幕后人物。</p><p></p><p></p><h4>斯坦福人工智能领袖李飞飞打造“空间智能”初创公司</h4><p></p><p></p><p>消息人士称，著名计算机科学家李飞飞正在建立一家初创公司，该公司利用类似人类对视觉数据的处理，使人工智能能够进行高级推理，这将是该技术的一次飞跃。李飞飞被认为是人工智能领域的先驱，她在最近的种子轮融资中为公司筹集了资金。</p><p></p><p>据悉，投资方包括硅谷风险投资公司 Andreessen Horowitz，以及李飞飞去年作为科学合伙人加入的加拿大公司 Radical Ventures。</p><p></p><p>在介绍这家初创公司时，一位消息人士提到了李飞飞上个月在温哥华 TED 大会上发表的演讲，她在演讲中说，最前沿的研究涉及一种算法，这种算法可以合理地推断出图像和文字在三维环境中的样子，并根据这些预测采取行动，这种算法使用的概念叫做“空间智能”。</p><p></p><p>李飞飞表示：“在空间智能的驱动下，大自然创造了这种看与做的良性循环”。她认为，自己在斯坦福大学的实验室正试图教计算机“如何在三维世界中行动”，例如，利用大型语言模型让机械臂执行任务，如根据口头指令开门和制作三明治。</p><p></p><p></p><h4>余承东职位调整，华为内部权力重新分配？知情人士：仍是华为终端 BG 一把手</h4><p></p><p></p><p>近日华为内部发布人事调整文件，宣布余承东将新担任终端 BG 董事长职位，原华为终端 BG 首席运营官何刚接任华为终端 BG CEO。目前余承东的职位分别是：华为常务董事、终端 BG 董事长、智能汽车解决方案 BU 董事长、智能终端与智能汽车部件 IRB 主任。</p><p></p><p>不少人称这是华为内部权力的重新分配。对此一位华为人士分析表示，此举并不意味着余承东权力范围缩窄。担任终端 BG 董事长意味着余承东仍然是该业务的一把手。此外，华为 IRB（投资评审委员会）部门的存在极为重要，其作用在于决定具体业务项目的投资立项。余承东仍然担任智能终端 IRB 主任，表明其对该业务具有投资决策权。此次余承东卸任，可能是要把更多重心放在汽车业务的信号。</p><p></p><p>去年 9 月华为车 BU 明确了要在 2025 年实现盈利的目标，当时华为车 BU 内部呈现明显的分工，余承东主要负责华为智选车模式，零部件模式、Hi 模式（Huawei Inside）的相关发布则由靳玉志负责。知情人士称，智选车并不属于车 BU，而归属于华为终端。目前，华为车 BU 已呈现了扭亏为盈的曙光。</p><p></p><p></p><h4>曝 990 万元拍周鸿祎迈巴赫仍未付清余款，双方回应：尾款已付</h4><p></p><p></p><p>5 月 3 日消息，周鸿祎最近卖车的事热度极高，但结束拍卖之后又受到了巨大关注，近日有消息称“褚会长 990 万拍迈巴赫至今未付清余款”。</p><p></p><p>对此，日前“褚会长”本人发布视频表示正式给大家一个回应，他解释了为何自己会拖延支付尾款，并表示经筹措已完成 990 万支付，目前车辆正在过户中。主要是因为二手车行业这两年进入了寒冬，资金压力大，资金周转难，利润也低，自己此次拍卖周的迈巴赫，本意是为了给二手车行业代言，给行业树立诚信透明的正面形象，推动二手车行业的进一步发展。</p><p></p><p>之后周鸿祎本人也发文透露，自己已经拿到尾款了，请大家放心。周鸿祎表示，接下来择个吉日交车，这件事就算尘埃落定了，还是要回到初心：第一，扣除税费后，以我和褚会长两人的名义全部捐赠王选基金会。第二，卖掉迈巴赫是为了证明国产新能源将成为企业家和消费者的第一选择。周鸿祎称，从下周开始会继续试驾各车厂送来的车，把真实情况反馈给大家，作为大家选车的参考。</p><p></p><p>此前，在 4 月 28 日举办的周鸿祎二手迈巴赫拍卖会上，其个人的二手迈巴赫最终以 990 万元被拍下，起拍价为 600 元。据悉，拍卖成功后，周鸿祎 990 万拍卖所得款和当晚直播间打赏所得将一分不留，扣税后全额捐赠。</p><p></p><p>对于这个最终成交价，周鸿祎表示：“我都懵了，原本以为卖到 100 万就差不多了。”值得一提的是，周鸿祎在公布拍卖规则时还提出，最后拍下车的人，未来三年每年将获得一次与自己吃饭见面的机会，有什么关于创业生意上的困惑问题都可以充分交流。</p><p></p><p></p><h4>苹果公司发布第 2 财季财报之际，挖走谷歌员工，组建人工智能团队</h4><p></p><p></p><p>5 月 3 日消息，苹果公司发布了 2024 财年第 2 财季（2024 年第 1 季度）财报，营收 908 亿美元，而去年同期营收为 948 亿美元，同比下降 4%；大中华区营收 163.7 亿美元，同比下降 8.1%。</p><p></p><p>苹果董事会授权额外 1100 亿美元用于股票回购，并宣布将股息从每股 0.24 美元增加到每股 0.25 美元。股息将于 5 月 16 日支付给截至 5 月 13 日在册的股东，创下该公司史上的最高纪录，并较上次授权的 900 亿美元回购额提升了 22%，超出市场预期。因为营收下滑优于市场预期且启动史上最大回购，当日股价盘后涨 7%。</p><p></p><p>4 月 30 日消息，据《金融时报》报道，苹果公司从谷歌挖走了数十名人工智能专家，并在瑞士苏黎世建立了一个“神秘的欧洲实验室”，以组建一支新的团队，负责研发人工智能模型和产品。</p><p></p><p>根据《金融时报》对 LinkedIn 个人资料的分析，自 2018 年苹果挖来 John Giannandrea（约翰・詹南德里亚）担任其首席人工智能执行官以来，该公司已经招募了至少 36 位谷歌人工智能专家。</p><p></p><p>据了解，苹果的主要人工智能团队位于加州和西雅图，但该公司最近扩大了位于瑞士苏黎世的专注于人工智能工作的办公室。有传言指出，苹果收购当地的人工智能初创公司 FaceShift（VR）和 Fashwell（图像识别）促使该公司决定在苏黎世建立名为“视觉实验室”的保密研究实验室。</p><p></p><p></p><h4>传马斯克再解雇特斯拉两名高管，员工信直指企业“躺平”文化</h4><p></p><p></p><p>据硅谷科技媒体报道，埃隆·马斯克在给公司高级经理的电子邮件中称，他已经解雇了两名特斯拉的高级管理人员，并计划再裁员数百人，因为公司销售额持续萎靡，并且先前过慢的裁员步伐令其不满。</p><p></p><p>报道称，最新被开除的两位高管分别为超级充电（Supercharger）业务的高级主管 Rebecca Tinucci（丽贝卡·蒂努奇）和新车项目负责人 Daniel Ho（丹尼尔·霍），两人于当地时间周二上午离职。</p><p></p><p>马斯克还计划解雇 Tinucci 和 Ho 的所有下属员工，其中仅超级充电业务就有约 500 人。报道还提到，前高管 Rohan Patel（罗汉·帕特尔）领导的特斯拉公共政策和业务发展团队也将被解散。</p><p></p><p>据称马斯克在电子邮件中写道，“希望这些行动表明，我们需要在员工人数和成本削减方面保持绝对的硬核（hardcore）。虽然一些高管人员正在认真对待，但大多数人还没有这样做。”</p><p></p><p>硬核原本指在上世纪七十年代从朋克分化出的较激烈、较极端的一个分支。马斯克在此处指的是“奋斗”精神以及高强度的工作状态，有些类似于“狼性”，与“躺平”“佛系”网络热词相对立。</p><p></p><p></p><h4>FSD 何时入华？特斯拉：具体时间不好说，预估是快了。外交部回应</h4><p></p><p></p><p>特斯拉 CEO 马斯克“旋风”访华，引发市场一片猜测。针对业内关注的 FSD 何时会在国内落地，特斯拉中国方面回应称，“目前具体时间不好说，但预估是快了。”</p><p></p><p>对此，外交部发言人林剑表示，中国国务院总理李强会见了马斯克，中国有关机构也同马斯克进行了相关的会谈。中方已经发布了消息稿。“作为原则，我愿再次重申，中国致力于高质量发展，坚定奉行合作共赢的开放战略，持续建设市场化、法治化、国际化的一流营商环境，积极促进外商投资，高度重视外资企业的相关诉求，切实保护外资企业的合法权益。我们对内外资企业一视同仁、平等相待，欢迎外资企业继续深化对华合作，共享中国经济发展成果，也希望外资企业遵守中国的法律法规，履行安全承诺。”林剑称。</p><p></p><p>据外媒引援两位知情人士透露的消息，特斯拉已经与中国科技巨头百度达成了合作，后者将允许特斯拉获得其在中国公共道路上收集数据的地图牌照。</p><p></p><p>据悉，作为双方协议的一部分，百度还将向特斯拉提供其车道级导航系统。消息人士称，这笔交易为特斯拉的驾驶辅助系统（特斯拉称为全自动驾驶套件 FSD）在中国的推出扫清了最后的监管障碍。</p><p></p><p>在中国，所有智能驾驶系统都必须获得测绘资质，才能在公共道路上运行。外国公司需要与获得许可证的国内公司合作。而百度就是获得测绘资质的十几家公司之一。</p><p></p><p>如果拥有地图服务许可证后，特斯拉将可以被允许在中国道路上合法运行其 FSD 软件，其车队也可以收集车辆周围环境的数据，如道路布局、交通标志和附近建筑物等。不过，目前尚不清楚收集到的数据是属于特斯拉还是百度。</p><p></p><p></p><h4>小红书通报一员工向男友泄露机密被辞退，男友：小红书 CEO 曾邀我加盟</h4><p></p><p></p><p>4 月 29 日消息，根据网传一张小红书内部廉政通报截图显示，小红书社区部 / 产品部 / 内容产品组员工杨某因在职期间存在利益冲突未申报且应他人要求多次泄露公司商业秘密，被给予辞退、永不录用，并在公司全员实名通报处罚，并列入行业黑名单。据悉，入职小红书后，杨某多次应其男友刘某的要求，通过个人微信、飞书文档、共享企微账号等途径向其泄露小红书核心商业机密，泄露的信息中包含大量小红书业务月报、战略目标文档等核心商业机密。</p><p></p><p><img src="https://static001.geekbang.org/infoq/a4/a4bbaba7095287817d12d7985c3458da.jpeg" /></p><p></p><p>通报称，该员工杨某入职小红书前，就职于某互联网公司。在上家公司任职期间，与该公司副总裁刘某建立恋爱关系。2023 年 1 月 30 日，杨某入职小红书，其在接受了公司的阳光申报培训，且明知该公司与小红书互为竞对关系的情况下，仍未向公司说明其与刘某的利益冲突关系。通报中的杨某男友刘某疑似是 B 站一员工，他在朋友圈发文回应此事称，过去大半年自己已淡出 B 站一线管理工作，并且计划留任至五一后离开。</p><p></p><p>他透露，今年春节前，小红书毛总曾邀请自己加入小红书，还主动提到知道自己女友（杨某）也在小红书任职，明确表示不受影响。但因自己尊重行业里的公司，并没有考虑加入小红书。刘某称，在行业里多年，自己与女友相处时非常注意不干扰对方工作，交流仅限于打工人吐槽一下工作。完全谈不上故意和涉及商业机密。刘某称，如果有分歧，欢迎通过法律手段解决，而不是通过现在这样的方式。</p><p></p><p></p><h4>字节跳动内部反腐辞退 61 人，收巨额“好处费”员工被刑拘</h4><p></p><p></p><p>从字节跳动内部人士处获悉，字节跳动企业纪律与职业道德委员会通报近日通报 61 起员工违法违纪案件，涉事员工全部被辞退。其中一名抖音员工利用职务便利，非法收受外部合作商巨额好处费，于 1 月 26 日被公安机关立案侦查并采取刑事拘留强制措施。此外，有多名员工长期存在违规报销、违规打车等行为。比如，一名员工从 2020 年 7 月至 2024 年 1 月，将虚增的发票金额或私人消费以业务招待费用的名义进行报销，侵占公司资产共计 3 万余元。另一名员工从 2020 年 10 月至 2023 年 8 月在上下班通勤期间违规使用企业账户打车，同时其还将团建费用以“拜访客户”、“参加会议”等差旅费名义进行报销，违规使用公司资产共计 3 万余元。</p><p></p><p>还有多名员工在未满足加班打车报销条件或者未实际拜访客户的情况下，通过外部购买发票、向出租车司机索要发票等形式进行虚假报销，涉案金额几千到上万元不等，公司已责令其退回全部违规所得。此外，还有一名员工因个人原因未在工区办公，将工卡借给外部人员使用并让其代刷卡，共计 21 个工作日。该外部人员在持有该前员工工卡期间，进入工区内部食堂就餐 14 次，侵占字节跳动公司资产 490 元。</p><p>今年 3 月，字节跳动内部还曾通报了 2023 年全年的反舞弊违规案件。抖音集团共查处舞弊类违规案件 177 起，其中 136 人因触犯廉洁红线被辞退， 23 人因涉嫌违法犯罪被移送司法机关处理。</p><p></p><p></p><h4>最高涨幅达 20%！多家国内芯片厂商相继宣布涨价</h4><p></p><p></p><p>4 月 29 日消息，据媒体报道，自去年四季度半导体市场需求开始回暖，近期已有多家国产芯片厂商开始相继宣布涨价，最高涨幅达到了 20%。浙江亚芯微电子有限公司于 4 月 23 日向客户发出“调价通知函”，宣布对全系列产品单价上调 15%-20% 不等，立即执行。涨价理由是国际贵金属市场价格近期涨幅平均超过 15%，有的材料最高峰达到 50%，且有持续上涨的趋势，导致原材料成本不断上涨。</p><p></p><p>深圳市创芯微电子股份有限公司也于 4 月 23 日发出“调价通知函”，宣布自 5 月 1 日后将对部分产品价格进行上调。涨价理由同样是由于金属等原材料价格的逐步上涨，半导体行业报价不断上涨，产品成本也在不断上涨。据市场研究机构 TechInsights 预计，2024 年全球半导体销售额将增长 24%。</p><p></p><p></p><h2>IT 业界</h2><p></p><p></p><p></p><h4>爆火 AI 硬件 Rabbit R1 翻车，被爆套壳安卓</h4><p></p><p></p><p>5 月 1 日消息，科技博主 Mishaal Rahman 发文称，Rabbit R1 内部运行 Android 系统，其整个界面都由安卓应用提供支持。</p><p></p><p>几个月前，Humane、Rabbit 两家初创公司陆续推出他们的人工智能设备 ——售价 700 美元的 Ai Pin 和售价 199 美元的 Rabbit R1。最初，一些人认为这些设备将开创可穿戴人工智能的新时代。然而，几个月过去了，对于这两款设备的争议逐渐增多。但争议不妨碍 R1 热卖。</p><p></p><p>本周二，Rahman 曝光了知名生成式 AI 硬件 Rabbit R1 是一个安卓应用程序，立即引来了科技圈的关注，毕竟年初该公司 CEO 吕骋高调宣传搭载的是全新操作系统 Rabbit OS，希望“摆脱当前智能手机使用的操作系统”。</p><p></p><p><img src="https://static001.geekbang.org/infoq/08/084e38193dacb62a4aa3bbf27cf51436.jpeg" /></p><p></p><p>同时，他还表示他们团队修改了 Rabbit R1 的 launcher APK，从而可以将其直接转换为安卓应用程序，甚至可以在谷歌 Pixel 6a 手机上运行。如今，甚至有用户已经在 Rabbit R1 运行安卓 App 了。</p><p></p><p><img src="https://static001.geekbang.org/infoq/1b/1b85d7810f67ae4b2b5a11f687488697.jpeg" /></p><p></p><p>所以简单来说，虽然 R1 上的操作系统可以是一个定制化安卓系统，甚至是一个定制化的 Android ROM，但其实只是包含在一个 .apk 文件中的 App，且可以在其他地方安装。</p><p></p><p>在 GitHub 上，还有人表示收到了泄露的代码，声称该设备只是运行几个自动化脚本：“这就是为什么他们只支持四个应用程序：Spotify、Midjourney、Doordash 和 UberEats。”“更糟糕的是，他们将用户会话存储在自己的计算机上，而没有任何额外的安全层。这既是对用户隐私的公然无视，也是一种极其糟糕的工程实践。”“可悲的是，对于任何了解该团队的人来说，都不应该感到震惊。毕竟两年前他们还在兜售 NFT。”</p><p></p><p><img src="https://static001.geekbang.org/infoq/b4/b4c5c322566ec711eda7d08aa919845b.jpeg" /></p><p></p><p>周三，吕骋发布了一份声明，辩称 R1 的界面并非一个安卓应用，并解释说他们调用了运行在云端的大模型（这一点实际也没有人质疑过）。</p><p></p><p>“Rabbit OS 和 LAM 运行在云端，并使用了经过定制的 AOSP 和底层固件修改。Rabbit OS 是专为 R1 设备定制的，我们不支持第三方客户端。使用盗版 APK 或网页客户端会带来重大风险；黑客可能会窃取你的数据。因此，我们建议不要使用这些盗版 Rabbit OS 应用。在今天 OTA 之后，我们实施了多项云验证改进来验证设备 / 客户端请求。”几个小时后，Rahman 在推特上表示，他的 Pixel 6 版本的 Rabbit 连接不上了，这似乎证实了吕骋关于新用户验证要求的说法。</p><p></p><p></p><h4>Sora 大片真相：人工特效参与，被指误导大众</h4><p></p><p></p><p>Sora 火爆短片《气球人》的背后艺术家团队揭秘，视频画面并非完全 AI 生成，大量视觉效果需人工后期实现。他们描述角色、处理不一致性问题，尽管提升视频素材质量，却面临 AI 理解摄影术语有限等挑战。团队认为 Sora 仍有进步空间，对工作流是补充，但也存在被误导性营销掩盖人工价值的问题。</p><p></p><p></p><h4>Claude 推出苹果 iOS 版 App，最新模型号称超越 GPT-4</h4><p></p><p></p><p>5 月 1 日，人工智能创业公司 Anthropic 首次推出了旗下大模型产品 Claude 的移动端 App，目前仅有 iOS 版。而且相比于 ChatGPT、Gemini 等竞争对手，Claude 的进度显得慢了好几拍。</p><p></p><p>Anthropic 公司的产品经理 Scott White（斯科特・怀特）说，许多用户一直通过移动设备访问 Claude 模型，促使 Anthropic 公司推出了 Claude.ai 的应用版本。据介绍，该应用除了提供聊天机器人等基础功能之外，还支持上传照片、分析图像。在此之前，Claude 只能通过网站或第三方模型库来使用。</p><p></p><p>Claude 应用程序将免费提供给 Claude 人工智能模型的所有用户，包括免费用户、Claude Pro 用户和新的 Claude Team 计划。</p><p></p><p>至于同期新推出的“Claude Team”计划，官方表示，该计划可为一个至少 5 人的团队提供 Claude 访问权限，每人每月 30 美元。</p><p></p><p></p><h4>小红书内测自研大模型“小地瓜”，回应 200 亿美元融资传言</h4><p></p><p></p><p>据媒体报道，多个独立信源透露，由小红书 AI 创新负责人张德兵（薯名：宇尘）牵头的大模型团队，在部分内部产品灰度测试自研通用大模型基座“小地瓜”。小红书 AI 产品的探索，则主要由小红书产品和设计负责人邓超（薯名：樱木）负责。</p><p></p><p>据此前报道，小红书成立了大模型团队。2023 年 10 月，在小红书 REDtech 青年技术沙龙上的分享中，张德兵第一次较为公开地提及了小红书大模型的两个落地方向：多模态技术，以及 AI 内容创作工具。</p><p></p><p>几名知情者表示，小红书对上线新 AI 产品的态度非常谨慎，原因是害怕破坏现有的内容生态。一名知情者说：“小红书的内容生态是活人种草，AI 在里面怎么样才不违和，这是困扰小红书的一个问题。”</p><p></p><p>4 月 29 日，有市场消息称，小红书正在进行新一轮估值 200 亿美元的融资，这将是小红书 Pre-IPO 轮融资，对未来的 IPO 有定价参考价值。对此，小红书回应称，该消息为不实信息。</p><p></p><p>此前还有消息称，小红书在 2023 年实现了历史性突破，首次实现盈利。据四位未公开的知情人士透露，小红书去年净利润达 5 亿美元（当前约 36.35 亿元人民币），营收达 37 亿美元（当前约 268.99 亿元人民币）。</p><p></p><p>小红书官网显示，该公司获得了阿里巴巴、腾讯、纪源资本和红杉资本等大牌企业机构投资。该公司由毛文超和瞿芳于 2013 年在上海创立，截至 2019 年 10 月，小红书月活跃用户数已经过亿，其中 70% 用户是 90 后。</p><p></p><p></p><h4>OpenAI 即将推出 ChatGPT 版搜索引擎；奥特曼：保持迭代部署很重要</h4><p></p><p></p><p>消息称，OpenAI 有望今年 5 月 9 日推出基于 ChatGPT 的全新搜索产品，进一步挑战谷歌的传统搜索巨头地位。日前，search.chatgpt.com 域名和相关的 SSL 证书已经被创建，Reddit 网友发布推文称该域名将于 5 月 9 日上线。</p><p></p><p>OpenAI 首席执行官 Sam Altman 曾在今年 3 月公开表态，对于在搜索引擎击败谷歌并不感兴趣。而 The Information 几个月前报道称，OpenAI 一直在开发一款部分由 Bing 提供支持的网络搜索产品，但目前尚不清楚具体细节。</p><p></p><p>5 月 2 日消息，OpenAI 对免费及 Plus 订阅用户全面开放历史聊天功能。在此之前，那些选择不贡献对话数据以训练模型的用户无法查看自己的聊天历史。然而，随着此次更新的推出，这一限制已被取消。</p><p></p><p>此前，为了启用该功能，用户需要同意 OpenAI 使用其聊天记录进行 AI 训练。现在，一旦开启历史聊天功能，用户便能轻松地从上一次对话的断点处继续交流，ChatGPT 将自动记住之前的对话内容，并据此进行智能回复。同时，OpenAI 也提供了不保存历史记录的一次性聊天选项。用户可根据自身需求选择是否保存聊天内容，灵活掌控个人信息的保留与分享。目前，这项更新已在 ChatGPT 的网页版上线，并即将在移动端版本中推出。</p><p></p><p>另外，近日，OpenAI 联合创始人兼首席执行官 Sam Altman（萨姆·奥尔特曼）在斯坦福大学发表演讲，提及 AGI（通用人工智能）发展、OpenAI 的迭代节奏等。有消息称，这场演讲在英伟达礼堂进行，超一千人在门口排队，热度颇高。“GPT-5 会更加智能，这将是历史上最令人瞩目的事件之一。”</p><p></p><p>在一段流出的视频中，奥尔特曼表示：“以高度的科学确定性来说，GPT-5 将比 GPT-4 智能很多，GPT-6 将比 GPT-5 智能很多，而我们远未触及极限。”此前 OpenAI 已推出 GPT-3.5 和 GPT-4。对于 OpenAI 的产品迭代，奥尔特曼认为尽早且频繁推出 AI 产品，“保持迭代部署非常重要，即便现在看来 ChatGPT 还有点令人尴尬，GPT-4 还显得愚蠢。要让社会为技术进步做好准备，依赖于迭代部署。”</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/6pkAgDI2fwxUu51icDpN</id>
            <title>AWS Batch为大规模模拟引入了多容器作业</title>
            <link>https://www.infoq.cn/article/6pkAgDI2fwxUu51icDpN</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/6pkAgDI2fwxUu51icDpN</guid>
            <pubDate></pubDate>
            <updated>Fri, 03 May 2024 00:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: AWS Batch, 多容器作业, 简化开发, 批处理管理
<br>
<br>
总结: AWS最近宣布支持AWS Batch中的多容器作业，这一新功能简化了运行模拟的过程，特别是在测试复杂系统时。多容器作业减少了作业准备所需的工作量，并且无需使用自定义工具来集成多个团队的工作，从而加快了开发时间。通过在一个作业中运行多个容器，开发人员现在可以使用较小的模块化容器来表示不同的系统组件。AWS Batch是一组批处理管理功能，可帮助开发人员、科学家和工程师在云上运行批处理计算作业。 </div>
                        <hr>
                    
                    <p>最近，AWS 宣布通过管理控制台支持 AWS Batch 中的多容器作业（Multi-Container Jobs）。这一新功能简化了运行模拟的过程，特别是在测试复杂系统（例如汽车自动驾驶和机器人等系统）时。</p><p></p><p>根据云供应商的说法，多容器作业减少了作业准备所需的工作量，并且无需使用自定义工具来集成多个团队的工作，从而加快了开发时间。AWS 首席布道师（EMEA）Danilo Poccia写道：</p><p></p><p></p><blockquote>传统上，AWS Batch 只允许单容器作业，并需要额外的步骤来将所有的组件合并到一个单体（monolithic）容器中。它也还不允许使用单独的“边车”（sidecar）容器（边车容器是通过提供数据日志等附加服务来补充主应用程序的辅助容器）。这项额外的工作需要跨多个团队进行协调（……），因为任何代码的更改都意味着要重建整个容器。</blockquote><p></p><p></p><p>AWS Batch 是一组批处理管理功能，可根据提交的批处理作业的数量和特定资源动态配置所需的计算资源数量和类型。该服务可帮助开发人员、科学家和工程师在云上运行批处理计算作业。</p><p></p><p>根据 AWS 的说法，这项新功能使得在汽车自动驾驶和机器人等领域运行大规模模拟变得更容易，这些工作负载通常划分模拟本身和与模拟交互的被测系统。IPG Automotive、MORAI 和 Robotec.ai 是已经在运行多容器作业的 AWS 客户之一。Poccia 补充道：</p><p></p><p></p><blockquote>使用多容器作业可以减少作业准备所需的工作，并且无需使用自定义工具来将多个团队的工作合并到单个容器中，从而加快了开发时间。它还通过定义明确的组件职责来简化 DevOps，使团队能够快速识别和解决自己专业领域的问题，而不会分心。</blockquote><p></p><p></p><p>通过在一个作业中运行多个容器，在执行批处理作业之前不再需要将系统重新构建为单体容器。开发人员现在可以使用 AWS 管理控制台、CLI 或 SDK 来定义多个较小的模块化容器，以表示不同的系统组件。</p><p></p><p>AWS 并不是唯一一家提供批处理管理功能的云供应商：微软提供了 Azure Batch，该服务可帮助开发人员跨可扩展的虚拟机（VM）集合来管理计算密集型的工作；Batch 是谷歌云（Google Cloud）的托管服务，用于调度、排队和执行批处理工作负载。但是，目前两者都不支持多容器作业。</p><p></p><p>这一新功能可在任何提供 AWS Batch 的区域使用，并且使用 AWS Batch 或多容器作业无需支付额外的费用。</p><p></p><p>原文链接：<a href="https://www.infoq.com/news/2024/04/aws-batch-multi-container-jobs/">https://www.infoq.com/news/2024/04/aws-batch-multi-container-jobs/</a>"</p><p></p><p>声明：本文为 InfoQ 翻译整理，未经许可禁止转载。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/lvmo0NSQ0VSKgD4yJTo0</id>
            <title>谷歌、OpenAI 都搞起了AI “造人”？创始团队：开源AI基因编辑器只是冰山一角</title>
            <link>https://www.infoq.cn/article/lvmo0NSQ0VSKgD4yJTo0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/lvmo0NSQ0VSKgD4yJTo0</guid>
            <pubDate></pubDate>
            <updated>Thu, 02 May 2024 10:30:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 译者, 策划, AI, 基因编辑器
<br>
<br>
总结: 人工智能系统开发了精确编辑人类 DNA 的开源工具，Profluent 公司使用人工智能算法设计了高功能基因组编辑器 OpenCRISPR-1，并将其开源，这一技术代表了人工智能驱动生物设计领域的重大飞跃。 </div>
                        <hr>
                    
                    <p>译者 | 王强、华卫</p><p>策划 | 华卫</p><p></p><p>“AI 能编辑 DNA 了，还是开源版！”</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8a076216c7121b634029e7fda63273c0.png" /></p><p></p><p>OpenCRISPR-1 开源链接：</p><p>https://github.com/Profluent-AI/OpenCRISPR</p><p></p><p>今天，人工智能系统不止可以设计出创作诗歌、代码和视频的模型，还开发出了精确编辑人类 DNA 的开源工具。这不仅是 AI 的巨大进步，还预示着，将来科学家可以比现在更精确、快速地对抗各种疾病。</p><p></p><p>近日，美国一家名为 Profluent 的初创公司公开介绍了这项技术，并预计于下个月在美国基因和细胞治疗学会年会上发表相关论文。“使用人工智能技术创建基因编辑机制史无前例，”美国加州大学旧金山分校生物工程和治疗科学系教授兼系主任 James Fraser 表示。</p><p></p><p>据悉，Profluent 是在分析了大量生物数据后，通过对 CRISPR-Cas 序列进行人工智能算法建模来设计出高功能基因组编辑器，并将其命名为 OpenCRISPR-1。通过 OpenCRISPR-1，该公司的人工智能系统从大规模序列和生物学背景中学习，产生了数百万种自然界中不存在的 CRISPR 样蛋白，从而成倍扩大了几乎所有已知的 CRISPR 家族。</p><p></p><p>并且，OpenCRISPR-1 基因编辑器正在被开源。这意味着，其允许个人、学术实验室和公司免费试用该工具。很多研究人员都会把开发的人工智能底层驱动软件开源出来，让其他人可以在他们的成果基础上继续开发工作，以加速新技术的开发步伐，但像 OpenCRISPR-1 这类生物实验室和制药公司开源技术发明的情况并不常见。不过，Profluent 并没有开源该编辑器本身的技术内容。</p><p></p><p>Profluent 还透露，OpenCRISPR-1 只是冰山一角，他们的平台能够随意生成更多的基因编辑系统。然而，尽管目前 OpenCRISPR-1 还没有投入临床，但已经招致不少除应用效果以外的担忧。</p><p></p><p></p><h1>完全由 LLM 驱动</h1><p></p><p></p><h1>将蛋白质多样性扩大 4.8 倍</h1><p></p><p></p><p>在这项研究中，Profluent 展示了世界上第一个使用人工智能从头开始设计的分子的精确基因编辑。基因编辑器是复杂的系统，需要多结构域蛋白质、DNA 和 RNA 之间复杂的空间和时间相互作用。使用人工智能设计功能差异化的基因编辑器，代表了人工智能驱动生物设计蓬勃发展领域的重大飞跃。</p><p></p><p>OpenCRISPR-1 的技术是由人工智能驱动、Cas9 样蛋白和指导 RNA 组成，完全使用 &nbsp;Profluent 的大型语言模型（LLM）开发。该模型学习的是氨基酸和核酸序列，这些化合物定义了科学家用来编辑基因的微观生物机制。也就是说，它分析了从自然界中提取的 CRISPR 基因编辑器的行为，并学习该如何生成全新的基因编辑器。</p><p></p><p>“这些人工智能模型从序列中学习，无论这些序列是字符、单词、计算机代码还是氨基酸序列。”Profluent 首席执行官 Ali Madani 表示。</p><p></p><p>据介绍，生成蛋白质语言模型通常在跨越广泛功能的大型、多样化的天然蛋白质序列数据集上进行预训练，可以生成反映天然蛋白质特性的真实蛋白质序列。然而，对于特定的应用，如产生新的基因编辑器，就需要将模型引导到特定的目标蛋白质家族。</p><p></p><p>为此，Profluent 进行了详尽的数据挖掘，以构建迄今为止最广泛的 CRISPR 系统数据集，被称为 CRISPR-Cas 图谱。为生成新的 CRISPR-Cas 蛋白，他们又在 CRISPR-Cas 图谱上训练了一个蛋白质语言模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/ff/ffa69783368c36158e23010629f0650e.png" /></p><p></p><p>图：生成的序列极大地扩展了 CRISPR 相关蛋白质家族的多样性，以蛋白质簇的数量来衡量，图中显示了每个蛋白质家族在不同类型的 CRISPR-Cas 系统中被发现的频率。</p><p></p><p>从该模型中生成了 400 万个序列，并使用生物信息学技术来去除简并序列，确定每个生成的蛋白质属于哪个 CRISPR-Cas 家族后，他们发现，这些模型产生的蛋白质将几乎所有天然存在的 CRISPR-Cas 家族的多样性扩大了 4.8 倍，并且之后可以生成更多的序列进一步扩大这种多样性。</p><p></p><p>鉴于 SpCas9 的广泛采用和临床成功，其使用模型生成了可与 SpCas9 互操作的 Cas9 样蛋白，并选择了其中 48 个生成的序列，用于在人类细胞中进行严格的功能表征。他们发现，当与脱氨酶配对时，OpenCRISPR-1 和 SpCas9 &nbsp;在精确编辑靶基因组中的单个碱基时具有相似的活性和特异性。此外，他们还能够保持碱基编辑活性，同时使用由另一种 Profluent 训练的蛋白质语言模型生成的脱氨酶来提高特异性。</p><p></p><p><img src="https://static001.geekbang.org/infoq/af/af8332d417b7cda2c7cc1361e1fdbd64.png" /></p><p></p><p>图：对于测试的 5 种生成的核酸酶中的 4 种，使用模型生成的 sgRNA 提高了编辑效率。</p><p></p><p>最后，为了进一步优化生成的核酸酶活性， Profluent 还训练了一个模型来为任何给定的 Cas9 样蛋白生成相容的 sgRNA。与 SpCas9 的 sgRNA 相比，这些生成的 sgRNA 可以提高所测试的五种蛋白质中四种产生的核酸酶的活性。</p><p></p><p></p><h1>CRISPR 基因疗法的“升级版”</h1><p></p><p></p><p>“我们与 OpenCRISPR 的意图是与尖端研究机构和药物开发人员合作，以一种强大而实用的方式安全地加速 CRISPR 基因疗法的开发。”Profluent 首席商务官 HilaryEaton 表示。</p><p></p><p>目前，基于 CRISPR 的技术已经改变了科学家研究和对抗疾病的方式，并提供了能够改变镰状细胞性贫血和失明等遗传疾病患者的治疗方法，但仍需加速发展以治疗数千种其他还无治愈之法的疾病。据介绍，OpenCRISPR-1 正是基于 CRISPR 的生物机制所构建。</p><p></p><p>源自微生物的基于 CRISPR 的基因编辑器虽然功能强大，但当移植到非天然环境（如人类细胞）中时，通常会显示出显着的功能权衡，人们希望能够生产出比经过数十亿年进化而来的天然基因编辑器更灵活、强大的基因编辑器。人工智能系统的设计恰恰能提供一种强大的替代方案，有可能绕过进化约束生成具有最佳属性的编辑器。</p><p></p><p>“我梦想着这样一个世界，我们可以在几周内按需提供 CRISPR。”美国加州大学伯克利分校创新基因组学研究所的基因编辑先驱兼科学主任 Fyodor Urnov 说。</p><p></p><p>事实上，OpenCRISPR-1 是整个业界努力构建可以改善医疗保健的人工智能技术的一个缩影。例如，华盛顿大学的科学家正在利用 ChatGPT 和 Midjourney 等图像生成器背后所采用的人工智能技术方法来组装全新的蛋白质，并致力于加速新疫苗和药物的开发。</p><p></p><p>“从长远来看，这可以通向一个快速为个人定制药物和治疗方法的时代，定制速度甚至比我们现在的还快。”Urnov 认为，生成式人工智能系统具有巨大的潜力，它们往往会通过从越来越多数据中学习的过程来快速改进自身。如果像 Profluent 这样的技术继续改进，其最终可以让科学家以更精确的方式编辑基因。</p><p></p><p>而目前看来， Profluent 也具备技术进化的资金支撑。3 月 21 日，Profluent 宣布完成 3500 万美元追加融资，融资总额达到 4400 万美元。这笔融资由 Spark Capital 领投，现有投资者 Insight Partners 和 Air Street Capital 以及来自 OpenAI、Salesforce、Octant Bio 和谷歌（包括谷歌 DeepMind 首席科学家 Jeff Dean）的天使投资人组成的财团也参与了投资。该公司此前还曾从 Insight Partners、Air Street Capital、AIX Ventures 和 Convergent Ventures 募集到 900 万美元种子轮资金。</p><p></p><p></p><h1>临床可能引发副作用</h1><p></p><p></p><p>虽然这项研究已经表明，人工智能模型可以生成能够编辑人类基因组的工具。但目前 Profluent 还没有对基因编辑器 OpenCRISPR-1 进行临床试验，因此尚不清楚其是否能达到或超过 CRISPR 的性能表现。</p><p>不过，可以确定的是，短期内这一技术进展不太可能影响医疗保健领域。Urnov 表示，事实上科学家们并不缺乏可以用来对抗疾病的天然基因编辑器，推动这些编辑器通过临床前研究的成本才是瓶颈所在，如安全性、制造和监管审查，经过这些步骤后才能将其用于治疗患者。</p><p></p><p>此外，这样的合成基因编辑器案例还引发了其他担忧。长期以来，科学家一直警告不要将 CRISPR 用于人类身体改造和治疗领域，因为这项技术相对较新，可能会产生引发癌症等不良副作用，还能提供一些不道德的用途，如对人类胚胎进行基因改造。</p><p></p><p>对此，Fraser 的看法是，“一个不道德的人，并不在乎他们使用的基因编辑器是不是人工智能创建的，他们只会继续使用现有工具。”</p><p></p><p>原文链接：</p><p>https://www.nytimes.com/2024/04/22/technology/generative-ai-gene-editing-crispr.html</p><p>https://www.biorxiv.org/content/10.1101/2024.04.22.590591v1</p><p>https://www.businesswire.com/news/home/20240422399482/en/Profluent-Successfully-Edits-Human-Genome-with-OpenCRISPR-1-the-World%E2%80%99s-First-AI-Created-and-Open-Source-Gene-Editor</p><p>https://www.profluent.bio/blog/editing-the-human-genome-with-ai</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/tR5ZZzt9t6MH1IOfOTsj</id>
            <title>强大到不敢给普通人用！史诗级大模型Sora如何让众行业一夜变天？</title>
            <link>https://www.infoq.cn/article/tR5ZZzt9t6MH1IOfOTsj</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/tR5ZZzt9t6MH1IOfOTsj</guid>
            <pubDate></pubDate>
            <updated>Wed, 01 May 2024 06:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 视频生成模型, Sora, 技术整合, DiT结构
<br>
<br>
总结: 2024年2月，OpenAI发布了视频生成模型Sora，其利用DiT结构取得了明显优势，成为全球关注焦点。Sora不仅能生成更长时间的视频，还能保持视频的连贯性和表现效果。其在影视行业中有望降低制作成本，提高制作效率。 </div>
                        <hr>
                    
                    <p></p><h2>视频生成模型“新王登基”，Sora 何以成为全球焦点？</h2><p></p><p>&nbsp;</p><p>2023 年以来，多模态视频生成技术取得了显著的进展和突破，从 Runway 到 Pika 再到年末的 VideoPoet，视频生成模型进入到加速阶段。2024 年 2 月，OpenAI 旗下视频生成模型 Sora 正式对外发布。Sora 一名源于日文“空”（そら sora），取自天空之意，以示其无限的创造潜力。与 Runway、Pika、VideoPoet 等“前辈”相比，Sora 在视频生成效果和质量上具有明显优势。也正因如此，Sora 一经发布就在全球范围内掀起了讨论热潮，迅速成为当前最受关注的模型之一。</p><p>&nbsp;</p><p>“Sora 的出现时间要比我们预想的要早很多，OpenAI 已经提前带来了惊喜”。WeShop 唯象 GM 吴海波在接受 InfoQ 采访时提到，从技术层面来看，Sora 并没有引入全新的理论框架，而是将现有技术进行了新的整合。自从 Sora 问世以来，人们对其背后的技术进行了深入分析。比如，Meta 的谢赛宁曾在 Twitter 上进行拆解，其认为 Sora 所采用的 DiT 结构，就是基于他在 ICCV 2023 发布的 DiT（Diffusion Transformer）思路构建的，这也是支撑 Sora 的一个重要基础。</p><p>&nbsp;</p><p>在模型架构方面，Runway、Pika 等模型底层采用的是扩散模型（Diffusion Model）技术，利用高斯噪音和 prompt，再通过 U-Net 对噪音进行解析，实现逐帧的渲染。虽然 prompt 通过 Transformer 技术得到了前后文的联系，但视频生成中却没有较大时间轴或前后联系的概念，从而导致先前的视频只能生成三四秒，画面跳跃跳帧等问题严重。</p><p>&nbsp;</p><p>而 Sora 利用 Transformer 替代 Diffusion 的 U-Net，不限制原始视频的尺寸，不仅能通过 Transformer 技术保证前后的连贯性，还能保证生成视频在各个画幅比例下都有很好的表现，从而生成时长更长、效果更好的视频。</p><p>&nbsp;</p><p>目前，Sora 能够生成 1 分钟的视频，深圳市鼎盛方圆科技发展有限公司创始人黄鸿波表示，理论上来看，Sora 是能够生成生成更长时间的视频的，但其中的不确定性会更多，也会需要更高的算力。“从零到一很简单，但再想往上则需要质的飞跃，难度比较大”。</p><p>&nbsp;</p><p>比起生成的视频时长，黄鸿波认为，Sora 这类视频生成模型更应解决的是如何保持人物一致性和场景一致性。这两点都是目前业内比较难以克服的难题。以人物为例，一段完整的视频中不仅存在主角，还存在配角和各种人物关系。在电影和电视剧的实际拍摄中，人是真实存在不会改变的，但 Sora 或其他目前现存的技术都无法保证人物的一致性。人物的每次生成，脸型、肤色、眼睛大小、痣的位置等都会发生变化。场景同样如此，不同的镜头会从不同角度进行拍摄，但周围的场景需要有一些变化。“从目前的视频演示来看，Sora 已经趋近完美，如果能解决人物一致性和场景一致性的问题，基本上就能达到影视公司想要的结果了”。</p><p>&nbsp;</p><p>此外，Sora 对“世界模型”的实现方式也存在一定争议。OpenAI 声称 Sora“扩展视频生成模型是构建物理世界通用模拟器的一条可行之路”。英伟达高级研究科学家 Jim Fan 也断言，Sora 是一个数据驱动的物理引擎，是一个可学习的模拟器，或“世界模型”。但也有人对此提出质疑。图灵奖得主 Yann LeCun 认为 Sora 并不理解物理世界，甚至称 Sora 对“世界模型”的实现方式注定是死路一条。</p><p>&nbsp;</p><p>具体来说，Sora 在生成视频时依赖于文本指令，这些文本描述了场景和意义。如果 Sora 能够理解视频内容，并在给定一段视频后，补充出更长的视频，且前后情节逻辑一致，那么这将是一个重要的进步。这将表明 Sora 不仅仅是通过视觉理解事物，而是能够从更深层次上理解视频内容。</p><p>&nbsp;</p><p>“长期来看，如果 Sora 能够在视频中实现首尾呼应，比如在电影中常见的前后呼应的情节，这表明它具有更长的因果链理解能力。这将是一个重要的里程碑，表明 Sora 越来越像是一个世界模型，能够理解物理定律和社会规则。”吴海波提到，目前，Sora 还处于一个比较早期的阶段，类似于早期的 GPT-3——它展现出了一定的能力，但尚未达到 ChatGPT 那样的成熟度。“但它的进步速度很快，2024 年值得我们期待，届时应该会有许多新的进展出现。”</p><p></p><h2>Sora如何重塑千行百业？</h2><p></p><p>&nbsp;</p><p>当前，Sora 还未正式对外开放。在近日的一场专访中，Sora 的核心团队成员表示 Sora 太过强大，还不能让普通人很快就用到，OpenAI 正在收集用户反馈，还有很多安全工作要做。而根据 OpenAI CTO Mira Murati 此前的说法，“Sora 最快在今年内开放公测”。</p><p>&nbsp;</p><p>作为一个基础模型，Sora 无疑会对各行各业产生影响，在影视、电商、游戏行业中，Sora 一定会带来新的想象力。其中，影视行业将会成为 Sora 的首选。</p><p>&nbsp;</p><p>目前，影视行业的制作流程涉及多个环节。编剧完成剧本后，会寻找合适的导演合作。在好莱坞或国内的大型制片厂，他们通常会先找普通演员拍摄样品，需要将 90 分钟的电影精华部分浓缩，拍摄成 30 至 40 分钟的样片，用于向投资人展示电影的内容、故事情节、人物设定以及特效应用等。只有当投资人认可了故事的创新点和市场潜力后，才会决定投资。不过，这类样片的制作成本相当高，每分钟的制作费用在 1 至 2 万元之间。</p><p>&nbsp;</p><p>如果引入 Sora 这类视频生成模型，将大幅降低制作成本，成本可能压缩至每分钟数千元。此外，Sora 还能免除影视制作的场景搭设、威亚特效、影视后期等工作，显著提高制作效率。</p><p>&nbsp;</p><p>“在与北京影视行业的合作中，我发现他们在拍摄电视剧和电影时，经常遇到一些无法通过常规手段拍摄的镜头，如宇宙大爆炸、地月轨道等场景，这些都需要依赖 3D 后期制作来完成。但这类镜头的制作成本极高。以电影行业常见的 25 帧/秒为例，一个 2 至 3 秒的镜头就包含约 70 帧的画面，按照帧计费的 3D 后期制作费用，这样短暂的镜头也需要投入上千甚至上万的成本。”黄鸿波介绍道，有了 Sora 技术后，影视公司可以将那些特效制作成本高昂或无法通过演员实际拍摄的场景，通过 Sora 或类似的视频生成模型来展现。“Sora 不仅对影视行业有益，它对传统广告制作、游戏和流媒体方面也有一定的影响，一些画面镜头的拍摄通过 AI 在几分钟内就能完成，节省了大量的人力物力。”</p><p>&nbsp;</p><p>在电商行业中，Sora 这类视频生成模型也带来了新的想象力。</p><p>&nbsp;</p><p>传统的产品视频拍摄需要模特、场景布置、拍摄以及后期制作等多个环节，而 Sora 只需输入相应的文本描述或图片，即可在短时间内生成逼真的视频，极大地提高了电商营销素材制作效率。此外，商家可以利用 Sora 生成产品在不同场景下的视频，或者展示产品在不同空间布局下的效果，从而提升消费者的购买意愿。</p><p>&nbsp;</p><p>虽然 Sora 在视频生成方面取得了显著进步，但要想真正应用在电商行业中，仍面临一些挑战。“目前，用户可以通过提交指令给Sora，Sora会在一段时间后生成视频反馈给用户。这种交互方式虽然令人兴奋，但也存在局限性，因为它缺乏明确的控制和交互方式。”吴海波提到，以电商为例，商家可能更希望基于某个已有商品生成视频内容，在将实体商品与视频结合方面，Sora 目前还无法满足需求。Sora 无法将商家的商品巧妙地融入视频中，并展示商品在真实场景中的应用，让潜在顾客直观地了解商品。</p><p>&nbsp;</p><p>“尽管 Sora 已经展示了在自由发挥状态下的创造力，但我们还不清楚如何将这些技术与现有电商平台有效结合，如何让它按照我们的需求生成内容，还有待进步一的优化。”吴海波表示，要想在电商行业中进一步拓展 Sora 技术的应用范围，还需要不断研究并探索新的方法，以实现商品与视频的完美结合。</p><p>&nbsp;</p><p>游戏作为较早落地 AIGC 技术的行业之一，在制作过程中也可引入 Sora 这类视频生成模型。黄鸿波提到，目前游戏行业比较容易落地的是大场景、风格转换和季节转换类型。</p><p>&nbsp;</p><p>比如，可以借助 Sora 技术，实现游戏中的季节转换等场景，通过每个季节 2-3 秒的场景交替生成游戏内的视频，这样不仅可以提升游戏的视觉体验，还能有效减少游戏的制作开发成本。游戏内的服装道具也可以通过 Sora 来完成。而对于游戏人物的动作，如跑步和飞翔，传统的制作方法通常涉及到底模建模、骨骼绑定以及动作合成。现在这些工作也可以通过 AI 技术来完成，在最后由人工进行必要的补充和调整，以确保动作的真实性更加出色。</p><p>&nbsp;</p><p>此外，光影和材料的仿真也是游戏制作中的重要环节，这些同样可以通过 AI 技术实现。例如，当角色从两米高的地方跳下时，不同材质的服装（如丝绸、粗布、盔甲）会产生不同的漂浮效果、落地速度和声音，这些细节在游戏和电影制作中都有着专门的处理流程。</p><p>&nbsp;</p><p>四足动物的动作设计是游戏行业的痛点之一。人类的走路和跑步动作相对自然，但四足动物的动作往往难以协调。而这类问题正是 Sora 这类技术可以发挥优势的地方。特别是像猫狗等常见的动物，由于不涉及复杂的 IP 和版权问题，更适合作为实践案例来解决动作设计上的挑战。</p><p>&nbsp;</p><p>不过，相较视频生成模型，图片生成模型在技术上已经更为成熟，这使得其在多个行业中的应用更加广泛和深入。</p><p>&nbsp;</p><p>“目前在游戏行业中应用最多的还是文生图模型。一般拥有自己 IP 的企业都会利用已有的形象素材，训练自家的文生图模型，生成视频或相关角色的形象参考”。据黄鸿波介绍，所有的文生图、文生视频、角色设计生成、形象设计生成，都无法直接采用生成产物，只是给设计人员一些灵感和启发，让他们以此为参考进行设计和开发。以一个海岛家园类的游戏为例，可以让 Stable Diffusion 等工具生成大量的海岛、家园、游戏风格设计图，给美术的同学一些启发，这也是目前企业内多数的落地形式。</p><p>&nbsp;</p><p>在电商行业中，图片生成模型也已得到广泛应用。吴海波提到，相较于视频生成技术，图片生成技术已经发展得更为成熟，因此在这一领域的应用也更为迅速。去年，核心团队来自蘑菇街的 AI 商拍工具 WeShop 上线，WeShop 正是基于 Stable Diffusion 模型提供 AI 智能商品图生成服务。目前，WeShop 主要服务于两类用户：一类是供应链为主的工厂老板，他们可以利用 WeShop AI 将商品图片转换成不同模特和背景的图片；另一类是计划拓展海外市场的电商，他们可以通过 WeShop AI 将国内商品图片适配到适合海外市场的模特场景中。</p><p>&nbsp;</p><p>“展望图片生成技术的未来，我认为 Sora 的成功表明模型规模的重要性，我们预期图片领域的基础模型也将取得显著进步。业界的技术路线和思路正趋于一致，大家都认识到需要引入 DiT 结构。尽管目前还有一条尝试纯 Transformer 基础架构的路线，类似 于GPT，但尚未超越现有技术。然而，随着 Sora 证明了大模型的有效性，我们可以预见将有更多资源投入到图片生成领域，推动其向前发展。这一点或许尚未得到广泛关注，但我坚信图片生成技术很快将迎来重大突破。”吴海波总结道。</p><p></p><h2>担心被 Sora 们取代？</h2><p></p><p>&nbsp;</p><p>Sora 给不同行业带来巨大变革可能得同时，也给就业市场带来了挑战，越来越多的从业者开始担心，自己终将被 Sora 们所取代。首当其冲的是影视行业从业者，不少声音开始讨论“特效公司要死了吗”“导演、后期是不是都要失业了”。</p><p>&nbsp;</p><p>对此，受访专家们普遍持乐观态度。以 CG 技术的出现为例，当年 CG 技术崭露头角时，许多动画师曾担忧自己的工作可能会受到威胁。然而，事实并非如此。实际上，CG 技术并未降低制作电影或动画的成本，反而使得成本有所上升。与此同时，CG 技术让人们能够创作出更高质量、更具视觉震撼力的作品，这反而激发了画师和导演的创造力，使他们能够制作出更为精彩的内容，也进一步提升了整个行业的标准。</p><p>&nbsp;</p><p>吴海波认为，面对 CG 技术这样的革新，我们应该积极拥抱变化，从中寻找新的机遇，而不是一味地担忧和抵触。如果我们固执地坚持旧有的工作方式而不愿适应，那么确实可能会面临问题。但与此同时，新技术也为我们打开了更广阔的市场，提升了行业的上限，并为我们提供了更多尝试不同角度和方法的可能性。“如果你坚持认为自己被新技术替代了，这或许是一种无法避免的心态。然而，我认为，有些工作被新技术解放，实际上是一件好事。换个角度看，我们可以说自己是从原有的束缚中得到了解放，迎来了新的机遇和挑战。”</p><p>&nbsp;</p><p>Sora 同样如此。目前来看，Sora 仍只是一款工具，并不能完全取代某一职位或环节，而是帮助人们更好地提升工作效率。以影视拍摄流程为例，尽管有 ChatGPT 这样的大语言模型协助，剧本编写仍需编剧来把控故事情节和故事性。分镜镜头的策划也需要导演来完成，因为模型生成的成品往往缺乏灵魂，需要人类加入细节、个人的情感和灵魂。同样，演员也是不可或缺的角色，因为观众既有人注重故事情节，也有人喜欢看明星的表演，如果取代了明星，电影就失去了其独特的意义。</p><p>&nbsp;</p><p>那么，Sora 究竟带来了什么，又能取代什么呢？</p><p>&nbsp;</p><p>黄鸿波认为，Sora 确实能加速视频和电影的制作效率，降低生产成本，并有可能取代部分特效制作公司的流程。但需要注意的是，这并非完全的取代，而是借助 Sora 完成一个大致的 demo，为特效公司提供思路，并替代部分相对简单的特效制作。原本需要十天才能完成的工作，现在可能只需要三五天就能完成。必须明确的是，任何技术的诞生都只是一种工具，其存在的目的是为了服务于人类。因此，完全的取代并不存在，工具的作用更多的是降低成本、提升效率。</p><p></p><h2>写在最后：参与到 AI 变革中来</h2><p></p><p>&nbsp;</p><p>近两年，AI 技术的快速演变和不断创新的特性超乎了所有人的预期，一个又一个创新模型的发布让人们不断惊叹于 AI 的潜力和能力。时代之下，更应该保持对 AI 技术发展的关注，随时准备迎接新的突破和变化，通过不断学习和适应新技术，在 AI 技术的浪潮中找到自己的位置，参与到 AI 变革中来。</p><p>&nbsp;</p><p>“我们现在的目标是首先参与到这场变革中来，将自己转变为一个 AI Native 的公司。我们从蘑菇街独立出一个团队来开发 WeShop，就是希望以创业团队的心态来完成这个项目。如果我们仍然使用传统的业务模式和资源来应用 AI 技术，我们可能会错过未来真正的大机会。因此，我们保持创业团队的状态，摒弃过去的包袱，以便在 AI Native 的环境中创造出新物种，抓住未来的机会。”吴海波认为，当前 AI 技术在电商领域的变革性影响难以清晰描绘，但其一定会为整个行业带来深刻变革，这不仅仅局限于在现有电商平台上增加智能问答功能或 AI 拍照等改进，而是当 AI 技术普及到一定程度时，人们将会见证一个全新的电商生态系统的崛起。</p><p>&nbsp;</p><p>对于影视和游戏行业，AI 带来的变革同样在发生，但目前都还缺少一个完整可落地的方案——一个能将文生图、文生视频等单一化工具串联起来的综合性工具。</p><p>&nbsp;</p><p>黄鸿波认为，理想的情况是，只需要手稿和文字描述，就能直接流程化生成包括 2D 图像、3D 模型、立绘、骨骼绑定以及动作生成等在内的完整一套内容。对于游戏行业而言，这样的综合性工具能够极大地提升开发效率。通过输入文字描述和手稿，工具能够自动处理生成游戏所需的各种资源，从而大大减轻开发者的负担。同样，影视行业也迫切需要这样的解决方案。只需要提供脚本，工具便能直接分析出完整的故事情节梗概，并基于这一情节生成围绕其展开的视频内容。这样不仅能确保画面风格的统一性和一致性，还能提高影视制作的效率和质量。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/WDp1hR6JQ7WzaHuVv3cP</id>
            <title>最新大模型推理优化进展：英伟达、阿里、腾讯和零一万物专家观点解读｜AICon</title>
            <link>https://www.infoq.cn/article/WDp1hR6JQ7WzaHuVv3cP</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/WDp1hR6JQ7WzaHuVv3cP</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 11:00:00 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大型模型, 经济增长, 训练挑战, 推理优化
<br>
<br>
总结: 大型模型的出现为新的经济增长注入了新的动力，但在训练和推理方面，它们也面临诸多挑战。为了解决这些挑战，业界专家们在AICon全球人工智能开发与应用大会上特别设置了“大型模型推理优化”专题，分享了关于大型模型的优化技术和经验。通过这些分享，观众可以深入了解大型模型的挑战和优化方法，为大型模型的应用和发展提供更多思考和借鉴。 </div>
                        <hr>
                    
                    <p>大型模型的出现为新的经济增长注入了新的动力，但在训练和推理方面，它们也面临诸多挑战。这些挑战包括计算资源的巨大需求、并行化限制、模型体积和训练难度、数据质量、能耗和推理速度、算力不足、数据处理难题、思维模式转变以及高昂的成本。</p><p></p><p>为了向业界提供更多思考和借鉴的机会，我们在 AICon 全球人工智能开发与应用大会上，特别设置了“大型模型推理优化”专题。这一专题由阿里巴巴的研究员林伟老师担任出品人，旨在为观众带来更严谨、更有启发的演讲。我们邀请了四位老师进行分享，他们的精彩演讲将为大家带来深刻的思考和丰富的收获。</p><p></p><h4>BladeLLM 大模型高性能部署框架</h4><p></p><p></p><p>我们很荣幸地邀请到阿里云的高级算法专家李深作为首个分享的嘉宾。作为阿里云人工智能平台 PAI 模型系统优化的 Tech Leader，他在模型压缩和推理优化等方面拥有超过 10 年的丰富经验。在本次大模型推理优化专题演讲中，李深将重点介绍阿里云的 BladeLLM 大模型高性能部署框架。BladeLLM 高性能部署框架是基于阿里云人工智能平台 PAI 的技术积累和实践经验构建的。该框架不仅应对了大模型在线服务部署中的场景特性、资源规模和性能指标等更高更复杂的要求，而且兼容了大模型主流生态，提供了灵活易用的接口。</p><p></p><p>在演讲中，李深将深入探讨大模型服务部署优化面临的主要挑战，以及 BladeLLM 架构与核心优化技术。这些技术包括高性能算子与 AI 编译优化、模型压缩与算法优化、长上下文优化等，将为听众呈现出多层次联合的极致性能优化方案。通过他的分享，听众将了解大模型服务部署中的主要瓶颈与技术挑战，探索大模型部署优化的主要技术手段，并且深入了解大模型在线服务的规模化生产部署的实践经验。</p><p></p><h4>当大模型推理遇到算力瓶颈，如何进行工程优化？</h4><p></p><p></p><p>本专题出席的第二位嘉宾是零一万物的资深算法专家李谋。他曾历任阿里达摩院和华为云 EI 服务产品部技术专家，目前担任零一万物大模型在线推理服务负责人。在本次专题演讲中，他将探讨当大模型推理遇到算力瓶颈时，如何进行工程优化。随着大语言模型的持续发展，其参数量和序列长度呈指数级增长，因此面临的算力挑战愈发严峻。他将结合大模型的算力需求和模型结构，详细介绍零一万物在构建 Yi 模型在线推理服务过程中所采用的优化技术手段。通过他的分享，听众将了解到大模型推理算力瓶颈及主要工程优化手段，以及大模型应用场景的未来发展趋势。</p><p></p><h4>TensorRT-LLM: Past, Present and Future</h4><p></p><p></p><p>我们很荣幸地邀请到英伟达的高级技术总监杨军作为我们的专题演讲嘉宾。作为英伟达 AI 计算架构部门的负责人，他主要关注于 AI 系统全栈优化技术。在本次大模型推理优化专题演讲中，他将分享关于 TensorRT-LLM 的主题：“TensorRT-LLM: 过去、现在与未来”。</p><p></p><p>TensorRT-LLM 项目源起于对大语言模型推理优化的迫切需求。在演进迭代过程中，团队不断进行设计思考，探索最佳方案以满足日益增长的需求。当前设计方案的核心原则将是他演讲的重点，将会深入探讨该方案背后的理念和技术实现。此外，杨军还将简要介绍 TensorRT-LLM 的未来规划，展望该项目在大模型推理优化领域的发展方向和趋势。通过他的分享，听众将获得对 TensorRT-LLM 项目的深入了解，探索其在过去、现在和未来的演进路径和价值。</p><p></p><h4>太极 Angel 助力生成式大模型高效落地</h4><p></p><p></p><p>我们邀请的第四位演讲的嘉宾是腾讯高级工程师刘凯。作为腾讯混元大模型推理方向负责人，他在大模型压缩优化及推理加速领域拥有丰富经验，曾带领团队完成了大模型压缩 &amp; 推理框架的从零到一的构建。在本次大模型推理优化专题演讲中，刘凯将分享关于“太极 Angel 助力生成式大模型高效落地”的主题。</p><p></p><p>随着生成式 AI 技术的迅速发展，模型规模不断增大，结构也从 Dense 向 MoE 进化。在这一背景下，大模型应用的性能、吞吐、成本成为关注焦点。他将介绍腾讯太极机器学习平台所研发的 Angel-HCF 推理框架和 Angel-SNIP 压缩框架，以支持混元文生文、文生图、文生视频、多模态等 AI 生成领域的优化，助力腾讯混元大模型在公司内全面铺开应用。</p><p></p><p>刘凯将深入探讨生成式 AI 技术的挑战和常用优化方法，重点介绍太极 Angel-HCF 大模型推理框架和太极 Angel-SNIP 大模型压缩框架。通过他的分享，听众将了解生成式 AI 的技术难点和优化手段，大模型推理加速的技术细节，以及大模型压缩的技术方法和后续发展。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/63/63f7b4fc8288624587ab5be6059fad48" /></p><p></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/a54Zpxv6yA55ZgHmWFt0</id>
            <title>大模型助力具身智能、电池研发与蛋白质研究，讯飞、深势科技、字节专家齐聚分享｜AICon</title>
            <link>https://www.infoq.cn/article/a54Zpxv6yA55ZgHmWFt0</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/a54Zpxv6yA55ZgHmWFt0</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 08:44:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型会议, 具身智能通用机器人, AI赋能电池研发, 生成式AI蛋白质科学研究
<br>
<br>
总结: AICon 全球人工智能开发与应用大会 暨 大模型应用生态展·2024 是由极客邦科技旗下 InfoQ 中国主办的技术盛会，主要面向工程师、产品经理、数据分析师的大模型会议，会议聚焦大模型训练与推理、AI agent、RAG、多模态大模型等热门方向。在专题论坛中，探索了大模型在具身智能通用机器人领域的创新探索、AI赋能电池研发、生成式AI如何助力蛋白质科学研究等议题，展示了人工智能在不同领域的前沿应用和潜力。 </div>
                        <hr>
                    
                    <p>AICon 全球人工智能开发与应用大会 暨 大模型应用生态展·2024 是由极客邦科技旗下 InfoQ 中国主办的技术盛会，主要面向工程师、产品经理、数据分析师的大模型会议，会议聚焦大模型训练与推理、AI agent、RAG、多模态大模型等热门方向。</p><p></p><p>在 AICon 的议题设置中，80% 都着眼于开发与应用，但在我们的专题论坛中，我们想要为大家带来更加开阔的视野，探索 AI 领域的未来前沿。因此，我们特别策划了【AI 前沿探索】的话题。我们邀请了科大讯飞 AI 研究院副院长、科研部部长李鑫博士来担任出品人，以确保我们选择的议题具有更高的质量。在经过认真评估后，我们为听众选择了三个精彩的议题。</p><p></p><h5>大模型在具身智能通用机器人领域的创新探索</h5><p></p><p></p><p>首先，我们很荣幸邀请到季超，他是科大讯飞的人形机器人首席科学家。季超长期从事机器人与智能装备硬件关键技术及产品开发，尤其涉及人机交互、具身智能和机器人强化学习运动控制等前沿方向。</p><p></p><p>在他的演讲中，他将重点探讨大模型在具身智能通用机器人领域的创新探索。这涉及到人形机器人集成人工智能、高端制造、新材料等先进技术，为未来产业发展带来颠覆性的机遇。他将深入分析智能机器人行业发展趋势，揭示产业现状和痛点，以及在大模型底层能力突破的基础上，探讨具身智能通用机器人的关键技术及系统集成。此外，他还将分享讯飞在这一领域的成果和进展，以及面向 AGI+Robot 行业生态构建的倡议。通过他的分享，您可以深入了解大模型浪潮下具身智能机器人的重大机会，探索企业在这场浪潮中的角色和贡献。</p><p></p><h5>AI for Science 新范式赋能电池研发</h5><p></p><p></p><p>继而，我们荣幸邀请到深势科技的高级材料研发总监王晓旭博士，他拥有深厚背景融合了广泛产学研合作与项目实施的宝贵经验。他将呈献一场题为“AI 赋能科学新范式：重塑电池研发领域”的专题演讲，深度剖析新能源产业电池创新面临的复杂挑战，并揭示人工智能在科研领域的最新应用趋势。</p><p></p><p>演讲核心，王晓旭博士将详述新能源领域的现状与未来趋势，强调电池材料研发在推动该行业跨越性发展中的核心地位。他将着重阐述“AI for Science”这一新兴科研模式如何为新能源科学研究，尤其是电池技术的演进，注入变革力量。此部分讨论将深刻聚焦于 AI 技术在加速电池仿真技术创新及大模型在精准预测电池性能、优化电池设计方面的革命性贡献。</p><p></p><p>通过他的分享，听众将不仅洞悉“AI for Science”新科研范式背后的思考，还能深入了解 AI 技术在电池材料科学中的实战应用案例，探索这些前沿技术如何通过算法的迭代与优化，从而在解决实际问题中彰显其巨大潜力与价值。</p><p></p><h5>生成式 AI 如何助力蛋白质科学研究</h5><p></p><p></p><p>最后，我们邀请到了字节跳动 ByteDance Research 的高级研究员郑在翔老师，拥有丰富的学术背景和研究成果。郑在翔将带来题为“生成式 AI 如何助力蛋白质科学研究”的演讲。他将深入探讨蛋白质科学在现实世界中的重要性以及人工智能在这一领域中的前沿应用。</p><p></p><p>在演讲中，郑在翔将介绍字节跳动在探索「大规模生成式人工智能赋能蛋白质科学研究」方面的最新研究进展。特别是，他将重点介绍如何利用大规模生成式人工智能技术，包括大语言模型（LLMs）与扩散概率模型（Diffusion Models），来构建和实现统一、通用且强大的「蛋白质基础模型」。</p><p></p><p>通过他的分享，听众将了解到生成式 AI 如何在蛋白质建模与设计领域发挥作用，以及如何利用海量蛋白质数据驱动最前沿的技术来解锁蛋白质科学的新可能性。</p><p></p><p><img src="https://static001.geekbang.org/wechat/images/f3/f3176374aa3d4f506f44d28392b53d74.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/d5/d599256893d25db9981ffa3bd9505efc.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/NEpZd52h3IHX8wzcFlaw</id>
            <title>神秘大模型一夜“征服”所有人，超GPT-4却无人认领？网友：OpenAI 要有大麻烦了</title>
            <link>https://www.infoq.cn/article/NEpZd52h3IHX8wzcFlaw</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/NEpZd52h3IHX8wzcFlaw</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 06:39:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: gpt2-chatbot, OpenAI, 模型, GPT-4.5
<br>
<br>
总结: 一款神秘的模型 gpt2-chatbot 在没有官方介绍的情况下引起了巨大关注，被认为可能是 OpenAI 的下一个模型或者是 GPT-4.5 或 5 的 beta 测试。尽管没有明确身份，但其表现出色，效果领先多个模型，在各领域展现出优秀的能力。对于其神秘来源和真实身份，仍存在各种猜测和推测。 </div>
                        <hr>
                    
                    <p>整理 | 华卫</p><p></p><p>昨晚，一个突然出现在 LMSys 基准测试网站的神秘模型，在大模型领域掀起了巨大波澜。用过的人都在夸，刚发布服务器就被挤爆，奥特曼也迅速出现在吃瓜现场...</p><p></p><p>而这一切的主角，就是 gpt2-chatbot。没有出处，也没有介绍，在没有官方文件的情况下，一夜间“惊艳”所有人的视线。</p><p></p><p>有人认为 gpt2-chatbot 可能是 OpenAI 的下一个模型，还有人说它是 GPT-4.5 或 5 的 beta 测试，甚至有评价称这可能是对 GPT 架构的根本升级。尽管该模型的系统提示表明它来自 OpenAI，但 gpt2-chatbot 却拒绝引用 OpenAI。</p><p></p><p>有意思的是，在众人猜测 gpt2-chatbot 身份的时刻，Open AI 的 CEO Sam Altma 发帖表达了对 gpt2-chatbot 的喜爱：“我确实对它情有独钟。”</p><p><img src="https://static001.geekbang.org/infoq/3f/3f40b607b4d7747592789c9299b890eb.webp" /></p><p></p><p>对此，有网友评价说：“如果不是 ChatGPT 的新版本，OpenAI 就有麻烦了！” 也有网友表示，“希望它不是 GPT-5，这个模型很难完成 Opus 擅长的推理任务。”</p><p></p><p>以下是部分用户对 gpt2-chatbot 的测试重点总结：</p><p>gpt2-chatbot 一直声称“基于 GPT-4”并具有“v2”个性，并称自己为 ChatGPT。其呈现自己的方式，通常与其他在 OpenAI 数据集上训练的模型的幻觉回复不同。它似乎使用了 OpenAI 的 tiktoken 分词器，对 OpenAI 使用的特殊 token 有反应，且对 Claude/Llama/Gemini 使用的特殊 token 没有反应。当需要提供联系方式时，gpt2-chatbot 会始终如一地给出 OpenAI 的信息，甚至比 GPT-3.5/4 的更详细。它表现出特定于 OpenAI 的提示注入漏洞，且从未声称属于 OpenAI 以外的任何其他实体组织。对于相同的提示，gpt2-chatbot 始终提供与 Anthropic、Meta、Mistral、Google 等模型不同的输出。</p><p></p><p></p><h1>效果领先多个模型</h1><p></p><p></p><p>gpt2-chatbot 一经发布，众多用户都涌入这一模型测试其在各领域的表现。从公开平台的反馈来看，该模型在多方面的能力和实际效果都赶上甚至超过许多其他的前沿模型。</p><p></p><p>例如，gpt2-chatbot 可以在 PyOpenGL 中一次性生成旋转 3D 立方体， 而 GPT-4、Gemini-1.5 和 Claude-3 需要尝试三次才可以。</p><p></p><p><img src="https://static001.geekbang.org/infoq/2a/2a39591deaf83c585bba17d3cebf23c2.webp" /></p><p></p><p>在解决兄弟姐妹之谜时，gpt2-chatbot 得出和 GPT-4 Turbo 相同的结果。</p><p></p><p><img src="https://static001.geekbang.org/infoq/b9/b9d92f04a907946e3793d232a6ab84c8.webp" /></p><p></p><p>除这两个案例外，还有许多网友抛出了 gpt2-chatbot 在解决各类问题时的优秀能力。</p><p>网友 @Andrew Gao：gpt2-chatbot 一口气正确解决了 IMO（数学奥林匹克）问题。</p><p></p><p><img src="https://static001.geekbang.org/infoq/6b/6bc094a465165cc1fd91f04d1d49ec45.webp" /></p><p></p><p>网友 @murat ：该模型可以解决一些 GPT-4 做不到的事情，如 A+B-1 的数学问题，打破了非常强的学习惯例。</p><p></p><p><img src="https://static001.geekbang.org/infoq/69/6934696c0b86f0a02e0b96aedf42c5aa.webp" /></p><p></p><p>网友 @Phil：用 gpt2-chatbot 制作 ASCII 艺术的效果领先于任何其他模型。</p><p></p><p><img src="https://static001.geekbang.org/infoq/7f/7f5b3931d78ea4fb34a69768535491ef.webp" /></p><p></p><p>网友 @murat ：gpt2-chatbot 第一次尝试就解决了在 Claude Opus 、GPT4 和 llama3-70b 模型上失败的 TypeScript 编写问题，并且没有错误。</p><p></p><p><img src="https://static001.geekbang.org/infoq/4f/4f30ed7ccdac5a75e729a30a4599c1c8.webp" /></p><p></p><p>不仅在复杂的代码操作任务以及用于测试新模型的所有编码提示上，gpt2-chatbot 比 Claude Opus 以及最新的 GPT-4 更好。当被要求规划 LLM 代理的计划以帮助用户预订晚餐时，gpt2-chatbot 也能给出出色的响应。</p><p></p><p></p><h1>模型的神秘来源</h1><p></p><p></p><p></p><blockquote>“在我看来，这个神秘模型很可能是 GPT-4.5 或 GPT-5，或者实际上是一个真正的 GPT-2 模型，由 OpenAI 或 LMSYS 提供。总的来说，它输出的内容质量，特别是格式、结构和整体理解，绝对是一流的。对我来说，这感觉就像是从 GPT-3.5 到 GPT-4 的一步，但以 GPT-4 为起点。”关于 gpt2-chatbot 的公开网页介绍（非官方）</blockquote><p></p><p></p><p>当需要提供联系方式时，gpt2-chatbot 会始终如一地给出 OpenAI 的信息，甚至比 GPT-3.5/4 的更详细。而且，该模型使用 OpenAI 的 token 分词器，对 OpenAI 使用的特殊 token 有反应。</p><p>一种猜测认为，gpt2-chatbot 实际上是基于 GPT-2 架构的，其表现出的能力大大超出了任何以前已知的 GPT-2 模型。</p><p></p><p>另一种可能性是，它实际上是一个 GPT-2 模型。最近（2024 年 4 月 7 日）Meta/FAIR Labs 和 Mohamed bin Zayed AI University of AI （MBZUAI） 的一篇题为《语言模型物理学：第 3.3 部分，知识容量缩放定律》的文章深入研究了 GPT-2 架构的细节，并确定：“ GPT-2 架构在知识存储方面与 LLaMA/Mistral 架构相当甚至超过，尤其是在较短的训练持续时间内。”</p><p></p><p>至于该模型被认为是 GPT-4 的强烈说法，可以通过主要利用 GPT-4 生成的数据集来解释。然而，gpt2-chatbot 确实有一个与 GPT-4 模型不同的速率限制，用于直接聊天：</p><p></p><p><img src="https://static001.geekbang.org/infoq/56/566db9cdb12bad9b39d958caf36b4210.webp" /></p><p></p><p>虽然尚未比较对总速率限制与用户特定速率限制的完整限制，但在每日用户限制以及其他一些总服务限制上比 GPT-4 模型更具限制性。这可能意味着，该模型在计算方面的成本更高，并且提供计算的人更喜欢用户使用 Arena （Battle） 模式来生成基准测试。</p><p></p><p>如果 LMSYS 是 gpt2-chatbot 的模型创建者，那么该文章的一些结果的应用就可以利用通过 LMSYS 生成的数据集进行训练等。</p><p></p><p>如果你想试用或者帮助解开 gpt2-chatbot 的身份谜题，现在可以进入到 LMSys 网站（https://chat.lmsys.org/）并选择 gpt2-chatbot。每个用户每天可以测试 8 条消息的直接聊天，之后可以切换到“竞技”模式尝试匹配到该模型选项。另外，尝试时至少需要对所有提示展开三次验证，以获得达到该模型平均能力的结果。</p><p></p><p>参考链接：</p><p>https://rentry.co/GPT2</p><p>https://twitter.com/itsandrewgao/status/1785013026636357942</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/PyE2ijOvWDiaL1fImie9</id>
            <title>谷歌裁掉整个Python语言团队！PyTorch 创始人回应：“核心语言团队无可替换”</title>
            <link>https://www.infoq.cn/article/PyE2ijOvWDiaL1fImie9</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/PyE2ijOvWDiaL1fImie9</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 05:13:32 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 长期人手不足, Python语言团队解散, Thomas Wouters, 谷歌新Python团队
<br>
<br>
总结: 谰言称谷歌解雇Python团队，但Thomas Wouters证实了这一消息，谷歌正在慕尼黑组建新的Python团队，可能是为了成本重组。谷歌Python团队成员表示团队工作内容和重要性，谷歌多个项目都是用Python开发，谷歌仍在寻找具备Python技能的人才。 </div>
                        <hr>
                    
                    <p></p><p>&nbsp;</p><p></p><blockquote>我们长期人手不足，但是我20年来最好的工作。</blockquote><p></p><p>&nbsp;</p><p>谷歌 Python 工程师、Python 指导委员会成员Thomas Wouters昨天在社交媒体上发布了一条消息，称谷歌解散了Python语言团队。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/cc/cc6072e7537b7e0d5b2e966709653732.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>其实上周就有消息称，为了GenAI，谷歌解雇了整个Python语言团队。</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/0a/0ad1ff3732b42a279d7b72c047735fd8.jpeg" /></p><p></p><p>&nbsp;</p><p>但由于缺少发布者的背景信息，这条消息没有引起太多的注意。但Thomas Wouters的出现，显然证实了“谷歌解雇Python团队”不是谣言。</p><p>&nbsp;</p><p>Thomas Wouters现在是谷歌员工，是CPython 核心开发，在Python 指导委员会任职8年多，同时也是Python 3.12 和 3.13 的发布经理。</p><p>&nbsp;</p><p>根据Wouters的说法，谷歌正在慕尼黑从头开始组建一个新的 Python 团队。有网友解释说，美国团队已经被解雇，而Wouters（位于荷兰）则被要求跨国加入到慕尼黑新招的团队中，训练新的团队。</p><p>&nbsp;</p><p>也有其他网友补充道，这次裁员可以是出于成本原因进行的重组，有的团队被彻底解散，有的则将两个团队合并为一个。这是“一种有利于低成本地区人们的模式。例如，两个团队合并，成本较高的经理被解雇，或者整个团队被解雇，但这些职责正在由工资较低的办公室的人员重新安排。”</p><p>&nbsp;</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/83/839378176d79ece37224502f5861d38d.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p>在Hacker News上，谷歌 Python 团队内部员工很快进行了回应，表示可能慕尼黑团队会“重拾”他们大部分或全部的工作，“对整件事真的很难过。这是我 20 年职业生涯（包括谷歌其他团队）中迄今为止最好的工作。我们是一个长期人手不足的团队，为谷歌的Python生态系统的很大一部分提供支持，多年来我们做了一些令人惊叹的工作。”</p><p>&nbsp;</p><p>这个消息惊动了领域内的很多开发者，包括 PyTorch 创始人、Meta 杰出工程师 Soumith Chintala，“显然 Google 解雇了整个 Python 基础团队，WTF！”，但“我认为基础/核心语言工程师很难被替代或变得可替代。他们拥有关于复杂代码和社交动态（social dynamics）的深厚知识，这些知识很难被记录下来。这对其他公司来说是一个介入和抢人的机会 (Meta 会开始接触他们，但可能无法吸收所有工程师)。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/a6/a6707155144fa7f0843fa7dc8f0bf9fb.jpeg" /></p><p></p><p>&nbsp;</p><p>在AI时代解雇Python团队，很多人表示不能理解：“谷歌是一家AI优先的公司，谷歌的未来与其AI产品紧密相连。所有AI都是用 Python 编写的，谷歌却解雇了所有 Python 团队。”</p><p>&nbsp;</p><p></p><p><img src="https://static001.geekbang.org/infoq/53/53d7bf97668458464a6e8ab1d831886f.jpeg" /></p><p></p><p>&nbsp;</p><p>&nbsp;</p><p></p><h2>谷歌不到十人的Python团队</h2><p></p><p>&nbsp;</p><p>根据网上谷歌 Python 团队成员爆料，目前该团队不到 10 人。团队的日常主要工作，除了为上游Python做出贡献之外，还包括：</p><p>&nbsp;</p><p>在谷歌维护一个稳定的 Python 版本，并确保 monorepo 中的所有内容都可以使用。在该员工任职期间，他们从 2.7 升级到 3.6，然后逐步升级到 3.11，每次更新都需要数月到一年多的时间，因为 Google 的规则是，如果您签入任何代码，您就要对其造成的每一次损坏负责；维护工具，使数千个第三方软件包不断从其开源版本更新，并为需要谷歌特定更改的软件包提供补丁队列；针对谷歌的风格指南和整体代码库，负责高度定制版本的工具，如 pylint 和 black；为 pybind11 做出贡献，并维护 C++ 集成工具；开发和维护 Python 的构建系统规则，包括付出巨大努力将 Python 规则转移到纯 Starlark 代码，而不是让它们纠缠在 blaze/bazel 核心引擎中；开发并维护了一个类型检查器（pytype），它可以在没有类型注释的情况下对代码进行推理，并使用一次一个文件的架构处理非常大的项目；对数亿行代码执行自动重构。</p><p>&nbsp;</p><p>该成员还表示，这只是团队工作的开发部分，他们还充当了谷歌的 Python “客服”，帮助解决棘手的问题，并为新人指明正确的方向。另外，Python团队还与许多其他团队合作，包括机器学习和 AI 团队、协作和 IDE 团队、protobuf 这样集成并生成 Python 绑定的团队、像 Google Cloud 这样希望向客户提供 Python 运行时的团队、就像 YouTube 这样有一个用 Python 构建的异常庞大系统的团队，他们需要保持它的性能和可维护性。</p><p>&nbsp;</p><p>正如该成员爆料的，由于Python的简单性和相对快速的维护，谷歌公司刚刚建立时就使用了Python，并且沿用至今：谷歌的搜索引擎、YouTube、机器学习、人工智能、机器人项目等都是用Python开发的。</p><p>&nbsp;</p><p>“Python 从一开始就是谷歌的重要组成部分，并且随着系统的发展和发展，这一点仍然如此。如今，数十名谷歌工程师使用 Python，我们正在寻找更多具备这种语言技能的人才。”谷歌计算机科学家兼研究总监 Peter Norvig 在2003年时说道。</p><p>&nbsp;</p><p>该公司的格言“Python 能用，C++ 必须用”，描述了它对这种多功能编程语言的依赖程度。</p><p>&nbsp;</p><p>早期，谷歌的创始人决定只要有可能就使用 Python，而在无法使用 Python 的地方只使用 C++。因此，当内存控制势在必行且需要低延迟时，就使用 C++。对于其他一切，Python 实现了易于维护和相对快速的交付。</p><p>&nbsp;</p><p>Python 的创建者 Guido Van Rossum 还在 2005 年加入谷歌团队并一直工作到 2012 年。</p><p>&nbsp;</p><p>是的，谷歌一直是 Python 编程语言的长期支持者和用户，除了 C++、Java 和 Go 之外，Python 是该公司的官方服务器端语言之一。Python 在许多谷歌内部系统上运行，并出现在许多 Google API 中，与 Google 的工程流程完美契合。</p><p>&nbsp;</p><p>Python 目前也是机器学习项目的开发人员最常使用的语言，包括谷歌著名的TensorFlow 框架就是以此为基础。</p><p>&nbsp;</p><p>还值得注意的是，谷歌与该语言本身和 Python 软件基金会有着密切的关系。</p><p>&nbsp;</p><p>谷歌从 2010 年起成为 PSF （Python软件基金会）赞助者，在 2021 年 2 月成为首个远景赞助者（赞助 35 万美元以其他资源）。资金主要用于提升 Python 生态的链供应安全，资源主要为Google Cloud的产品。另外，谷歌还参与赞助了Python的各类活动，比如 PyCon 和 EuroPython等。</p><p>&nbsp;</p><p>&nbsp;</p><p>参考链接：</p><p>https://www.learnenough.com/blog/10-Companies-Using-Python-In-2023-&amp;-Why-It's-Their-Go-To</p><p>https://news.ycombinator.com/item?id=40183125</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/01daa1c8fdd58f2d07af448ab</id>
            <title>模型量化与量化在LLM中的应用 ｜ 得物技术</title>
            <link>https://www.infoq.cn/article/01daa1c8fdd58f2d07af448ab</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/01daa1c8fdd58f2d07af448ab</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 02:11:31 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 模型推理优化, 大模型, 知识蒸馏, 剪枝
<br>
<br>
总结: 近年来基于Transformer架构的大模型在各项任务中取得了SoTA成绩，但其在推理过程中面临着巨大的内存需求和并行性较差的挑战。为了提高推理效率，常见的推理优化方式包括知识蒸馏、剪枝和量化等方法。知识蒸馏通过构造小模型来监督学习原模型的知识，剪枝则是通过剪除不重要的权重来提高模型的推理效率。量化则是将模型的参数或整个推理过程从浮点转化为整型，以降低显存需求并提升计算速度。 </div>
                        <hr>
                    
                    <p>一、模型推理优化</p><p>随着模型在各种场景中的落地实践，模型的推理加速早已成为AI工程化的重要内容。而近年基于Transformer架构的大模型继而成为主流，在各项任务中取得SoTA成绩，它们在训练和推理中的昂贵成本使得其在合理的成本下的部署实践显得愈加重要。</p><p>大模型推理所面临的挑战主要有以下两点：</p><p>巨大的内存（显存）需求，主要来自于模型本身参数和推理的即时需求。对于一个LLaMA2-30B的模型，载入显存其模型本身需要约60GiB的显存，推理过程中，单个token的KV cache 需要1.6MiB左右的显存：6656(layer dim) * 52(layer num) *2 (K &amp; V) * 2(fp16, 2bytes)；对于一个2048个token的请求则需要3.3GiB的显存。并行性较差，因为生成过程通常在时序上是一个串行的过程，导致decoding的过程较难并行，成为计算的瓶颈。</p><p>常见的推理优化方式有知识蒸馏（Knowledge Distillation,KD），剪枝（Pruning）和量化（Quantization），以及针对LLM的内存优化而提出的各种方案（如Flash Attention、Paged Attention等）。</p><p>蒸馏指通过直接构造小模型，作为学生模型，通过软标签与原标签结合的方式监督学习原模型的知识，从而使小模型具备与原模型相当的性能，最终用小模型代替大模型从而提高推理效率。</p><p><img src="https://static001.geekbang.org/infoq/3b/3bc790f85d215e9da2b4c30b59ad9999.png" /></p><p></p><p>【图片出处：Knowledge Distillation: A survey,2021,p2】</p><p>剪枝则是通过靠剪除模型中不重要的权重从而给模型“瘦身”，提高模型的推理效率，为了保证模型的能力，通常剪枝过程也需要伴随着模型基于训练数据的微调。根据剪除权重的维度不同，可以分为结构化剪枝（structured pruning）和非结构化剪枝（unstructured pruning）。</p><p>结构化剪枝：通常按权重张量的某一或多个维度成块剪除不重要的通道，并保持正常的矩阵乘法；但因剪除的通道影响上下层的推理，需要检查网络的逻辑准确性。非结构化剪枝：随机剪除权重张量中的不重要的元素，因而它通常会保持原本的权重结构，而造成稀疏的乘法计算，但并不能适配于通用的硬件，因而需要专用的硬件才能实现加速。</p><p>目前剪枝在LLM中的应用较少，如以下基于Activation-aware的剪枝工作[1]，主要是基于权重本身的的绝对值大小和输入张量的绝对值大小做非结构化剪枝，使权重张量本身稀疏化，而模型的精度损失也并不能达到工程化的要求。</p><p><img src="https://static001.geekbang.org/infoq/a7/a7c0368905dd39abaf44aa1b947bbf90.png" /></p><p></p><p>【图片出处：A simple and effective pruning approach for large language models,2021,p2】</p><p>再如下图最近结构化剪枝的工作[2]，通过搜索的方法寻找模型中的子结构，并通过重训练以保持模型精度，剪枝后的模型的精度相比原模型有很大的降低，只能跟同等参数量（剪枝后）的其他较小模型比较以显示其方法的意义。</p><p><img src="https://static001.geekbang.org/infoq/ae/aeb0eb2fafa1edef996b514c562a896c.png" /></p><p></p><p>【图片出处: Sheared LLaMA: accelerating language model pre-training via structured pruning,2023,p3】</p><p><img src="https://static001.geekbang.org/infoq/0d/0d83b7c9efbf7661762c8790a6988d5b.png" /></p><p></p><p>【图片出处: huggingface/Sheared-llama-1.3B】</p><p>而量化之所以会成为神经网络以及LLM的首选，主要有以下的优点：</p><p>降低显存的直观体现。一般LLM权重用FP16存储，而权重量化为int4之后，则直观上体积减小为原本的1/4（实际可能由于embeddings不量化，内存分配等一些原因会稍多一些），对显存的资源需求大大降低。W4A16、W8A16等算子的加速，从而提升计算速度。</p><p>二、量化简介</p><p>base</p><p>量化的本质通常是将模型的参数，或整个模型的推理过程从浮点转化为整型。</p><p>量化参数通常由 scale 和 zero-point两个值构成，前者为浮点，后者为整型。设x为一个张量（它可以为权重，也可以是推理的中间变量），其量化过程可以表示如下，</p><p><img src="https://static001.geekbang.org/infoq/51/51b8b963e750c6f5580e3cc25ff6507f.png" /></p><p></p><p>用b表示量化位宽，q{min}与q{max}分别表示整型值域的范围，例如int-8量化可以取[-128,127]，即q{min}=-2^(b-1)=-128，q{max}=2^(b-1)-1=127，clamp(a;q{min},q{max})表示输入值a基于[q{min}, q{max}]范围的截断操作，x{int}表示量化后的结果，s和z表示量化参数scale和zero-point。</p><p><img src="https://static001.geekbang.org/infoq/da/da234a5b498e5d7cc54cd1f64849d189.jpeg" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/3e/3e119ca9f4e393858c574320fd9a287d.jpeg" /></p><p></p><p>【图片出处：A Survey of Quantization Methods for Efficient Neural Network Inference,2021,p5；An Introduction to Quantization of Large Language Models,p12】</p><p>而从整型到浮点的反量化过程如下，</p><p><img src="https://static001.geekbang.org/infoq/3e/3e13360e74521164813dd2e8f356269e.png" /></p><p></p><p>关于量化参数，有很多算法基于搜索，最优化，LKD(layer-by-layer 蒸馏)等各类算法计算其较优解，从而尽可能减少量化引起的精度损失；而最直接的计算scale 和方法即是基于张量元素min/max。</p><p><img src="https://static001.geekbang.org/infoq/9a/9aa841df9c07e3b537030073e1cf54a7.png" /></p><p></p><p>以下是一段简单的代码表示张量x从fp32量化到int8整型，再反量化回fp32的示例：</p><p>x-&gt;x{int}-&gt;x_hat的过程的一个示例如下：</p><p><img src="https://static001.geekbang.org/infoq/4b/4b8a12a0c1c83c17d3667ed20d5ffec0.jpeg" /></p><p></p><p>量化前x：</p><p><img src="https://static001.geekbang.org/infoq/22/223cba97eca9b055644e871c66177745.png" /></p><p></p><p>量化后x_hat：</p><p><img src="https://static001.geekbang.org/infoq/b3/b36a52f20d47026d42be234f14bb46d5.png" /></p><p></p><p>对称/非对称</p><p>相比于非对称量化，对称量化的定义是量化所映射的整型值域基于0值对称，即上述公式的zero-point为0，qmax = -qmin，从而使量化的表达形式更为简化。</p><p>非对称量化有利于充分利用量化范围。例如Conv+ReLU输出的激励张量，其值皆为正值，若使用对称量化，则浮点将全部映射到[0~127]范围，有一半的范围未使用，其量化精度不如非对称量化。</p><p><img src="https://static001.geekbang.org/infoq/f6/f6d480f825af24ad569ea61dbf969e7c.png" /></p><p></p><p>【图片出处：A Survey of Quantization Methods for Efficient Neural Network Inference,2021,p5】</p><p>实际中往往选择对权重张量做对称量化，而对输入张量做非对称量化。以下是来自qualcomm 的量化白皮书中的分析，如权重和输入都选择非对称量化时，以Linear层的矩阵乘法为例，将表达式展开如下：</p><p><img src="https://static001.geekbang.org/infoq/15/151824173d1d8a86b7ee729d2c655108.png" /></p><p></p><p>第一项是整型张量的乘法操作，是必须的即时操作；第三、四项的操作包含了scale，zero和整型权重的乘法，这些都是提前预知的，因而可以事先计算作为偏置加上；第二项的计算依赖x{int}，是每次推理需要即时计算的，而这会造成额外算力。</p><p>因而当我们将权重量化改为对称量化时(zW=0)，则上式简化为如下，即时计算时，只需要计算第一项的矩阵乘法，第二项是预先算好的偏置项：</p><p><img src="https://static001.geekbang.org/infoq/11/11dcec1e03ae66ab2d595f2b17e07a69.png" /></p><p></p><p>而当两者都是对称量化时的表达式，则简化如下：</p><p><img src="https://static001.geekbang.org/infoq/97/97f4f242c70ea5ef73a9c51a5fdf0ad3.png" /></p><p></p><p>对比原模型中的浮点计算W{x}，W{int}x{int}是整型与整型之间的乘法，后者在Nvidia GPU上的运算速度远快于前者，这是量化模型的推理速度大大加快的原因。</p><p>三、LLM的量化</p><p>Challenges in LLM Quantization</p><p>从模型表现的角度来讲，量化自始至终要解决的一个前提是，如何保持量化后模型的精度，即让模型的使用者觉得量化后的模型在推理效率提高的同时，还能保持原来的性能。</p><p>神经网络中需要量化的操作主要是卷积层Conv(x;W)和全连接层Wx，即主要是按上一部分描述的操作分别对W和x做的权重量化（Weight Quantization,WQ）和激励量化(Activation Quantization,AQ)。</p><p>而不同于CNN模型或者小型Transformer模型，基于Transformer的大模型的矩阵乘法产生的激励张量通常有较多的离群值(outliers)，即离值分布的大多数点形成的点群较远的值, 这些绝对值较大但占比较低的元素值增加了量化难度。而如何取舍outliers通常是量化工作中的一大难点，若过分考虑之，则会因量化范围过大而降低量化的表达范围，若过分截断之，通常会因这些绝对值较大的值，在模型推理中对结果有较大影响，而导致模型效果变差，而后者在LLM的量化则尤为明显。</p><p>下图分别是Resnet18与Opt-13B的某层输入张量的元素值统计，sigma表示各自分布的标准差，Resnet18输入的极大值约为28sigma，且绝对值6sigma以外的比例在0.05%；而Opt-13B网络输入的极大值越为325sigma，且绝对值6sigma以外的比例在0.2%。从量化效果而言，Resnet18的int-8精度基本无损失，而Opt-13B的int-8模型的精度已崩塌。</p><p><img src="https://static001.geekbang.org/infoq/92/92d07af440553ecea102058adaa93917.png" /></p><p></p><p>【图片出处：An Introduction to Quantization of Large Language Models,p20 】</p><p>在应对激励量化的挑战这方面，有一些方案尝试降低量化精度，比如SmoothQuant提出的思路。</p><p><img src="https://static001.geekbang.org/infoq/d9/d998ea52768e0d2b58dd5ce466b72317.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/4d/4dfa64ad5a90847943a29fcdbb180804.png" /></p><p></p><p>【图片出处：SmoothQuant,p4】</p><p>在矩阵乘法中，他们通过按比例缩小输入张量X的值，而将缩小的比例补偿给权重张量W，即把问题从量化X和W转化为了量化 X·diag(s^(-1))和diag(s)·W。从而在保证乘法运算的积保持不变的前提下，降低张量X的量化难度。而在实际工程中，这种量化方案引起的量化误差对大模型的推理效果仍然有比较明显的影响，即使在int-8精度量化亦有明显的误差。如以下对Llama2-7B的SmoothQuant应用结果显示其perplexity非常糟糕，难以在实际中应用。</p><p><img src="https://static001.geekbang.org/infoq/4a/4ad07766fc8bdbe97b07aef8f9016397.jpeg" /></p><p></p><p>所以在目前工程部署中的实用方案，大多以weight-only的量化方案为主，即放弃activation的量化。</p><p>GPTQ</p><p>GPTQ是最早被工程化部署所接受的量化方案，W8A16或W4A16的量化效果在多数场景中都有与原模型较为接近的表现，而且其量化过程非常快。</p><p>量化过程</p><p>以矩阵乘法的基本单元操作为例，基于 weight-only量化前后的乘积的均方差，可以写出如下优化函数，</p><p><img src="https://static001.geekbang.org/infoq/69/69f62e56ea3c1c66fde74db9aaa6c4eb.png" /></p><p></p><p>W 是在Transformer 中的Linear层权重，X表示其对应的输入。离线量化的过程是逐模块（Transformer）逐层（Q,K,V,O,Fc1,Fc2）做量化。</p><p>参数和数据定义如下：</p><p>W∈R^{K×M}，X∈R^{M×N}，Y=W×X∈R^{K ×N}calibrate set：部分数据用作推理，用于查看各层输入张量的值范围，并基于此量化。</p><p>具体量化过程如下：</p><p>计算Hessian（上述优化函数对于W_hat的Hessian，而非反向传播中的Hessian），加入扰动项：</p><p><img src="https://static001.geekbang.org/infoq/c1/c183164f545ee821018b8ec2f0c31b43.png" /></p><p></p><p>act order sort（desc_act，值范围相近的column一起做量化），基于diag(H)对W基于M维度作列重排，同理，对应地H在两个维度上重排。求逆H^(-1)（cholesky分解）。对W沿维度M，从左到右逐块量化，block size B=128，其右侧还未量化部分基于H^(-1)更新，以补偿量化损失。</p><p><img src="https://static001.geekbang.org/infoq/e9/e9cf2ef4febb053c6ce3263a73906964.png" /></p><p></p><p>（inner loop）针对每个block内部，逐列量化，计算误差，并对该block内部未量化的列，基于误差更新。</p><p><img src="https://static001.geekbang.org/infoq/3f/3f2e9f6ebc3332e2ce71133a0f7a0f79.png" /></p><p></p><p><img src="https://static001.geekbang.org/infoq/a5/a5a03be4b32417abae3b7d2596f956fe.png" /></p><p></p><p>（outer loop）操作完该block，更新其后面的所有列：</p><p><img src="https://static001.geekbang.org/infoq/32/326cf6a97263b2fcec4c74dd3b7b7f72.jpeg" /></p><p></p><p>group_size</p><p>若不指定group size，默认g=-1，以所有列为单位统计量化参数，并对每一行的权重做量化，对于W∈R^{K×M}，量化参数的数量为K×1。</p><p><img src="https://static001.geekbang.org/infoq/45/45e69320558abdfd8b8345ad7f004221.jpeg" /></p><p></p><p>若指定group size，例如g=128，则会以每128列为单位统计量化参数，并对每一行的权重做量化，对于W∈R^{K×M}，量化参数的数量为K×(M/g)。</p><p><img src="https://static001.geekbang.org/infoq/51/5198a9f1cccb9b620c73fa7ced7830d8.jpeg" /></p><p></p><p>重排desc_act</p><p>根据Hessian Matrix H，基于diag(H)对W基于M维度作列重排。其目的是优先量化绝对值较大的activaiton对应的weight的列，这些列在推理中被视为更为影响结果的重要的列，因而希望在量化这些列时尽可能产生较小的误差，而将更多的量化误差转移到后面相对不重要的列中。</p><p>部分实验表明desc_act对量化损失的效果在多数的任务中是有效的trick。</p><p><img src="https://static001.geekbang.org/infoq/4e/4e847a1a3a177bdb223d6e1a32d69fe6.png" /></p><p></p><p>Perplexity of Pygmalion-7B with GPTQ [7]</p><p>【图片出处：https://huggingface.co/reeducator/vicuna-13b-free/discussions/22】</p><p>算子</p><p>严格来说基于weight-only的W4A16相比于原本的W16A16并没有太多效率的提升，而且推理中还加入了quant/dequant过程；而随着weight-only成为LLM量化的主流且应用越来越多，有很多开源的工作基于W4A16高效算子的编写为量化算法的推理提速赋能，比如GPTQ的python package&nbsp;AutoGPTQ已集成于开源工具exllama，后者基于triton和CUDA重写了量化乘法的并行计算。在exllama/exllama_ext/matrix.cuh可以看到dot_product8_h对out=W_hat·x=(W{int}-z)s·x=(W{int}-z)x·s的实现。</p><p><img src="https://static001.geekbang.org/infoq/27/271d6606fe590ac9637cc000cf8141de.jpeg" /></p><p></p><p>【图片出处：https://github.com/turboderp/exllama/blob/3b013cd53c7d413cf99ca04c7c28dd5c95117c0d/exllama_ext/matrix.cuh#L86】</p><p>AWQ</p><p>相比于GPTQ从最优化问题出发设计方案，AWQ是基于搜索提出的量化方案。</p><p>用Q(·)表示量化反量化过程，则修改前的量化过程如下：</p><p><img src="https://static001.geekbang.org/infoq/4f/4f2f9676ac2c22f87f338bd454cab825.png" /></p><p></p><p>修改后，量化过程如下，加入了对W的缩放：</p><p><img src="https://static001.geekbang.org/infoq/35/35ed9998ce521495d5758d44c2764808.png" /></p><p></p><p>搜索</p><p>AWQ 的全称为Activation-aware Weight Quantization, 即对Weight的量化过程考虑Activation的值的影响。其出发点也是基于在Weight的各个通道中，处理对应的Activtion的值较大的通道则相对重要，反之则相对不重要，进而通过乘以一个缩放系数Δ去体现其重要性，而Δ的值和范围则通过输入的activation的张量值设计。</p><p><img src="https://static001.geekbang.org/infoq/65/6539e94da31664f5b533cf980c72f586.png" /></p><p></p><p>搜索的衡量标准依据Linear层量化前后输出结果的比较，取MSE结果最小者为最优解。</p><p><img src="https://static001.geekbang.org/infoq/e2/e20e88d34ac24710150c27aa27d24595.png" /></p><p></p><p>效果</p><p>从模型表现效果方面，通过逐层 scale search 寻找最优的缩放系数，从而取量化误差最小的解，以下来自AWQ paper的效果比较，从Perplexity的角度，显示在两代Llama的测试上其量化结果稍优于GPTQ及GPTQ的排序版。</p><p><img src="https://static001.geekbang.org/infoq/55/5573a48245c16726b9a8deefb9522479.png" /></p><p></p><p>【图片出处：AWQ, p6】</p><p>从实际任务的准确率来看，AWQ的准确率与GPTQ的act_order版本（GPTQ-R）相当，而速度优于后者。</p><p><img src="https://static001.geekbang.org/infoq/4d/4d1cfda551de1612636efb809f2154d0.png" /></p><p></p><p>【图片出处：AWQ, p5】</p><p>从模型的计算性能方面，GPTQ因为有reorder操作，矩阵乘法是MV（matrix×vector），为不连续的内存访问，而AWQ不存在reorder操作，矩阵乘法为（matrix×matrix），速度更快。</p><p>四、总结</p><p>关于LLM的量化工作目前的SOTA performance，基本上都是基于weight-only 的量化模式，模型在GPU运行所需的显存降低是其主要的贡献。</p><p>从模型的表现来看，因为存在不可避免的量化损失，且LLM模型通常比传统的CNN模型对量化要敏感得多，虽然在很多任务上量化后的LLM表现与量化前差距不大，但是在一部分任务上可能依然无法胜任。</p><p>从模型的加速来看，weight-only的量化促使底层加速的工作基本上都在W4A16、W3A16、W8A16等乘法算子上的加速，从paper上提供的理论数据上来看通常相较于FP16模型只有 1.x ~3.x 倍速度的提升，而实际部署效果可能低于此数值，其加速效果远不如传统量化方法的W4A4、W8A8等全整型的乘法算子。</p><p>总体来说，LLM领域的量化工作还很初步，若在实际任务中对模型的表现精度要求十分高，更推荐单纯基于KV cache等方向提高单位显存吞吐量的算法和工具，如Flash Attention-2、Paged Attention等。</p><p>五、Reference</p><p>1. A Simple and Effective Pruning Approach for Large Language Models, 2023.</p><p>2. Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning, 2023.</p><p>3. A White Paper on Neural Network Quantization, 2021.</p><p>4. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models, 2023.</p><p>5. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, 2023.</p><p>6. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration, 2023.</p><p>7. Some evaluation on GPTQ performance.</p><p></p><p>*文/xujiong</p><p></p><p>本文属得物技术原创，更多精彩文章请看：得物技术官网</p><p></p><p>未经得物技术许可严禁转载，否则依法追究法律责任！</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/uKouBwPIKIime1Vl0U45</id>
            <title>曝谷歌Python团队全员被裁；清华系团队“国产Sora”：视频突破16秒；“社恐”周鸿祎：喊话贾跃亭、雷军送自己车｜AI周报</title>
            <link>https://www.infoq.cn/article/uKouBwPIKIime1Vl0U45</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/uKouBwPIKIime1Vl0U45</guid>
            <pubDate></pubDate>
            <updated>Tue, 30 Apr 2024 01:12:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 谷歌, Python, 裁员, 团队
<br>
<br>
关键词: 杨植麟, 月之暗面, 套现, 消息不实
<br>
<br>
关键词: 拜登, TikTok, 法案, 出售计划
<br>
<br>
关键词: 特斯拉, 毁约, 应届生, HR
<br>
<br>
总结: 谷歌 Python 团队遭裁员，月之暗面否认套现传闻，拜登签署涉TikTok法案，特斯拉被曝毁约应届生。 </div>
                        <hr>
                    
                    <p></p><h2>热门资讯</h2><p></p><p></p><p></p><h4>谷歌 Python&nbsp;团队全员被裁</h4><p></p><p></p><p>谷歌 Python 工程师、CPython 核心开发者兼 Python 指导委员会成员 Thomas Wouters ，昨天晚上他在社交媒体发布动态称：包括自己在内的同事、主管均已被裁员。从他的描述来看，公司并没有直接解雇他们 —— 而是要求调岗到国外的团队。有其他网友补充道，是谷歌将 Python 团队负责的工作合并到另一个团队中，并让原来的团队离开。</p><p></p><p><img src="https://static001.geekbang.org/infoq/8a/8ae77b5959f7d65724dbddab99b0f633.png" /></p><p></p><p>“谷歌 Python 团队全员被裁” 消息很快就传遍社交媒体和开发者社区。有网友表示对谷歌 Python 团队的工作感到好奇，认为让一个团队专门研究一种编程语言是没有意义的。</p><p></p><p>对此，谷歌 Python 团队内部员工回应，除了为上游 Python 做出贡献之外，团队还要在 google 中维护了一个稳定的 python 版本，并确保 monorepo 中的所有内容都可以使用它、维护工具、针对谷歌风格指南和整体代码库高度定制工具，开发和维护 Python 的构建系统规则、对数亿行代码执行自动重构等。</p><p></p><p></p><h4>杨植麟套现数千万美元？月之暗面回应：消息不实</h4><p></p><p></p><p>据悉，上一轮融资完成后，月之暗面（Moonshot AI）创始人杨植麟通过售出个人持股已套现数千万美金。有知情人士表示，月之暗面最近这轮融资涉及一些老股交易，但红杉等老股东都没有出售股份。“有传言创始人及相关人员套现金额在 4000 万美金。”另有业内投资人表示，“公司成立第一年就套现这么多，这种情况并不多见。”</p><p></p><p>4 月 23 日，针对创始人杨植麟通过售出个人持股“套现数千万美元”的消息，月之暗面方面回应称，上述消息不实，月之暗面此前已公布员工激励计划。根据月之暗面官方公众号 3 月 11 日发布的信息，从 2024 年开始，该公司将在取得重要进展时发起员工期权回购计划，确保团队成员能够分享公司发展的果实（2024 年底启动首次期权回购计划）。此外，公司每年会定期根据工作表现进行调薪和期权增发，确保薪酬和期权充分反映出员工个人的成长和贡献。</p><p></p><p>“公司希望长期发展，不是短期套利路线。”月之暗面方面表示，不过对于发布股权激励计划与创始人是否套现之间更为直接的关系，公司方面暂未透露。</p><p></p><p></p><h4>拜登签署涉 TikTok 法案，字节：没有出售计划</h4><p></p><p></p><p>4 月 25 日，字节跳动方面表示，外媒有关字节跳动探索出售 TikTok 的消息不实，字节跳动没有任何出售 TikTok 的计划。</p><p></p><p><img src="https://static001.geekbang.org/infoq/72/727463d296dcee1cb71eb582539c3ee4.png" /></p><p></p><p>当地时间 4 月 24 日，美国总统拜登签署了价值 950 亿美元的一揽子对外援助法案，该法案还涉及强制字节跳动剥离旗下应用 TikTok 美国业务。在相关条款中，字节跳动被限期约九个月剥离其美国业务，否则将面临美国全国禁令。此外，如果拜登确定出售事宜取得进展，他还可行使一次 90 天的延期权力。</p><p></p><p>与字节跳动关系密切的知情人士表示，TikTok 业务运营所依赖的算法被认为是字节跳动整体业务的核心，这使得字节跳动极不可能连带着算法将 TikTok 出售。TikTok 只占据字节跳动总营收和日活跃用户的一小部分，因此在最坏情况下，字节跳动宁愿在美国关闭这款应用，也不愿把它卖给潜在美国买家。而且，关闭 TikTok 对字节跳动业务的影响有限，该公司也不必放弃其核心算法。</p><p></p><p>据悉，今年目前为止，TikTok 及其母公司字节跳动总共花费了逾 700 万美元 (约合 5072 万元人民币) 资金，以试图阻止美国会通过可能在美国封禁 TikTok 的立法。最新公开的游说披露报告显示，今年头三个月，仅字节跳动就在 TikTok 的内部说客身上花费了创纪录的 268 万美元（约合 1941 万人民币），目的是游说国会和联邦官员。还有数据显示，2023 年字节跳动和 TikTok 的游说支出超过 800 万美元，另外 TikTok 今年在电视和数字广告活动上花费了 450 多万美元。</p><p></p><p>尽管拜登已经把 TikTok 剥离法案签署成为法律，要求字节跳动限期出售 TikTok 美国业务，但是这位美国总统将继续使用 TikTok 协助他的连任竞选活动。拜登竞选团队的一名官员表示，拜登要确保年轻选民在今年 11 月的大选前收到他的信息，TikTok 就是传播这些信息的方式之一。</p><p></p><p>据悉，拜登竞选团队在今年超级碗比赛举行当天，也就是 2 月 11 日发布了第一个 TikTok 视频，该视频的点击量超过 1000 万次。但是，在其竞选团队之后发布的 149 个视频中，不到一半的视频浏览量低于 10 万，只有 9 个视频的点击量超过了 100 万。</p><p></p><p></p><h4>特斯拉被曝毁约应届生，蔚来、极氪 HR“在线抢人”</h4><p></p><p></p><p>4 月 24 日消息，“特斯拉被曝毁约应届生”的话题登上微博热搜，有多名网友在社交平台上表示在拿到入职 offer 之后遭到了特斯拉单方面解约。对此，有被特斯拉解约的应届生表示，他是 4 月 24 日上午接到了 HR 的电话，被告知因公司政策临时调整，岗位被取消，将赔偿一个月的工资，但是春招基本结束，找工作不太好找。</p><p></p><p>另有拿到销售岗位 offer 的应届生表示，原定 4 月中旬的入职培训，入职办理均被告知取消，HR 在电话中称如果后续有需要，可以重新面试。针对毁约一事，特斯拉 HR 回应称，临时收到招聘需求调整和变化的通知，需要进行人员优化。</p><p></p><p>据悉在相关话题的评论区中，已有不少如蔚来、极氪等企业 HR 招聘账号表示，欢迎被解约的应届生投递简历。</p><p></p><p></p><h4>“忙碌”的周鸿祎：喊话贾跃亭、雷军给自己送车</h4><p></p><p></p><p>4 月 24 日，贾跃亭发布视频回应周鸿祎称为盗窃者鼓噪不应该是你的价值观，向年轻奋斗者倡导的不应该是抄袭和山寨。FF 虽然只交付 11 台车，但是是完全原创创新的塔尖产品，有非凡的颠覆性意义。贾跃亭称会在合适的时机，把 FF91 带回中国制造。随后，周鸿祎再次回应称，贾总别光讲 ppt，先送辆车到 360 大厦小广场让我试试。</p><p></p><p>此前周鸿祎就表示，“我不是为贾会计（指贾跃亭）说话，他带着图纸到美国那么长时间也没造出几辆车，高合汽车反而造出来了，他却反过来告人家。我觉得他犯了一个错误，（特斯拉 CEO）马斯克造车都要来中国设厂，他却去美国造车，完全没有搞明白中国的优势在哪里。”</p><p></p><p>此外，周鸿祎发文称自己借机向雷军求“送一辆小米 SU7”体验一下被雷军拒绝后，大量网友开始玩梗喊话雷军送一台给周鸿祎，但交车地点必须是朝阳公园东 5 门。</p><p></p><p>据悉，该梗出自 2012 年小米手机一举成名后，周鸿祎开始进军手机行业并在与小米在微博上发生了一场骂战。雷军将周鸿祎比喻成“东方不败”，指责周鸿祎是炒作，并且用抄袭的办法做智能手机。360 方面则反驳说，小米手机的项目，也是雷军从魅族“偷”来的思路。</p><p></p><p>彼时，周鸿祎一度约架雷军称，约你见面谈一谈，下周一上午十点朝阳公园门口见。在小米首部授权传记《一往无前》中提及了这次约架，其中提到，雷军不但亲自在微博上迎战，还对约架事件进行了认真部署。他让黎万强带着市场部的刘飞、钟雨飞等几位同事到朝阳公园东门踩了点，认真探究了站位和撤离路线。在雷军心中，他真的准备大干一场。不过最终这次约架不了了之。对此，有不少网友表示，雷总真的是从来不打无准备的仗。</p><p></p><p>这周，360 集团创始人周鸿祎很忙：参加完北京国际车展、中关村论坛“投资北京”大会做演讲。虽然不会开车，也没有造车，但周鸿祎最近却成为汽车圈的话题人物。在北京国际车展，他更是爬上一辆越野车顶，成为了全场焦点。有网友评价称，“周鸿祎成为了北京车展历史上最老的车模”。他在微博上表示自己“有点社恐”，为证明自己是真人，所以爬上车顶。</p><p></p><p></p><p><img src="https://static001.geekbang.org/infoq/82/8276028740cd3afdacf10f5bb0365f88.png" /></p><p></p><p></p><h4>百度突然宣布：百度百科 App 将关闭服务</h4><p></p><p></p><p>4 月 23 日，根据百度百科 App 下线通知，百度百科团队决定于 2024 年 6 月 30 日关闭百度百科 App 的服务，将在百度 App 中的百度百科小程序继续提供服务。</p><p></p><p>公开资料显示，百度百科是百度公司推出的一部内容开放、自由的网络百科全书。内容涵盖了包括经济、文化、历史事件、政治人物、科学技术等在内的几乎所有知识领域，是一个重要的信息获取和知识共享平台。百度百科自 2006 年 4 月 20 日上线以来，已经发展成国内最大的中文维基百科。数据显示，百度百科已收录了超 2800 万个词条，参与词条编辑的用户超过 780 万人，几乎涵盖了所有已知的知识领域，日均访问量高达 4 亿次。</p><p></p><p>作为百度旗下一知名应用功能，App 突然宣布下线，确实会令人感到意外。值得一提的是，在百度百科 App 宣布下架的前一天，APP 还进行了一次更新，主要提示修复了一些 bug。</p><p></p><p></p><h4>美国全面取缔竞业协议</h4><p></p><p></p><p>当地时间 4 月 23 日，美国联邦贸易委员会（FTC）宣布，全面禁止所有员工（包括高级管理人员）签署新的竞业禁止协议。对于现有的竞业协议，高级管理人员的现有竞业协议仍然有效，其他员工则在规定生效日期后不再强制执行。这一规则将在公布 120 天后生效，对于“高级管理人员”的定义则是收入超过 151164 美元且处于“政策制定职位”的人。</p><p></p><p>根据 FTC 的估计，禁止竞业协议将带来以下影响：1、新企业成立率提高 2.7%，每年新增 8500 家新企业；2、每年平均增加 17000-29000 项专利，十年间每年专利增速为 11-19%；3、未来十年员工的平均年收入将额外增加 524 美元（约合 3796 元人民币）。</p><p></p><p>有科技行业从业者表示：“对于任何在美国从事科技工作的人来说，这都是一个重大消息。”</p><p></p><p>SteveDouglas 的 CEO Matt Shore（马特·肖尔）表示，这项禁令通过后，领导层已经需要避免员工“因为工作体验不佳而出现大规模外流”。Shore 认为，如果顶尖人才因竞业协议松绑而选择离去，那么企业必然需要拿出更有竞争力的薪酬方案来加以挽留。</p><p></p><p>延伸阅读：<a href="https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;mid=2651203552&amp;idx=1&amp;sn=ffda7f1c1d208cca028b698fa4be45ac&amp;scene=21#wechat_redirect">全世界 IT 人苦竞业久矣！今天，美国全面废除竞业协议</a>"</p><p></p><p></p><h4>清华大学成立人工智能学院，首任院长系图灵奖获得者姚期智院士</h4><p></p><p></p><p>清华大学成立人工智能学院，以发展人工智能核心基础理论和架构，以及“人工智能 + X”为重点。该学院旨在培养 AI 顶尖人才，推动原始创新，由图灵奖得主、中国科学院院士姚期智担任首任院长。学院将依托国家战略，创新人才培养模式，吸引世界顶尖人才，突破关键核心技术，打造合作平台，推进产学研用协同创新，努力成为中国人工智能发展的引领者，建设成为世界顶尖的人工智能人才和创新高地。</p><p></p><p></p><h4>英伟达收购两家 AI 创企，要让 AI 芯片更便宜</h4><p></p><p></p><p>4 月 25 日，英伟达宣布达成一项最终协议，收购 Run:ai。据 CTech 报道，这笔交易价预计约为 7 亿美元。同时，英伟达还收购了另一家以色列 AI 公司 Deci。其中，Run:ai 约有 150 名员工，累计融资 1.18 亿美元；Deci 拥有约 100 名员工，累计融资 5500 万美元。英伟达与 Deci 的交易未公开披露，交易价未知。</p><p></p><p>据外媒报道，被英伟达最近达成收购交易的这两家创企，旨在降低开发或运行生成式 AI 模型的成本，帮助客户更有效地利用 AI 计算资源，进而提振英伟达 AI 芯片的需求。</p><p></p><p></p><h4>马斯克 AI 公司接近达成 60 亿美元融资，特斯拉市值一夜大涨 4028 亿元</h4><p></p><p></p><p>有市场消息称，马斯克可能即将获得数十亿美元的资金，以将其聊天机器人 Grok 打造成为 ChatGPT 的劲敌。两位知情人士透露，马斯克麾下初创公司 xAI 正在融资 60 亿美元，在不包括这笔投资的情况下，对该公司的估值为 180 亿美元。其中一名知情人士表示，红杉资本是参与本轮 xAI 融资的投资者之一，另一名知情人士表示，预计融资将在未来两周内完成。在人工智能赛道上，这轮融资是规模最大的单笔融资之一。</p><p></p><p>北京时间 4 月 25 日凌晨，特斯拉大涨超 12%，创 2022 年 1 月以来最大单日涨幅，市值大涨 556 亿美元（约合人民币 4028 亿元），总市值重返 5000 亿美元上方。</p><p></p><p>虽然特斯拉 23 日公布的财报未达预期，但表示已经更新未来的车型阵容，将新车型的推出时间提前，原先对外宣布的时间是 2025 年下半年开始生产。</p><p></p><p></p><h2>IT 业界</h2><p></p><p></p><p></p><h4>阿里巴巴发布了首个千亿参数的大模型 Qwen1.5-110B</h4><p></p><p></p><p>阿里巴巴发布了首个千亿参数的大模型 Qwen1.5-110B。此前它发布了 0.5B、1.8B、4B、7B、14B 和 72B 不同规模参数的版本。阿里巴巴称，Qwen1.5-110B 模型在基础能力评估中与 Meta-Llama3-70B 相媲美，在 Chat 评估中表现出色，包括 MT-Bench 和 AlpacaEval 2.0。Qwen1.5-110B 与其他 Qwen1.5 模型相似，采用了相同的 Transformer 解码器架构。它包含了分组查询注意力（GQA），在模型推理时更加高效。该模型支持 32K tokens 的上下文长度，同时它仍然是多语言的，支持英、中、法、西、德、俄、日、韩、越、阿等多种语言。</p><p></p><p></p><h4>最强国产 Sora，清华团队生数科技突破 16 秒长视频生成</h4><p></p><p></p><p>4 月 27 日，在中关村论坛未来人工智能先锋论坛上，生数科技联合清华大学正式发布中国首个长时长、高一致性、高动态性视频大模型——Vidu。该模型采用团队原创的 Diffusion 与 Transformer 融合的架构 U-ViT，支持一键生成长达 16 秒、分辨率高达 1080P 的高清视频内容。Vidu 不仅能够模拟真实物理世界，还拥有丰富想象力，具备多镜头生成、时空一致性高等特点。Vidu 是自 Sora 发布之后全球率先取得重大突破的视频大模型，性能全面对标 Sora，并在加速迭代提升中。</p><p></p><p>生数科技是一支清华背景的大模型创业团队，致力于专注于视频生成、3D 生成、图像生成等多模态领域。据悉，Vidu 的快速突破源自于团队在贝叶斯机器学习和多模态大模型的长期积累和多项原创性成果。其核心技术 U-ViT 架构由团队于 2022 年 9 月提出，早于 Sora 采用的 DiT 架构，是全球首个 Diffusion 与 Transformer 融合的架构，完全由团队自主研发。</p><p></p><p>清华人工智能研究院、清华人工智能研究院副院长朱军博士对媒体表示：Vidu 的视频时长会继续突破，“另外，我们的架构是支持多模态的，视频模态只是当前阶段最重要的。”据生数透露，Vidu 目前正在加速迭代提升，面向未来，Vidu 灵活的模型架构也将能够兼容更广泛的多模态能力。言下之意，还说生数科技是“中国 sora”，就有点太没想象力了。</p><p></p><p></p><h4>Meta 开放 Quest 头显操作系统</h4><p></p><p></p><p>Meta 为应对苹果 Vision Pro 带来的竞争压力，决定向其他硬件制造商开放自家的 VR 操作系统，并发布了 Meta Horizon OS，同时欢迎华硕、联想、Xbox 等知名科技品牌加入，共同构建多元化的 VR 硬件生态。此外，Meta 还对应用生态系统进行改造，以吸引更多用户投入 Meta 的 VR 世界。</p><p></p><p></p><h4>苹果发布 OpenELM，基于开源训练和推理框架的高效语言模型</h4><p></p><p></p><p>4 月 24 日消息，在 WWDC24 之前，苹果在 Hugging Face 平台上发布了一个“具有开源训练和推理框架的高效语言模型”，名为 OpenELM，其源码及预训练的模型权重和训练配方可在苹果 Github 库中获取。</p><p></p><p>根据官方介绍，大型语言模型的可重复性和透明性对于推进开放研究、确保结果的可信度以及调查数据和模型偏差以及潜在风险至关重要。OpenELM 使用分层缩放策略，可以有效地分配 Transformer 模型每一层的参数，从而提高准确率。例如，在参数量约为 10 亿的情况下，OpenELM 与 OLMo 相比准确率提升了 2.36%，同时所需的预训练 tokens 数量仅有原来的 50%。</p><p></p><p></p><h4>微软发布小模型：与 GPT-3.5 能力不相上下</h4><p></p><p></p><p>4 月 23 日消息，微软发布了一种具有成本效益的小型语言 AI 模型，可以创建社交媒体帖子等任务，同时使用较少的数据量。微软在一份声明中称，该 AI 模型被称为“Phi-3-mini”，在评估语言、编码和数学能力等一系列基准测试中，其表现甚至可以超越那些体积相当于其两倍的 AI 模型。</p><p></p><p>微软 Azure AI 平台企业副总裁 Eric Boyd 称，Phi-3-mini 的能力与 GPT-3.5 这样的大语言模型不相上下，只是体积更小。与大型 AI 模型相比，小型 AI 模型通常运行成本更低，在手机和笔记本电脑等个人设备上表现更好。</p><p></p><p>微软称，这种小型 AI 模型旨在执行一些更简单的任务，使其更容易被资源有限的公司使用。例如，一家小公司可以使用 Phi-3-mini 来总结一份长篇文件的要点，从市场研究报告中提取相关的见解和行业趋势。</p><p></p><p></p><h4>商汤科技发布日日新 5.0 大模型，全面对标 GPT-4</h4><p></p><p></p><p>4 月 24 日，商汤科技发布日日新 SenseNova 5.0 大模型，采用 MOE 混合专家架构，拥有超过 10TB tokens 训练，推理上下文窗口达到 200K，全面对标 GPT-4 Turbo。该模型体系提供自然语言处理、图片生成、自动化数据标注等功能，并已应用于中文语言大模型应用平台和生成式 AI 模型，如 AI 文生图创作、数字人生成等，实现了大模型按需所取，加速生成式 AI 在产业落地。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/Qc2Wfg8X8No3R7QpQlR7</id>
            <title>26岁带着百人团队冲刺大模型，面壁智能天才CTO：高效比参数更重要</title>
            <link>https://www.infoq.cn/article/Qc2Wfg8X8No3R7QpQlR7</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/Qc2Wfg8X8No3R7QpQlR7</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Apr 2024 10:07:55 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 面壁智能, 大模型, 曾国洋, 技术团队
<br>
<br>
总结: 面壁智能是一个致力于大模型研发的技术团队，由曾国洋领导。团队注重高效、灵活和创新的工作状态，不追求参数数量，而是致力于突破模型的智能极限，提高模型性能。他们推出了多款大模型，如CPM-Bee和CPM-Cricket，并积极探索端侧模型的应用场景，致力于为用户带来更大的价值和商业空间。 </div>
                        <hr>
                    
                    <p></p><blockquote>出品｜InfoQ 《大模型领航者》访谈主持｜霍太稳，极客邦科技创始人兼 CEO访谈嘉宾｜曾国洋，面壁智能 CTO作者｜褚杏娟</blockquote><p></p><p></p><p>“尽管有所谓的‘百模大战’，但实际上，国内真正能够成功训练大模型并掌握相关技术的团队并不多。”面壁智能 CTO 曾国洋说道，“不是简单地训练出一个模型就意味着掌握了全部技术。”</p><p></p><p>面壁智能起于一群学术极客。2021 年，清华大学计算机系长聘副教授刘知远的牵头成立了面壁智能成立，团队成员主要来自清华大学 NLP 实验室，而曾国洋成为这家初创公司的技术 1 号位。</p><p></p><p>曾国洋如今更以“天才少年”的形象被人熟知：8 岁学编程、高中去旷视实习、大二加入清华 NLP 实验室。人们通常很难将眼前这个 98 年的少年，跟“BMTrain、BMInf 主要作者”“OpenBMB 开源社区发起人”“当红大模型创业公司 CTO”等联系在一起，但 26 岁的他确实已经被推到了大模型时代的舞台中央。</p><p></p><p></p><p></p><p></p><h3>从自己 coding 到看别人 coding</h3><p></p><p></p><p>2022 年 8 月，面壁智能开始公司化运作。直到去年年初，面壁智能只有 10 个人不到。当时的曾国洋依然活跃在编程一线。</p><p></p><p>作为程序员的曾国洋，是早期第一批申请试用 GitHub Copilot 的用户之一。他把 AI 看成是合作伙伴：AI 辅助程序员完成某些任务，而程序员则可以专注于更具创造性和战略性的工作。</p><p></p><p>“我很喜欢能够帮助加速编程的工具，”曾国洋说道，“我们不应该简单地认为只要代码被写出来，程序员的工作就完成了。编写代码只是程序员工作的一部分，如何将想法架构化以及合理划分模块并确保它们之间的有效协作等，都是程序员工作中相当重要的一部分。”</p><p></p><p>去年 5 月份后，面壁智能的规模越来越大，内部也设立了数据处理、模型训练、模型评测、算法、Infra、运维等不同的团队，以便更好地训练大模型。他的工作重心逐渐转为保证组织的有效协作。</p><p>在此期间，面壁智能迎来了许多对通用人工智能（AGI）充满激情和信仰的年轻人，“他们对 AGI 有浓厚的兴趣和追求，甚至愿意降薪过来。”</p><p></p><p>但在爆火之前，大模型并没有被广泛关注和应用，因此有相关经验的人才很少。这意味着几乎所有人都是从头开始学习和探索大模型。因此，团队在招揽新人时并不把大模型经验放在首位，而是更看重候选人的学习意愿、对新技术的热情、以及创新和解决问题的能力。</p><p></p><p>如今，面壁智能已经拥有超 100 人的科研团队，平均年龄 28 岁。这支团队的“清北”含量 80%，此外还有来自阿里、字节、百度等一线公司的骨干。</p><p></p><p>面壁智能没有给技术团队设立严格遵循 KPI 的管理形式，也没有在每一个非常具体的时间点设定明确规划，只是制定了一个大概的发展节奏和方向，因为合作的都是顶尖聪明的同事，而聪明人是会自己给自己定目标的。“我们要做的不是个人明星，而是明星团队，让聪明人能更好地合作、互相创造价值，一起创造更伟大的价值。”</p><p></p><p>面壁智能倾向“小而美”的技术团队。曾国洋强调，“小而美”并不是说团队规模小，而是指团队能够保持高效、灵活和创新的状态，成员能够频繁交流、头脑风暴，共同推动项目发展。对于技术创业公司来说，这样的团队更加敏捷和灵活，更容易产生新的思想和创新。每个成员能充分发挥自己的专长和创造力，同时快速响应市场变化和技术演变。</p><p></p><p>大模型团队的研发速度可以用争分夺秒来形容。面壁智能团队之前以两周为单位的内部迭代频率已经成为过去式，如今的节奏已经将近一周一迭代了。不断演进期间，也让面壁智能对自己做的事情有了更深入的思考。</p><p></p><p></p><h3>不再一味追求参数</h3><p></p><p></p><p>国内庞大的市场规模为大模型创业提供了巨大的发展机遇，但 OpenAI 等国外公司的频繁迭代，确实也给了国内公司很大的技术压力。时至今日，很多公司的大模型发布出来时，都是对标的 OpenAI。</p><p></p><p>不过，曾国洋表示，“我们并不过分担忧落后的问题。”他分享了一段自己的经历：</p><p></p><p></p><blockquote>ChatGPT 刚刚发布时，大家都赞叹它强大能力并讨论需要投入多少资源才能追赶上。后来，我自己投入了一些资金，买了几百条数据训练我们的模型。那次训练完测试后，我感受到了 ChatGPT 的那种效果。这个瞬间让我意识到，我们离它实际上并没有想象中那么遥远。这个经历不仅让我自己感到振奋，也给了我们团队巨大的信心和动力。它证明了我们的努力和方向是正确的，只要我们继续坚持，完全有可能达到甚至超越行业领先者。</blockquote><p></p><p></p><p>曾国洋有作为技术人的自信和思考。</p><p></p><p>“我们将 OpenAI 的成就和国际市场的竞争态势当作一种衡量自己的标杆，但不会盲目跟随。我们清楚地认识到，OpenAI 的技术路线可能并不适合我们，我们需要根据自己的实际情况和优势来制定发展策略。”曾国洋说道。</p><p></p><p>回顾 2023 年，面壁智能一直略显低调地走在大模型潮头：当年 5 月，发布了百亿参数的 CPM-Bee 大模型；年中，推出了千亿参数多模态模型 CPM-Cricket，综合能力对标 GPT-3.5、超越 LLaMA 2。</p><p>但在 2021 年、2022 年，国内在大模型上进行了大量探索，但最终都没有出现一个像 ChatGPT 的突破性应用。这让面壁智能的技术团队意识到，一味地追求模型参数量行不通，训练出一个大模型也不是最难的部分，更难的是如何突破模型的智能极限，在用同等参数、同等数据量情况下，更快速低成本地跑出更好的模型性能。</p><p></p><p>在曾国洋看来，未来大模型的发展应该朝着高效率的方向发展：大模型要为用户带来更大的价值和更广阔的商业空间，而这主要取决于模型创造的价值和创造这一价值所需的成本。</p><p></p><p>今年 2 月份推出的 MiniCPM 模型就是面壁智能对大模型高效探索的样板间。发布会上，面壁智能 CEO 李大海提出了要“以小搏大”，曾国洋也表示 MiniCPM 用 2B 干掉 LLaMA 的 13B。这意味着，面壁智能正式进入小尺寸端侧模型的竞技场，并且还将其完全开源，以帮助大模型行业整体技术发展。</p><p></p><p>起初，端侧模型并不在团队计划中，但是在测试中发现并验证了这么高性能的模型可以在手机上顺畅运行，这给团队打开了新世界的大门：一旦模型能够在手机上运行，他们就能在端侧探索出更多应用场景，如汽车、VR、智能家居场景等。</p><p></p><p>端侧模型的优势在于，不需要频繁与云端服务器通信，因此处理速度更快；在本地设备上运行，不需要消耗大量的网络带宽和云计算资源，具有成本优势；可以在没有网络连接的情况下仍然发挥作用，这意味着其可以在各种环境下稳定运行。</p><p></p><p>端侧小模型的性能天花板也远未达到。在模型的极致效率方面，通过模型压缩、量化、剪枝等，性能可以进一步优化。其次，端侧设备本身也存在优化空间，硬件制造商可以考虑如何在硬件设计上更好地支持大模型运行。</p><p></p><p>“我有预感，像 GPT-3.5 这样高水平的模型，可能在一两年内就能在移动设备，比如手机上，完全运行起来。”曾国洋说道。</p><p></p><p>在面壁智能看来，大小模型的技术有互相打通、增进提升之处。面壁 MiniCPM 基座模型、多模态模型等“小钢炮”系列领先的端侧模型，都是基于公司千亿级模型研发路线延伸，将淬炼化的大模型训练方法下放至小模型训练中，来实现高效、低成本的模型训练与应用。</p><p></p><p></p><h3>“不会因别人而改变”</h3><p></p><p></p><p>变化，是大模型创业公司时刻要面对的问题。就像曾国洋常常被问到：Transformer 会不会突然被新的技术取代，从而让之前的投入都白费？</p><p></p><p>曾国洋对技术的快速变化并不过分担忧。“技术的发展是一个循序渐进的过程，不可能一夜之间出现一个全新的技术彻底颠覆现有的一切，而我们对此毫无准备。”在制定研发路径时，团队也是根据技术发展趋势和团队正在进行的工作，逐步调整目标和方向的。</p><p></p><p>对于市面上时不时蹦出来的热点模型或产品，曾国洋也表现得很冷静。</p><p></p><p>以 Sora 为例，曾国洋认为这显示出人们对创意性工作的兴趣，但对于是否跟随这一技术路线则需慎重。</p><p>“对于创业公司来说，需要格外考量战略目标与投入成本。即使是资金充裕的大公司，虽然有能力进行，但产出并不总是明确，短期内可能无法快速为大众提供实质性的服务。”曾国洋说道，面壁智能致力于将技术更好融入到实际产品和解决方案中。</p><p></p><p>对于前段时间刷屏的月之暗面 Kimi，曾国洋则一方面表示肯定，“Kimi 用户的增长迅速，表明它成功地解决了一些用户的痛点”，另一方面也反思自己，“可能没有充分利用我们在某些方面的先发优势。”</p><p>他特别提到了去年 5 月份面壁智能推出的一项读论文功能，虽然早就有了类似产品，但当时很可惜没有深入挖掘和清晰传达该功能可以解决的痛点。</p><p></p><p>但曾国洋强调，面壁智能的战略不会因为市场上的其他产品而改变。“我们一直在寻找大模型技术在普通人生活中的应用，并努力解决实际问题，而不仅仅是提供通用的解决方案。”</p><p></p><p>最近，李彦宏“开源模型会越来越落后”的观点也引起了很大的争议，有人“力挺”、有人“怒怼”。</p><p></p><p>对于面壁智能来说，开源是成立之初就做好的选择。正如李大海所说的：“我们一直是开源的受益者，所以也希望做出自己的贡献。并且，一款拥有良好口碑的开源模型，一定是经受住了方方面面的反复检验，在模型性能、体验等综合表现方面，拥有远超过 PPT 成绩的行业认可度。对于我们研发团队，一方面是 360 度无死角的考核压力，另一方面在挑战成功后也会有巨大的成就感。”</p><p></p><p>曾国洋坦诚，开源模型要追赶闭源模型确实会面临一定的挑战。这是因为在技术快速发展的过程中，闭源模型可能会因为有更好的知识产权保护和商业秘密而获得一定的优势。这种情况下，开源模型需要同时关注技术创新和与闭源模型保持竞争力。</p><p></p><p>但技术发展进入瓶颈期，那么开源和闭源模型可能就会在技术水平上趋于一致。在这种情况下，开源模型由于其开放性和社区的支持，会有更多的机会迎头赶上，甚至超越闭源模型。另外，开源模型的发展速度也取决于社区和市场的支持程度。如果有更多的个人和组织支持，那么开源生态的发展自然会更快。</p><p></p><p>此外也很重要的一点是，开源本身在技术影响力的建设方面是特别重要的，可以更好地让大众体验团队的技术实力，从而在人才吸引力和市场信心提升方面取得更强的竞争优势。</p><p></p><h3>“没有刻意区分 C 端和 B 端”</h3><p></p><p></p><p>对于以科研人员为核心创始团队的大模型创业公司来说，在战略、产品、经营等方面需要更强的专业管理者。2023 年，时任知乎 CTO 的李大海加入面壁成为 CEO，面壁向更为成熟的大模型商业公司迈下重要一步。</p><p></p><p>今年 4 月，面壁智能又完成了新一轮数亿元融资，由春华创投、华为哈勃领投，北京市人工智能产业投资基金等跟投，知乎作为战略股东持续跟投支持。除了通过融资获取资金外，面壁智能目前已经能够通过提供服务和产品实现一定的收入。</p><p></p><p>面壁智能是国内最早探索 Agent 的大模型公司之一。对于 Agent，每个公司、每个人的理解都不一样。在面壁智能看来，Agent 的边界还未被定义。“模型是底座是一切应用的基础，然后 Agent 是支撑应用很重要的中间层，”李大海认为，无论 to B 还是 to C，本质上都是“大模型 + Agent 的上层应用”。</p><p></p><p>曾国洋认为，Agent 实际上是介于纯大模型和通用人工智能（AGI）之间的一个中间状态或节点。Agent 的模型能力必须足够强大，才能有足够的智能理解和处理请求和执行任务。Agent 还需要与外部系统和接口进行交互，来不断拓展能力边界。同时，还能够调用已有的知识库来提供检索和回答服务。</p><p></p><p>而对于大模型领域，李大海曾表示，大模型应用可能会百花齐放，然而通用千亿大模型不会太多，可能只有极少数的几家公司能够最终突出重围。这基本也是行业的共识，基座模型的角逐注定是千军万马过独木桥。</p><p></p><p>那么，大模型公司做应用是对应用侧公司的一种降维打击吗？</p><p></p><p>曾国洋坦言，大模型公司由于其先进的技术和强大的数据处理能力，可能会对那些依赖传统技术或缺乏足够技术储备的应用公司产生影响。如果应用公司的技术壁垒不够坚固，就可能会在大模型技术的快速发展和迭代中受到冲击。例如 Jasper AI 这样的 AI 内容提供商可能会因为 OpenAI 发布了新的 ChatGPT 版本而受到影响。</p><p></p><p>然而，他也表示，应用公司也有自己的竞争优势，比如对特定市场的深入理解、强大的客户关系和品牌忠诚度等，这些都是他们的“护城河”。</p><p></p><p>2024 年，行业更加重视应用落地是当前的大趋势，因为目前模型已经基本可用了。李大海判断，从今年开始，大模型厂商会开始出现分层。但这个分层不是因为市场，更多是因为技术门槛：大家需要更强的模型、更高效率的推理，更好的 Agent 等，但不是每家公司都能跟得上这样的技术要求。</p><p></p><p>“大模型是一个行业级别的机会，哪怕不做基座大模型，做应用层也有非常多的空间。但不是每家公司都能够持续做基座大模型的训练，期间有些公司可能就会转型做其他的事情。能活下来的公司一定是技术和产品市场能力都很强的选手。”李大海表示。</p><p></p><p>对于未来的大模型应用，曾国洋提出了一种分工模式：一些简单的、重复性的任务由小型的、特定领域的模型来处理；而更复杂的、需要高级认知能力的思考任务则可能由大型通用模型来完成。</p><p></p><h4>结束语</h4><p></p><p></p><p>最初，人们普遍认为 AI 会先替代那些繁琐的工作，从而让人类有更多的时间从事创造性的工作。但现实情况似乎相反，AI 开始在创作领域发挥作用，而人类仍然在处理日常的工作任务。</p><p></p><p>但曾国洋观察，大模型简化、加速开发任务是正在发生的事情。在创建大模型应用企业的交流中，技术人员并不需要掌握大量的编程代码，更重要的是他们对最终产品的理解能力。</p><p></p><p>“对于想要有效利用大模型的人来说，掌握大量的编程能力并不是必需的。更重要的是能够将自己的思维方式与模型同步，确保模型理解并执行自己的想法。”曾国洋说道，“让模型理解你的想法是一种独特的体验，它要求用户对模型的运作方式有一定的了解，并且能够清晰地表达自己的概念和目标。”对于其他公司来说，大模型技术会以不同形式的工具、功能出现。</p><p></p><p>面壁智能的目标是实现通用人工智能，团队为此有一个清晰的路线图，包括在文本模态上要达到的效果、未来向多模态和具身智能的转变等规划。2024 年，面壁智能将继续专注于模型的研发和优化。而面壁智能的选择能否助其顺利发展、实现自己的 AGI 理想，还需要时间来回答。</p><p></p><p>栏目介绍</p><p></p><p><a href="https://www.infoq.cn/theme/230">《大模型领航者》</a>"是 InfoQ 推出的一档聚焦大模型领域的访谈栏目，通过深度对话大模型典范企业的创始人、技术负责人等，为大家呈现最新、最前沿的行业动态和思考，以便更好地参与到大模型研发和落地之中。我们也希望通过传播大模型领域先进的实践和思想理念，帮助潜在大模型应用者、创业者、开发者等做好各类决策和选型。</p><p></p><p>如果您有意向报名参与栏目或想了解更多信息，可以联系：T_demo（微信，请注明来意）</p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/5h8vXmEcRXYPBkD6VQUF</id>
            <title>大模型的“瘦身”革命：巨头逐鹿轻量化大模型 | 大模型一周大事</title>
            <link>https://www.infoq.cn/article/5h8vXmEcRXYPBkD6VQUF</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/5h8vXmEcRXYPBkD6VQUF</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Apr 2024 08:06:34 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 大模型, 移动设备端, 长文本处理, 开源领域
<br>
<br>
总结: 大模型的快速发展使得了解最新技术动态和积极学习成为从业者的必修课。本周重点关注了轻量化和设备端集成，展示了AI应用向移动设备端迁移的趋势，以及长文本处理能力的竞争。同时，开源领域也有不少新进展，如苹果公司和Snowflake的开源模型。 </div>
                        <hr>
                    
                    <p>大模型的快节奏发展，让了解最新技术动态、积极主动学习成为每一位从业者的必修课。InfoQ研究中心期望通过每周更新大模型行业最新动态，为广大读者提供全面的行业回顾和要点分析。现在，让我们回顾过去一周的大模型重大事件吧。</p><p></p><h2>一、重点发现</h2><p></p><p>本周，轻量化和设备端集成成为行业的热点。微软的&nbsp;Phi-3&nbsp;系列小模型和苹果的&nbsp;OpenELM&nbsp;系列端侧小模型的发布，展现了AI应用向移动设备端迁移的趋势。这也预示着未来智能手机和笔记本电脑等设备将能够处理以往只能在云端或高性能服务器上执行的复杂任务，极大地扩展了端侧AI的潜力和应用范围。此外，长文本处理能力的竞争再次增大。Kimi&nbsp;发布之后，商汤和浪潮分别升级自身模型的长文本能力并展开第二波围剿。</p><p></p><h2>二、具体内容</h2><p></p><p></p><h3>大模型持续更新</h3><p></p><p>4&nbsp;月&nbsp;23&nbsp;日，微软推出了&nbsp;Phi-3&nbsp;系列小模型，并发布了其技术报告。值得注意的是，Phi-3-mini&nbsp;型号，仅拥有3.8&nbsp;亿参数，已在众多性能评估标准上超越了&nbsp;Llama&nbsp;3&nbsp;模型。为了促进开源社区的发展，微软特别设计了与&nbsp;Llama&nbsp;系列相兼容的模型架构。4月&nbsp;23&nbsp;日，商汤科技最近宣布推出了其最新的大模型——日日新&nbsp;SenseNova&nbsp;5.0&nbsp;大模型，该模型采用了混合专家（MoE）架构。SenseNova&nbsp;5.0&nbsp;在超过&nbsp;10TB&nbsp;tokens&nbsp;的数据集上进行了训练，上下文推理长度达到了200k。4&nbsp;月&nbsp;25&nbsp;日，浪潮海岳大模型2.0正式发布。在长文本、长图文、长语音处理方面能力进行升级。</p><p></p><h4>开源领域</h4><p></p><p>4&nbsp;月&nbsp;22日，苹果公司开源了&nbsp;OpenELM&nbsp;系列小模型，涵盖2.7亿、4.5亿、11亿和30亿四种参数规模。这些模型能在个人设备上运行，包含从&nbsp;2.7亿到&nbsp;30&nbsp;亿参数的不同版本，旨在推动设备端&nbsp;AI&nbsp;应用。4&nbsp;月&nbsp;25&nbsp;日，Snowflake&nbsp;推出了名为&nbsp;Arctic&nbsp;的开源大型语言模型（LLM）。Arctic&nbsp;模型采用独特的&nbsp;Dense-MoE&nbsp;混合&nbsp;transformer&nbsp;架构，以低成本实现较高企业智能水平。此外，Arctic&nbsp;的上下文窗口初始设置为&nbsp;4K，团队正在研发支持无限序列生成的技术，未来将扩展到&nbsp;32K。</p><p></p><h4>多模态领域</h4><p></p><p>4&nbsp;月&nbsp;22&nbsp;日，腾讯&nbsp;Robotics&nbsp;X&nbsp;和腾讯&nbsp;AI&nbsp;Lab&nbsp;提出了多模态AI大模型&nbsp;SEED-X。该模型是对之前&nbsp;SEED-LLaMA&nbsp;的升级版，能够理解任意尺寸和比例的图像，并包含多模态预训练和指令调整两个阶段，使用大规模多模态数据集增强模型的适应性和灵活性。在定量和定性实验评估中展现了卓越的性能，尤其在公共基准测试和现实世界应用场景中表现突出。4&nbsp;月&nbsp;22&nbsp;日，西湖大学、浙江大学的研究团队发布多模态大型语言模型（MLLM）——Cobra。它利用&nbsp;Mamba&nbsp;语言模型并融合视觉编码器，以线性计算复杂度提供高效的推理性能。在多个基准测试中，Cobra&nbsp;展现了与参数更大型模型相媲美的性能，尤其是理解和处理视觉信息方面。4&nbsp;月&nbsp;25&nbsp;日，北京大学&nbsp;Yuangroup&nbsp;开源的&nbsp;open-sora&nbsp;更新升级。新增功能包括支持长达&nbsp;16&nbsp;秒的视频生成，最高720p&nbsp;的分辨率，并且能够处理不同宽高比的文本到图像、文本到视频、图像到视频、视频到视频以及无限长视频的生成需求。</p><p></p><h4>科研领域</h4><p></p><p>4&nbsp;月&nbsp;22&nbsp;日，美国AI蛋白质设计公司&nbsp;Profluence&nbsp;推出了世界上首个开源的AI生成的基因编辑器&nbsp;OpenCRISPR-1。成功实现了对人类基因组的精确编辑。该技术基于与&nbsp;ChatGPT&nbsp;相同的方法，通过分析大量生物数据，生成了数百万种自然界中不存在的&nbsp;CRISPR&nbsp;类蛋白质，扩展了&nbsp;CRISPR&nbsp;家族的多样性。</p><p></p><h3>应用探索</h3><p></p><p></p><h4>新产品新应用/功能</h4><p></p><p>4月&nbsp;20&nbsp;日，文生图服务平台&nbsp;Leonardo.ai&nbsp;引入了新的图片样式引导功能。该功能允许用户上传个性化图片以生成更为精确和多样化的图像成果。该功能类似于用户友好的视觉微调工具，使得用户能够根据自己的需求定制连贯的连环画作或保持视觉一致性的宣传海报。4&nbsp;月&nbsp;22&nbsp;日，腾讯公司宣布其协作&nbsp;SaaS&nbsp;产品线全面整合了腾讯混元大模型。这包括了企业微信、腾讯会议、腾讯文档等核心产品，以及腾讯乐享、腾讯电子签、腾讯问卷和腾讯云AI代码助手等其他工具。4&nbsp;月&nbsp;25&nbsp;日，阿里巴巴通义实验室在通义&nbsp;APP&nbsp;上线&nbsp;EMO&nbsp;模型。该AI技术能通过人物照片和音频生成同步口型和表情的视频。为防止技术被滥用，通义实验室在应用内预置了经过审核的音频模板，暂不开放用户自定义音频，并采取了算法和人工两道审核机制，确保内容安全。</p><p></p><h4>智能体</h4><p></p><p>4&nbsp;月&nbsp;25&nbsp;日，Sanctuary&nbsp;AI&nbsp;推出了第七代&nbsp;Phoenix&nbsp;人形机器人。新一代机器人具有更长的运行时间、更快的构建速度、更低的制造成本、增加的运动范围和耐用性，以及更高的视觉和触觉感知能力，同时与麦格纳国际合作，推动通用人工智能机器人在汽车制造等领域的应用。</p><p></p><h3>基础设施</h3><p></p><p>4&nbsp;月&nbsp;21&nbsp;日，中山大学、哈佛大学的研究人员针对多模态大模型的创造力进行研究并提出&nbsp;Creative&nbsp;Leap-of-Thought（CLoT）的训练方法，旨在打破常规思维，激发模型的创新能力。CLoT&nbsp;能够有效提升多模态大模型在创造性任务中的表现，超越了包括&nbsp;GPT-4&nbsp;在内的其他先进模型。此外，该研究还构建了&nbsp;Oogiri-GO&nbsp;数据集，为进一步研究提供了资源。4&nbsp;月&nbsp;22&nbsp;日，阿里云的百炼平台为&nbsp;Llama&nbsp;3&nbsp;模型提供了一站式的解决方案，覆盖了模型的训练、部署和推理等关键环节。目前，阿里云在一定时间内对&nbsp;Llama&nbsp;3&nbsp;模型的开发和调用实行免费政策，用户可以在百炼模型广场上申请试用&nbsp;Llama&nbsp;3，并与其他模型进行性能对比。4&nbsp;月&nbsp;23&nbsp;日，华为云在香港峰会上宣布，将在香港提供即开即用的&nbsp;AI&nbsp;云服务，为大模型训练和推理提供高效、长稳、可靠的&nbsp;AI&nbsp;算力。华为云通过全链路云化工具链支持大模型的高效迁移、开发和运行，并特别优化了昇腾云的大模型专区，以支持“百模千态”应用的快速落地。4&nbsp;月&nbsp;24&nbsp;日，高通发布骁龙&nbsp;X&nbsp;Plus&nbsp;芯片。该芯片采用&nbsp;4nm&nbsp;工艺，具备&nbsp;10&nbsp;核心和最高&nbsp;3.4GHz&nbsp;主频，GPU&nbsp;算力达&nbsp;3.8TFLOPS，并支持先进的连接技术。该芯片在&nbsp;AI&nbsp;性能上达到&nbsp;45&nbsp;TOPS，与骁龙&nbsp;X&nbsp;Elite相当，且在多线程&nbsp;CPU&nbsp;性能上超越了苹果&nbsp;M3&nbsp;芯片。</p><p></p><p>报告推荐</p><p>Sora来袭，国内发展文生视频模型的土壤如何？各公司用脚投票开闭源路线的当下，开源在大模型市场进程中的价值正在被重新定义吗？人型机器人重回视野，大模型是否助力其刷新能力上限？Devin和智能编码助手是同一条赛道上的不同节点？多家企业宣布All&nbsp;in&nbsp;AI，对市场意味着什么？答案尽在InfoQ研究中心近期发布的《2024&nbsp;年第&nbsp;1&nbsp;季度大模型监测报告》，关注「AI前线」公众号，回复「季度报告」免费下载，一睹为快吧~</p><p></p><p><img src="https://static001.geekbang.org/infoq/df/df2037200d792e5be89596273fdcf950.png" /></p><p></p><p></p><p>报告预告</p><p>AGI究竟是什么？AI&nbsp;Agent&nbsp;如何助力人工智能走向AGI时代？在营销、金融、教育、零售、企服又有哪些典型应用和案例？欢迎大家持续关注InfoQ研究中心即将发布的《中国AGI市场发展研究报告&nbsp;2024》。</p><p></p><p><img src="https://static001.geekbang.org/infoq/0c/0c0207976c6592ac74b5109332dc9e1c.jpeg" /></p><p></p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/eVugB4V9E9cEsqaEJ27O</id>
            <title>大模型开闭源争吵不休：开源落后闭源一年，决定模型能力的不是技术？</title>
            <link>https://www.infoq.cn/article/eVugB4V9E9cEsqaEJ27O</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/eVugB4V9E9cEsqaEJ27O</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Apr 2024 06:36:17 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 开源, 闭源, 大模型, 模型能力
<br>
<br>
总结: 作者华卫讨论了开源和闭源模型在大模型时代的争议。虽然开源模型在崛起，但其背后问题不可忽视。访谈中专家们分析了开源和闭源模型的能力差异，以及影响因素。他们认为模型能力取决于团队的能力、数据、算力等因素，而开源并不一定能带来更好的效果。整体来看，开源社区仍落后于闭源社区，但差距在逐渐缩小。 </div>
                        <hr>
                    
                    <p>作者 | 华卫</p><p></p><p>开源和闭源之争，在大模型时代依然延续着。前不久，百度创始人李彦宏在内部讲话中发出“开源模型会越来越落后”的言论，再次将这一话题引爆。</p><p></p><p>不仅有许多业内人公开提出不同看法，似乎还接连迎来市场层面的“回应”：Meta 时隔两日发布性能直追 GPT 4 的开源大模型 Llama 3，苹果、微软又各自开源了针对手机等移动设备的语言模型 OpenELM 和 Phi-3 Mini。</p><p></p><p>然而，尽管开源模型在今天的崛起有目共睹，其背后的问题依然不可回避。由于本身的黑盒属性，开源的“众人拾柴火焰高”优势并不能完全显现在大模型上，甚至成本和效率更受影响。那么对于各个行业的厂商来说，身处如今的大模型市场，该做出怎样的选择？</p><p></p><p>带着这一问题，InfoQ《极客有约》特别邀请了零一万物开源负责人林旅强担任主持人，与 Data Strato 副总裁史少锋、华为 AI 科学家张敏、LLMFarm &nbsp;创始人 &amp; CEO 宜博， 在 AICon 全球人工智能与机器学习技术大会即将召开之际，一同探讨开源与闭源模型的现状、差异及未来发展。部分亮点如下：</p><p></p><p>整体开源落后于闭源，以 GPT 为代表大概是一年时间的差距；模型能力的差异不在于开或闭，而是背后的人与团队；自建模型还是购买第三方服务，企业要根据各自的商业场景选择成本和合规需求最适合的部署方式；企业使用大模型可能不止一套，会像今天使用云一样是混合架构；正确认识大模型的能与不能才是避坑最好的条件。</p><p></p><p>在访谈的第一部分，四位专家分别对开源、闭源大模型的成本能力和效益进行了分析；第二部分分析了两类大模型面临的技术和合规挑战；第三部分则是从实际应用与效果角度进行了分析。以下为访谈实录，经编辑。</p><p></p><p></p><p>完整视频参看：</p><p><a href="https://www.infoq.cn/video/pKua6PxVgxvdDygcgrWd">https://www.infoq.cn/video/pKua6PxVgxvdDygcgrWd</a>"</p><p></p><p></p><h1>开源、闭源哪家强？</h1><p></p><p></p><p>林旅强：目前从模型能力的角度来说，开源阵营和闭源阵营之间整体是什么样的情况？</p><p>张敏：大模型是从 ChatGPT 热起来以后，被越来越多的人和公司关注到，现在看是有开源、闭源之说。闭源的代表是 OpenAI，以及 Claude 也有一部分模型是闭源的。开源来看，从 Llama 1 到最新的 Llama 3，效果越来越好，大家也越来越认可这些模型，最近看到 Meta 的 400B 大模型，效果已经和 GPT 4 非常接近了。从开发者角度，我们希望能看到更多效果更好的开源模型，这实际上对整个大模型领域的繁荣可能会有更多帮助。</p><p>宜博：个人认为，整个开源和闭源社区的模型分为三个阶段：小于 GPT 3 或者 3.5 的，接近于 GPT 3 和 3.5 的，接近于 GPT 4 的。去年上半年， OpenAI 发了 GPT3.5 和 GPT 4 之后遥遥领先于整个开源社区；到去年下半年时，开源社区的情况有了很大改变，发布了很多接近于 GPT 3-3.5 能力的新模型，今年上半年开始有一些部分能力已经靠近 GPT 4 的开源模型。</p><p>整体来讲，开源社区当前还是落后于闭源社区，如果以 GPT 为标准呢，大概是一年时间的差距。开源社区其实一直处在追赶闭源社区的态势，但这种差距在缩小。今年上半年又发了 Sora，开源社区开始追 Sora，到现在为止虽然做了很多努力，但效果还差很多。</p><p>史少锋：刚才两位老师发表了他们的观点，我觉得整体上大家的感觉差不多，就是一开始闭源模型遥遥领先或让人眼前一亮，但随着更多的开源模型被放出来，开源的能力也在快速跟上。作为模型使用者，今天我们主要还是通过 API 的方式来用大模型，但现在新的开源模型能力越来越强，同时对计算资源的要求在不断降低。我们期待不远的将来，开源模型可以在本地跑起来，能够完全私有化地去支撑一些应用，这对我们有很大的吸引力。</p><p>林旅强：那什么因素会严重影响开源和闭源模型的能力差异呢？</p><p>针对这个问题，我个人认为开源和闭源模型的能力差异，重点不在于它开源或闭源，而是它的研发团队的能力差异。至于做出来的模型要开源还是闭源，是进一步从该公司的整体商业模式去考虑的点。之前 Llama 推出的时候，我非常兴奋，觉得终于有人运用开源来突围闭源的大模型了，因为训练模型成本实在太高，要开源本来就不容易；虽说至今二者仍有些差距，但如果不开源就没机会给开发者和产业界有另一种选择了。</p><p>史少锋：的确，模型会很依赖于开发团队的工程能力，并不在于开源还是闭源。今天的开源模型也并不是真正的开源，正如百度创始人李彦宏所说，大模型本身就是一个黑盒子，并不能指望社区有多少贡献。除此之外，模型还依赖于掌握的数据语料质量、丰富程度以及算力规模。这也是为什么今天我们看到，只有非常大型的公司才能开发出让整个业界为之一亮的大模型。</p><p>宜博：我认同开源和闭源对模型能力的影响并不在于形式，而在于背后的人，和背后的团队所持有的资金、算力、数据。</p><p>林旅强：大模型跟开源软件有一点很不一样的地方，就是开源软件有可能因为社区不断有代码贡献而变得更好，但现在业内所谓的开源大模型则是把权重 open 出来，没办法以开源社区贡献上游的模式让算法和数据质量更好，确实很依赖出品团队的能力，如数据、框架算法调优、算力门槛还有最新方法的挑选。所以在我们看来，模型能力的差异不在于开源或闭源，而在于团队的人才密度有多高。</p><p>张敏：数据、算力和算法对大模型都至关重要，算法是与团队是强相关的，这对于模型最终效果的提升是非常重要的。</p><p>林旅强：刚才我们讨论到开源、闭源模型的能力，那它们的差距到底是逐步缩小还是增大？开源是不是会越来越不好？闭源越来越领先？</p><p>宜博：我认为差距并不是持续扩大和缩小，而是永远在动态平衡变化的状态。</p><p>林旅强：那照你的描述是不是永远闭源走在前面，开源在追赶？</p><p>宜博：这一点其实是由行业现状决定的，比如在服务器领域，Windows 现在很难追得上 Linux，iOS 有一些领域也追不上安卓。大模型领域是由 OpenAI 开始主导的，所以在其领头羊位置不变的情况下，不管是闭源还是开源的，只要落后于 OpenAI 都是在追赶。</p><p>林旅强：所以这个问题应该调整为，GPT 跟其他模型的能力是逐渐缩小还扩大。</p><p>史少锋：站在百度文心一言的角度来说，我理解他们在思考的是有没有必要做开源，开源模型并不一定能像普通开源软件那样有“众人拾柴火焰高”的效果，反而要花费更多的时间和精力去做各种合规、对外发布、问题收集等流程。在这种情况下，他们认为开源没有必要，闭源的话效率更高，可以使团队更加聚焦于训练下一代模型。某一天 OpenAI 把大模型开源了，是否能代表开源打倒了闭源呢？我觉得也不是。</p><p>林旅强：那从成本、能力、效益分析的话，部署自己的大模型与使用第三方大模型在初期成本上有什么不同？长远来看，自建模型与购买模型服务在成本上又会如何变化？</p><p>宜博：我们做了很多轮实践发现，假如第一次去验证模型，用 API 调用是最划算的，因为 API 用量很少。但如果要跑数据，一定要用自己的服务器和开源模型去做，否则成本太大了。比如我们曾经有个项目，大概算下来，全部跑 API token 比自己购买服务器的成本要贵 200 多万。再就是推理部署的未来环境，用户量大到一定程度后会有个临界点，可能就用自己的服务器比较划算了。所以，要根据大家各自使用的场景去选择不同的成本策略。</p><p>张敏：从我们对接的客户来看，他们是更希望通过本地的私有化部署来做业务支撑，这对数据安全是非常有好处的。</p><p>史少锋：站在用户的角度，我觉得今天的 SaaS 大模型服务已经非常便宜，如果自己去搞部署，那成本就高了去了。目前 Open AI 的价格不代表以后，大家都在卷，很多价格会更低，国内甚至有免费开放给公众使用的。对于 To B 领域，可能第一考虑的是数据安全，To C 没有看到用私有化部署的。</p><p>林旅强：确实，除了部署成本外还有一些隐性的成本，比如客户是不是愿意模型平台把他通过 API 所调用的数据拿出去再训练。个人去使用的话， API 确实门槛比较低，现在各家的价格都还算是比较便宜。</p><p>那如果从总体的成本控制方面，企业应该如何去选择适合自身的大模型策略？</p><p>我个人认为要看企业本身想怎么用大模型，如果单 API 就能够解决且量没有很大的情况下，先去把 API &nbsp;稳定地搞起来；但如果要结合非标的数据场景去做，那只能加上开源的部署。</p><p>宜博：企业真正在用的时候，一般是一个递进的验证过程，首先用最便宜的 API 去验证 POC，甚至直接在 ChatGPT 上免费验证，之后如果有开源的部署需求，再去验证场景。过程中需要企业自己想清楚，如何在满足场景的情况下选择成本和合规需求最适合的部署方式。</p><p>林旅强：我想补充一点，之前有人问国内是需要私有部署的多还是调 API 的多，我就说要先看合规问题。因为现在有政策要求用国产服务，但还有一些人是用了“套壳网站”调外网大模型的 API 。</p><p>张敏：大模型也有参数量的大小区别，我们真正在给客户在做应用时，还是要根据业务领域的效果来看。在百度的文心一言里，也是用大模型和小模型一起来支持用户需求。</p><p>史少锋：企业使用大模型后，可能也会像今天使用云一样是混合架构，根据不同需求一部分可能会放在公有云上，一部分放在私有云。为了确保应用端的用户无感，可以把 SaaS 版的大模型作为一个 Plan B，相当于做了一层保护机制。综合而来的话，以后企业可能不止一套大模型。</p><p>林旅强：我也想补充一下，现在所谓的大模型到底多大？从成本能力与效率分析来讲，我们也得把大模型分为不同档次。虽然 scaling law 是存在的，但越大的模型性价比越往下；而小模型现在要做出效果的门槛其实也很高。目前不管多大的模型都有各种不同的成本要去考虑，所以最终还是需要回到具体场景和商业产品的本质来看。</p><p></p><p></p><h1>技术与合规挑战</h1><p></p><p></p><p>林旅强：在技术实现层面，自建大模型与采用第三方模型在技术难度和支持上有何不同？</p><p>宜博：现在自建大模型一般有几种难度：第一种是买一个小机器放在办公室，如果要买高算力机器放在机房或者自建机房，难度指数是很高的；第二种，有了算力去部署时，也会遇到各种各样的问题，如推理框架选择、速度、机器使用等，这些对于没有专业技能团队的非技术企业消耗很大，过程中虽然所有技术人员学了很多东西，但公司的环境部署和上线成本非常大。</p><p>史少锋：我觉得这个问题并不是很精确，自建大模型和用第三方模型的技术难度和配置不同。今天大家都在用第三方模型，但自建大模型还是偏少，大家更多还是用外部做得好的模型，区别就是自己部署的大模型和第三方 SaaS 大模型之间的区别。就像刚才宜博说的，自己去部署要操心的是方方面面，包括硬件采购、运维、算力扩容、模型部署和升级、调优等。相较而言，用第三方模型更简单，很多代码拿来就可以用，但这个情况也在逐渐改变。</p><p>随着开源生态越来越健全，软件也越来越丰富，下载速度可能更快，以后笔记本都能跑一些参数不太大的模型。在并发量或需求量不太大的场景下，自建大模型不会比第三方模型复杂太多，gap 会逐渐缩小。</p><p>张敏：用开源大模型去做部署就像站在巨人肩膀上，会走的更快。自建则需要具备很多前提，如数据、算力、算法和好的团队，成本可能要远高于使用开源。</p><p>林旅强：自建大模型的难度比较大，技术实践已经是一道门槛，像开发者本身的能力水平、背后商业机会以及交付能力等。直接采用第三方模型，也需要运维、部署的知识能力和资源投入。所以企业还是要按照能力和成本考量去选择。</p><p>另外，我们都知道大模型可能涉及到数据安全和个人隐私的保护。在自建与第三方模型使用中，数据安全与隐私保护分别面临哪些挑战？大家怎么去做呢？最简单的是，担心就全部私有化部署，如果数据不需要任何安全和隐私保护，就全部调 API。也就是说，还是从业务角度去选。</p><p>宜博：实际上我们会遇到几种情况，第一种情况就是直接调用闭源模型的 API，他们号称数据不会被拿去训练，但实际经常会发现数据被使用了；第二种是当你用三方算力平台训练模型时，也会发现有自己训练数据被拿去的情况。大家知道现在监管非常严，内部虽然保证数据安全和隐私，但实际上做合规很耗精力，面临的细节挑战还蛮多。现在整个落地量不大，所以问题还没有那么凸显，但我认为未来会逐步变得重要。</p><p>史少锋：针对大模型，我认为不管是自建还是第三方、私有化部署还是公有，都应该足够重视数据安全和隐私保护。即便自建大模型，训练时没有识别出数据隐私，也可能导致信息泄露。而开源模型正因为要开放给众多用户，在安全和隐私方面也可能做得很好。Meta 发布的 Llama 3，就花了很多功夫在多个层次进行安全检测。</p><p>这就像我们经常讨论的，闭源软件安全还是开源软件安全？闭源软件可能因为黑客看不到源代码，所以找不到安全漏洞，但不为人知的漏洞可能会存在更长的时间；开源软件貌似因为代码开放容易被抓到漏洞，但因为被很多人盯着，促使其在不断地提高安全性，长久来说可能反而做得更好。</p><p>张敏：数据安全对于大模型来说确实非常重要，训练时会牵涉到用户的隐私数据，抓取也可能存在攻击性数据，从而导致输出问题。另一方面，即使大模型做了私有化部署，使用过程中产生的数据也需要做安全保护。</p><p>林旅强：再补充一个点，很多人在讲数据安全时并没有考虑到跨境传输。现在出海很热，实际应用来讲可能每个地区对于数据跨境的要求不同，在各个市场各自部署的成本也就更高。合规不只要考虑到中国，还有客户所在的国家，像欧盟、美国都会有相关的数据法规。</p><p></p><p></p><h1>实际应用与效果</h1><p></p><p></p><p>林旅强：利用开源或闭源大模型解决实际业务场景，在部署过程当中有哪些区分？大家分享一下踩过的坑，也教教怎么避坑。</p><p>宜博：第一个观点是尽量用 RAG，不要一上来就做 SFT 训练；第二个是尽量不要一上来就用 Langchain，要花大量的时间去学习未来 90% 都用不上的代码。</p><p>张敏：我们去跟客户做支撑的时候，需要把用户场景和数据越早明确下来越好，这对于我们的方案设计和模型选择都非常重要。</p><p>史少锋：关于大模型在具体业务场景的避坑，我觉得还是要实践出真知，有一套针对自己场景的测试数据集，因为大模型过于通用，并且也会升级。我们想到的办法是可以用另一个更高水平的大模型来对多个模型的输出打分。建立一套测试体系，对于不停迭代模型去提升结果准确性很有必要。</p><p>林旅强：我觉得要能够正确认识大模型能够为你解决什么问题，作为避坑的前提条件。就像张老师刚刚讲的，很多客户现在误以为大模型跟神仙一样什么都能干，这其实是有问题的，大模型只是在某一些方面确实做得比过去好很多，甚至比人类强。但我们还是要把业务流拆解出来，哪部分去接入大模型？能够做什么？怎么解决幻觉问题？RAG 好在哪里、难在哪里？也绝不是那些开源数据集测评的打分越高代表越好，还是得从具体场景切入，认真把内部评测标准搞好，才会知道坑在哪里。所以我觉得，正确认识大模型的能与不能才是避坑最好的条件。</p><p>现在线上有个问题，即使训练内部模型也需要对涉及用户的数据进行脱敏，在这方面有没有一些比较好的工具或经验？各位老师实际有没有接触过用户的数据，以及会用什么方式把用户的数据脱敏？</p><p>史少锋：我们本身就是做数据治理，也调研了市面上很多数据平台在这方面的做法。成熟的数据平台都有一套数据合规方面的功能体系，其次会通过 AI 去识别数据中的敏感信息，在导出时提醒用户，还有一些敏感信息打码、用户访问 policy 以及数据溯源的配合功能。对于一些自建的大数据平台，是借助工具和统一平台去数据溯源、定义用户访问权限，来把风险被控制到最低。</p><p>宜博：这块我们做的比较少，一般的客户数据就在本地或者企业内部查询了，脱敏拿出来的情况还比较少。</p><p>张敏：我们这边做的更多是回复角度方面，如果涉及到敏感内容的话，会对回复做过滤处理或者换一种方式去回答。</p><p>林旅强：那你怎么知道它是敏感的？</p><p>张敏：我们会做一些检测，如果问题本身涉及到敏感词，就需要做过滤和管控。</p><p>史少锋：其实常用常见的 PII 信息是有一套正则规范的，身份证号码、Email、社保号等都有，即便是文本型的识别也并不是特别难。</p><p>林旅强：未来开源模型如何能利用好社区优势？有哪些方向和趋势？</p><p>开源确实比较能实践社区的方法，闭源提供 API 的就只能是用户。在我看来，当前的“开源”大模型并不是真的把数据或训练代码开源出来，而是把训练的结果也就是权重给 open 出来，海外也有称为开放大模型的。可是，它又不像是闭源软件的二进制，开发者又能基于开放大模型来做二次开发，例如 SFT、继续预训练等，情况有点介于软件开源和闭源光谱当中的中间态。因此，开源模型仍然有一定的被二创的空间，闭源模型则不太容易这样操作，所以我认为，即使开源大模型没有开源软件那么开放，但开源模型社区的优势就是可以有很多二创。</p><p>刚刚说大模型太热，如何解决预期过高的问题？业务方老板可能误以为未来一切都靠 AGI 了，但其实当下能做的事非常有限，我们如何向这些没那么懂但手头有预算且脑中有想象的用户，去正确传递大模型的能力界限？</p><p>宜博：我们从去年到今年做最多的就是给大家分享大模型是什么？什么能干？什么不能干？边界在哪里？背后的原理是什么？现在能真正把这些事情和自己的想法都传递给客户的团队还比较稀缺，希望有更多的程序员和技术领导者加入进来。第一，不要太高估短期大模型的能力；第二，不要太低估长期大模型的能力；第三，在当下把能落地的场景先落地。</p><p>张敏：我们这边的做法是，通过 GPT 4 中目前我们认为的最好效果，让大家客观看到当前大概做到什么程度。</p><p>史少锋：刚刚宜博说的是大家眼下不用对大模型期望太高，要知道它目前只是一个助手，还需要懂业务和有专业技能的人去做最后的把关，同时我们只有不断去试去用，才能找到最适合的方向。现在大家看到文生图出来也没多久，但有很多文章配图都换成了 AI，意味着这方面的生产效率已因此得到很大提升。</p><p>林旅强：总结一下就是， AI 的天花板取决于使用者的个人理解和业务认知。在你的指导之下去做工作的 AI，不可能做得比你还厉害，你才是 AI 的天花板。当我们都了解到这一点，就知道它的局限。</p>
                ]]>
            </content:encoded>
        </item>
        
        <item>
            <id>https://www.infoq.cn/article/7y5946vBWVXNpJvYzmZl</id>
            <title>苹果与 OpenAI 重启谈判，Siri 或引入 ChatGPT，网友：国行用文心一言？</title>
            <link>https://www.infoq.cn/article/7y5946vBWVXNpJvYzmZl</link>
            <guid isPermaLink="false">https://www.infoq.cn/article/7y5946vBWVXNpJvYzmZl</guid>
            <pubDate></pubDate>
            <updated>Mon, 29 Apr 2024 06:15:57 GMT</updated>
                
                
            <content:encoded>
                <![CDATA[
                    
                        <div> 关键词: 苹果, OpenAI, iPhone, 人工智能
<br>
<br>
总结: 苹果正在与OpenAI就在iPhone上集成聊天机器人功能进行谈判，以加速推进人工智能技术的发展。虽然目前尚未达成最终协议，但这一举措标志着两家公司之间对话的重启，同时苹果也在与谷歌讨论将Gemini引入iPhone。这一举动显示了苹果在人工智能领域的积极探索和合作意愿，未来可能会为用户带来更多智能化的功能和体验。 </div>
                        <hr>
                    
                    <p>整理 | 华卫</p><p>&nbsp;</p><p>据外媒报道，苹果正在就iPhone集成聊天机器人功能，加紧与 OpenAI 的谈判。有知情人士透露，两家公司已开始讨论可能达成的协议条款，以及如何将OpenAI功能集成到苹果的下一代iPhone操作系统 iOS 18中，但因为审议是私人的，他们要求不透露身份。</p><p>&nbsp;</p><p>虽然目前尚未达成协议，但此举标志着两家公司之间对话的重启。今年早些时候，苹果曾与OpenAI就一项交易进行谈判，但那之后双方却很少合作。</p><p>&nbsp;</p><p>去年，苹果公司 CEO 蒂姆・库克表示，他个人在使用OpenAI的 ChatGPT，但发现“有许多问题需要解决”。他承诺，新的人工智能功能将以“非常完善的基础”进入苹果的平台。</p><p>&nbsp;</p><p>之前的报道指出，一直以来，苹果在与多家大型人工智能公司进行谈判，以寻求在手机设备上集成聊天机器人功能方面的潜在合作伙伴关系。今年3月Mark Gurman就在一份新报告中称，苹果正在与谷歌讨论将Gemini引入iPhone。</p><p>&nbsp;</p><p>Gemini 于去年 12 月推出，是谷歌迄今为止最强大的大型语言人工智能模型，还在今年 2 月取代了 Google Bard版本。尽管刚开始发布时出现了一些严重问题，包括生成令人不安以及不准确的插图，但目前Gemini 是谷歌面向公众的重要人工智能项目。</p><p>&nbsp;</p><p>针对上述情况，现在苹果、OpenAI和谷歌的代表都拒绝置评。苹果尚未就使用哪些合作伙伴做出最终决定，也不能保证一定会与OpenAI和谷歌达成协议，也可能选择另外的人工智能供应商。</p><p>&nbsp;</p><p>在 ChatGPT 的迅速崛起以及随后来自谷歌、Microsoft 和 Meta 等公司的生成式 AI 工具和功能的激增期间，苹果研究人员也正在研究一种新模型 ReALM，可以为 Siri 提供苹果用户一直想要的生成式AI升级。据介绍，目前该模型可以比 ChatGPT 更好地理解在屏幕、对话和背景引用（如在后台运行的应用程序或功能）时的上下文问题。</p><p>&nbsp;</p><p>但对苹果来说，将生成式 AI 功能外包给合作伙伴，不仅将有助于加速苹果向聊天机器人的推进，还可以规避风险和减轻其平台的责任。</p><p>&nbsp;</p><p>无论苹果最终的计划是什么，可能都要等到 6 月举行的苹果全球开发者大会 （WWDC） 才能知道。</p><p>&nbsp;</p><p>参考链接：</p><p><a href="https://mashable.com/article/apple-openai-partnership-ios-18">https://mashable.com/article/apple-openai-partnership-ios-18</a>"</p><p><a href="https://www.bloomberg.com/news/articles/2024-04-26/apple-intensifies-talks-with-openai-for-iphone-generative-ai-features">https://www.bloomberg.com/news/articles/2024-04-26/apple-intensifies-talks-with-openai-for-iphone-generative-ai-features</a>"</p><p><a href="https://mashable.com/article/apple-google-gemini-iphone">https://mashable.com/article/apple-google-gemini-iphone</a>"</p>
                ]]>
            </content:encoded>
        </item>
        
    </channel>
</rss>